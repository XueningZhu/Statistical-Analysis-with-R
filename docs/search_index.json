[["index.html", "统计分析（以R语言为工具）：辅助材料 前言", " 统计分析（以R语言为工具）：辅助材料 朱雪宁 &amp; 任怡萌 2022-07-09 前言 这是《统计分析（以R语言为工具）》一书所用R语言代码辅助材料，还在更新中。错漏之处难免，欢迎指出错误或提出改进意见。 相关下载： 一些配套数据的打包文件，提取码：zi7w 本书所使用的课件： 第1章：统计分析与R语言，提取码: rgrg 第2章：R语言中的数据管理及预处理，提取码: bgl3 第3章：基本统计分析，提取码：4670 第4章：数据描述与可视化，提取码：3h4g 第5章：参数估计与假设检验，提取码：0smb 第6章：线性回归，提取码：iais 第7章：逻辑回归，提取码：4d67 第8章：降维分析，提取码：lvkp 使用本教程必须安装的软件包： resahpe reshape2 plyr dplyr readxl lubridate stringr e1071 fBasics tidyr corrplot ggplot2 zoo plotly purrr data.table magrittr BSDA pROC ROCR psych 本教程中用到的软件包列表（更新R软件后可以在一个基本R中运行如下命令）： pkgs &lt;- c(&quot;resahpe&quot;, &quot;reshape2&quot;, &quot;plyr&quot;, &quot;dplyr&quot;, &quot;readxl&quot;, &quot;lubridate&quot;, &quot;stringr&quot;, &quot;e1071&quot;, &quot;fBasics&quot;, &quot;tidyr&quot;, &quot;corrplot&quot;, &quot;ggplot2&quot;, &quot;zoo&quot;, &quot;plotly&quot;, &quot;purrr&quot;, &quot;data.table&quot;, &quot;magrittr&quot;, &quot;BSDA&quot;, &quot;pROC&quot;, &quot;ROCR&quot;, &quot;psych&quot;) install.packages(unique(pkgs)) 编译本教程所用的R软件环境： devtools::session_info() ## ─ Session info ──────────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.0 (2020-04-24) ## os macOS Catalina 10.15.7 ## system x86_64, darwin17.0 ## ui RStudio ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Asia/Shanghai ## date 2020-12-02 ## ## ─ Packages ──────────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 4.0.0) ## backports 1.1.10 2020-09-15 [1] CRAN (R 4.0.2) ## bookdown 0.21 2020-10-13 [1] CRAN (R 4.0.0) ## BSDA * 1.2.0 2017-07-30 [1] CRAN (R 4.0.0) ## callr 3.5.1 2020-10-13 [1] CRAN (R 4.0.2) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 4.0.0) ## class 7.3-16 2020-03-25 [1] CRAN (R 4.0.0) ## cli 2.1.0 2020-10-12 [1] CRAN (R 4.0.2) ## colorspace 1.4-1 2019-03-18 [1] CRAN (R 4.0.0) ## corrplot * 0.84 2017-10-16 [1] CRAN (R 4.0.0) ## crayon 1.3.4 2017-09-16 [1] CRAN (R 4.0.0) ## data.table * 1.13.2 2020-10-19 [1] CRAN (R 4.0.2) ## desc 1.2.0 2018-05-01 [1] CRAN (R 4.0.0) ## devtools 2.3.0 2020-04-10 [1] CRAN (R 4.0.0) ## digest 0.6.27 2020-10-24 [1] CRAN (R 4.0.2) ## dplyr * 1.0.2 2020-08-18 [1] CRAN (R 4.0.2) ## e1071 * 1.7-3 2019-11-26 [1] CRAN (R 4.0.0) ## ellipsis 0.3.1 2020-05-15 [1] CRAN (R 4.0.0) ## evaluate 0.14 2019-05-28 [1] CRAN (R 4.0.0) ## fansi 0.4.1 2020-01-08 [1] CRAN (R 4.0.0) ## fBasics * 3042.89.1 2020-03-07 [1] CRAN (R 4.0.2) ## fs 1.4.1 2020-04-04 [1] CRAN (R 4.0.0) ## generics 0.0.2 2018-11-29 [1] CRAN (R 4.0.0) ## ggplot2 * 3.3.0 2020-03-05 [1] CRAN (R 4.0.0) ## glue 1.4.2 2020-08-27 [1] CRAN (R 4.0.2) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 4.0.0) ## htmltools 0.5.0 2020-06-16 [1] CRAN (R 4.0.2) ## htmlwidgets 1.5.2 2020-10-03 [1] CRAN (R 4.0.2) ## httr 1.4.2 2020-07-20 [1] CRAN (R 4.0.2) ## jsonlite 1.7.1 2020-09-07 [1] CRAN (R 4.0.2) ## knitr 1.30 2020-09-22 [1] CRAN (R 4.0.0) ## lattice * 0.20-41 2020-04-02 [1] CRAN (R 4.0.0) ## lazyeval 0.2.2 2019-03-15 [1] CRAN (R 4.0.0) ## lifecycle 0.2.0 2020-03-06 [1] CRAN (R 4.0.0) ## lubridate * 1.7.8 2020-04-06 [1] CRAN (R 4.0.0) ## magrittr * 1.5 2014-11-22 [1] CRAN (R 4.0.0) ## memoise 1.1.0 2017-04-21 [1] CRAN (R 4.0.0) ## mnormt 1.5-7 2020-04-30 [1] CRAN (R 4.0.0) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 4.0.0) ## nlme 3.1-147 2020-04-13 [1] CRAN (R 4.0.0) ## openxlsx * 4.1.5 2020-05-06 [1] CRAN (R 4.0.0) ## pillar 1.4.6 2020-07-10 [1] CRAN (R 4.0.2) ## pkgbuild 1.1.0 2020-07-13 [1] CRAN (R 4.0.2) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.0.0) ## pkgload 1.1.0 2020-05-29 [1] CRAN (R 4.0.2) ## plotly * 4.9.2.1 2020-04-04 [1] CRAN (R 4.0.2) ## plyr * 1.8.6 2020-03-03 [1] CRAN (R 4.0.0) ## prettyunits 1.1.1 2020-01-24 [1] CRAN (R 4.0.0) ## pROC * 1.16.2 2020-03-19 [1] CRAN (R 4.0.0) ## processx 3.4.4 2020-09-03 [1] CRAN (R 4.0.2) ## ps 1.4.0 2020-10-07 [1] CRAN (R 4.0.2) ## psych * 1.9.12.31 2020-01-08 [1] CRAN (R 4.0.0) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.0.0) ## R6 2.5.0 2020-10-28 [1] CRAN (R 4.0.0) ## Rcpp 1.0.5.4 2020-11-05 [1] local ## readxl * 1.3.1 2019-03-13 [1] CRAN (R 4.0.0) ## remotes 2.1.1 2020-02-15 [1] CRAN (R 4.0.0) ## reshape * 0.8.8 2018-10-23 [1] CRAN (R 4.0.0) ## reshape2 * 1.4.4 2020-04-09 [1] CRAN (R 4.0.0) ## rlang 0.4.8 2020-10-08 [1] CRAN (R 4.0.2) ## rmarkdown 2.5 2020-10-21 [1] CRAN (R 4.0.0) ## ROCR * 1.0-11 2020-05-02 [1] CRAN (R 4.0.0) ## rprojroot 1.3-2 2018-01-03 [1] CRAN (R 4.0.0) ## rstudioapi 0.11 2020-02-07 [1] CRAN (R 4.0.0) ## scales 1.1.1 2020-05-11 [1] CRAN (R 4.0.0) ## sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 4.0.0) ## spatial 7.3-11 2015-08-30 [1] CRAN (R 4.0.0) ## stringi 1.5.3 2020-09-09 [1] CRAN (R 4.0.2) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.0.0) ## testthat 2.3.2 2020-03-02 [1] CRAN (R 4.0.0) ## tibble 3.0.4 2020-10-12 [1] CRAN (R 4.0.2) ## tidyr * 1.1.0 2020-05-20 [1] CRAN (R 4.0.0) ## tidyselect 1.1.0 2020-05-11 [1] CRAN (R 4.0.0) ## timeDate * 3043.102 2018-02-21 [1] CRAN (R 4.0.0) ## timeSeries * 3062.100 2020-01-24 [1] CRAN (R 4.0.2) ## usethis 1.6.1 2020-04-29 [1] CRAN (R 4.0.0) ## vctrs 0.3.4 2020-08-29 [1] CRAN (R 4.0.2) ## viridisLite 0.3.0 2018-02-01 [1] CRAN (R 4.0.0) ## withr 2.3.0 2020-09-22 [1] CRAN (R 4.0.2) ## xfun 0.18 2020-09-29 [1] CRAN (R 4.0.2) ## yaml 2.2.1 2020-02-01 [1] CRAN (R 4.0.0) ## zip 2.0.4 2019-09-01 [1] CRAN (R 4.0.0) ## zoo * 1.8-8 2020-05-02 [1] CRAN (R 4.0.2) ## ## [1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library "],["ch1.html", "第1章：统计分析与R语言", " 第1章：统计分析与R语言 本章从统计分析入手，首先介绍了统计分析的四个主要步骤：了解业务问题、数据收集及清洗、数据描述及探索分析，以及模型构建及解读。在统计分析实务中，以上四个步骤必不可少。接下来，在1.2节中介绍了R语言，包括R的下载与安装、如何使用以及如何用好R这一利器。这也是本书接下来各章节中配套使用的编程语言。R语言具备使用广泛、扩展便捷等诸多优势。 "],["ch2.html", "第2章：R语言中的数据管理与预处理 案例引入 2.1 基本数据类型 2.2 数据结构 2.3 数据的读入与写出 2.4 数据集管理及预处理 习题答案", " 第2章：R语言中的数据管理与预处理 案例引入 电影是大众喜闻乐见的一种艺术形式。如今，电影市场中百花齐放，喜剧片、动作片、科幻片、动画片等电影类型争奇斗艳，各类元素文化在电影中都得以展现。 随着我国人民生活水平的提高，电影也成为不少家庭娱乐项目中的一个重要组成部分。从2008年的票房冠军《赤壁上》（2.7亿票房），再到2019年的票房冠军《哪吒之魔童降世》（49.3亿票房），中国电影票房一直保持快速增长态势。据《全球电影产业发展报告(2019)》统计数据显示，中国电影产业于2018年发展为全球第二。中国电影市场蓬勃发展的同时，也因为前景光明，利润巨大，从而导致行业规范模糊，从业人员鱼龙混杂，以至于烂片层出不穷，受观众诟病。电影品质良莠不齐，有的电影在时间的长河中历久弥新，有的电影却消失得无声无息。高评分电影反映了观众对于不同题材的喜好，针对高评分电影进行统计分析可以为日后电影拍摄提供有效支持。 本章节采用的是某电影榜单上排名top250的电影数据，数据集包含250部电影的名称、评分等数据。数据的变量说明表如下所示： 变量类型 变量名 详细说明 取值范围 属性 score 电影评分 [8.3,9.6] type 影片类型 爱情、动作、动画等 duration 电影时长（分钟） [84,131] rank 电影排名 [1,250] nation 制片国家/地区 中国，美国，英国等 档期 showtime 电影上映时期 [1931/1/30,2017/11/24] Year 电影上映年份 [1931,2017] 导演基本信息 director 导演名字 导演名字 rm(list = ls())#清空环境 movie &lt;- read.csv(&quot;./data/top250.csv&quot;,fileEncoding = &quot;gbk&quot;)#读取数据 head(movie)#电影数据示例 ## rank name showtime duration ## 1 1 肖申克的救赎 1994/9/10 142 ## 2 2 霸王别姬 1993/1/1 171 ## 3 3 这个杀手不太冷 1994/9/14 110 ## 4 4 阿甘正传 1994/6/23 142 ## 5 5 美丽人生 1997/12/20 116 ## 6 6 千与千寻 2001/7/20 125 ## director type score nation ## 1 弗兰克·德拉邦特 剧情 9.6 美国 ## 2 陈凯歌 爱情 9.5 中国 ## 3 吕克·贝松 动作 9.4 法国 ## 4 Robert 爱情 9.4 美国 ## 5 罗伯托·贝尼尼 喜剧 9.5 意大利 ## 6 宫崎骏 动画 9.2 日本 2.1 基本数据类型 2.1.1 数值型 数值型变量是一种定量数据类型，这类数据的取值是连续的。例如，电影数据集中的评分(score)就是用数值类型存放的。可以通过如下方式查看这一列对应的数据类型并对数值型数据进行加减乘除运算。 class(movie$score)#查看数据类型 ## [1] &quot;numeric&quot; #自己为变量附一个数值 a=2;class(a) ## [1] &quot;numeric&quot; exp(1000) # 正无穷 ## [1] Inf -10 / 0 # 负无穷 ## [1] -Inf exp(1000) / exp(990) # NaN类型 ## [1] NaN exp(10) ## [1] 22026.47 2.1.2 字符型 字符型变量指用于存储文字的变量类型。在R语言中，用单引号或双引号定义的即是字符型数据。 #字符的定义 a = &quot;2&quot; class(a) ## [1] &quot;character&quot; # 判断电影数据集中，变量“类型(type)&quot;,&quot;电影名称(name)&quot;是不是字符型变量 class(movie$name) ## [1] &quot;character&quot; class(movie$type) ## [1] &quot;character&quot; 2.1.3 逻辑型 逻辑型数据即取值为TRUE或者FALSE的数据类型。 # 读入数据时设置把字符数据保留，不转换为factor movie = read.csv(&quot;./data/top250.csv&quot;, header = T, stringsAsFactors = F, fileEncoding = &quot;gbk&quot;) movie$type[movie$name == &quot;霸王别姬&quot;] == &quot;爱情&quot; ## [1] TRUE # 想在数据集中挑选大于9分的喜剧电影名称（name）？ movie$name[movie$type == &quot;喜剧&quot; &amp; movie$score &gt; 9] ## [1] &quot;美丽人生&quot; &quot;三傻大闹宝莱坞&quot; ## [3] &quot;触不可及&quot; &quot;两杆大烟枪&quot; # 逻辑语句加减 (1 == 2) + (3 &lt; 4) ## [1] 1 2.1.4 因子型数据 因子型数据是R语言中比较特殊的一个数据类型，它用于存储类别型变量。例如：性别（男性、女性），年龄分段（未成年人、成年人）等。除存储取值水平无序的类别型变量外，因子型数据还可以设置类别变量各水平的次序。因子型数据可使用命令factor()来定义。 ## 1.什么是因子型数据 ## (genders = factor(c(&quot;男&quot;, &quot;女&quot;, &quot;女&quot;, &quot;男&quot;, &quot;男&quot;)))#生成因子型变量 ## [1] 男 女 女 男 男 ## Levels: 女 男 (class = factor(c(&quot;Poor&quot;, &quot;Improved&quot;, &quot;Excellent&quot;), ordered = T))#设置因子水平高低 ## [1] Poor Improved Excellent ## Levels: Excellent &lt; Improved &lt; Poor ## 2.改变因子型数据各水平的编码顺序 ## (class = factor(c(&quot;Poor&quot;, &quot;Improved&quot;, &quot;Excellent&quot;), ordered = T, levels = c(&quot;Poor&quot;, &quot;Improved&quot;, &quot;Excellent&quot;))) ## [1] Poor Improved Excellent ## Levels: Poor &lt; Improved &lt; Excellent ## 3.因子型和字符型数据互相转换 ## # 输入原始字符变量 all = c(&quot;男&quot;, &quot;女&quot;, &quot;女&quot;, &quot;男&quot;, &quot;男&quot;) # 将字符型变量变成因子型 gender = as.factor(all) # 变换后的数据类型 is.factor(gender) ## [1] TRUE class(gender)#判断gender的数据类型 ## [1] &quot;factor&quot; 2.2 数据结构 rm(list = ls())#清理工作空间 movie = read.csv(&quot;./data/top250.csv&quot;,fileEncoding = &quot;gbk&quot;,stringsAsFactors = FALSE)#导入数据 2.2.1 向量 向量(vector)是所有数据结构中最基础的形式，用于存储同一种类型数据的一维数组。 向量的基本操作包括向量创建、向量的索引提取以及集合的运算： ## (1)向量创建## c(1, 1, 1, 2, 3, 3, 1, 2, 4, 1, 2, 4, 4, 2, 3, 4, 1, 2, 3, 4) ## [1] 1 1 1 2 3 3 1 2 4 1 2 4 4 2 3 4 1 2 3 4 c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; # seq(起始值, 终止值, 步长) seq(0, 10, by = 2)#生产0-10之间以2为间隔的等差数列 ## [1] 0 2 4 6 8 10 1:10#生成1—10的连续数列 ## [1] 1 2 3 4 5 6 7 8 9 10 ##(2)向量索引## x&lt;-c(1, 1, 1, 2, 3, 3)#生成向量 x[5]# 引用x向量中的第5个元素 ## [1] 3 which(x == 3)#查看x向量中3所在的位置 ## [1] 5 6 which.max(x)#查看x向量中最大值所在的位置 ## [1] 5 which.min(x)#查看x向量中最小值所在的位置 ## [1] 1 ##(3)集合运算## intersect(c(1, 2, 3, 3, 12, 4, 123, 12), c(1, 2, 3))#求交集 ## [1] 1 2 3 union(c(&quot;狗熊会&quot;, &quot;聚数据英才&quot;), c(&quot;狗熊会&quot;, &quot;助产业振兴&quot;))#求并集 ## [1] &quot;狗熊会&quot; &quot;聚数据英才&quot; &quot;助产业振兴&quot; setdiff(10:2, 5:3)#求差集 ## [1] 10 9 8 7 6 2 常见的向量类型包括数值型向量、字符串向量，这里分别介绍两种向量类型的基本操作。 （1）数值型向量 以下介绍match(), cut(), sort(), order()函数： ## 数值型向量 ## x&lt;-c(10,6,4,7,8)#创建数值向量 min(x)#求最小值 ## [1] 4 max(x)#求最大值 ## [1] 10 range(x)#求范围 ## [1] 4 10 # match函数 x &lt;- c(1, 1, 1, 2, 3, 3, 1, 2, 4, 1, 2, 4, 4, 2, 3, 4, 1, 2, 3, 4) (y &lt;- letters[x]) # letters是一个内置字符串，里面储存26个字母字符 ## [1] &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;c&quot; &quot;a&quot; &quot;b&quot; &quot;d&quot; &quot;a&quot; &quot;b&quot; &quot;d&quot; ## [13] &quot;d&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; match(y, letters[1:4]) ## [1] 1 1 1 2 3 3 1 2 4 1 2 4 4 2 3 4 1 2 3 4 x &lt;- c(&quot;a&quot;, &quot;c&quot;, &quot;g&quot;, &quot;h&quot;) letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; ## [13] &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; ## [25] &quot;y&quot; &quot;z&quot; match(x, letters) ## [1] 1 3 7 8 # cut函数 (Age = c(72,21,39,74,62,76,64,43,94,44,87,43,42,35,39,46,45,33,24,38))#生成向量 ## [1] 72 21 39 74 62 76 64 43 94 44 87 43 42 35 39 46 ## [17] 45 33 24 38 # 将年龄数据离散化 label = c(&#39;青年&#39;, &#39;中年&#39;, &#39;老年&#39;) #设置标签 (ages = cut(Age, breaks = c(20, 35, 50, 100), labels = label))#划分区间 ## [1] 老年 青年 中年 老年 老年 老年 老年 中年 老年 ## [10] 中年 老年 中年 中年 青年 中年 中年 中年 青年 ## [19] 青年 中年 ## Levels: 青年 中年 老年 # sort和order函数 (x = c(1,5,4,6,7))#生成向量 ## [1] 1 5 4 6 7 sort(x) ## [1] 1 4 5 6 7 order(x) ## [1] 1 3 2 4 5 x[order(x)] ## [1] 1 4 5 6 7 （2）字符串向量 以下介绍nchar(), cutsubstr, paste(), grep(), gsub()函数： # nchar用来提取字符串的长度 nchar(&quot;欢迎关注狗熊会&quot;) ## [1] 7 # 看看数据集中的电影名字的长度分别是多少 nchar(movie$name) ## [1] 6 4 7 4 4 4 5 6 4 6 5 7 7 6 9 ## [16] 5 2 4 2 4 8 4 3 6 5 4 4 5 8 4 ## [31] 4 3 2 4 4 9 5 5 4 5 5 4 5 7 3 ## [46] 5 3 4 3 4 4 4 2 4 4 5 3 4 5 6 ## [61] 4 9 3 3 4 4 4 6 8 6 4 8 6 4 5 ## [76] 3 4 4 6 4 4 4 7 4 3 4 4 3 2 2 ## [91] 7 10 9 4 3 2 3 5 2 4 4 5 4 8 3 ## [106] 2 5 3 7 4 5 4 3 4 5 4 4 5 4 10 ## [121] 7 4 4 3 4 2 4 4 8 5 2 2 4 4 7 ## [136] 5 3 4 4 4 5 5 5 6 7 13 4 3 5 4 ## [151] 5 7 4 4 6 6 3 4 3 4 6 5 5 3 5 ## [166] 2 4 2 3 3 3 2 4 2 4 5 4 4 7 4 ## [181] 5 8 6 4 3 5 4 5 5 4 9 3 6 10 5 ## [196] 2 5 2 4 6 4 4 4 4 3 5 8 5 4 5 ## [211] 13 4 2 4 4 12 8 2 4 4 5 6 4 3 5 ## [226] 7 5 5 2 2 4 5 5 5 5 2 4 2 4 7 ## [241] 8 4 7 3 2 4 4 5 4 8 # 中英文的字符长度计算方法有不同 nchar(&quot;Welcome to follow the CluBear&quot;) ## [1] 29 # substr提取子字符串 substr(&quot;欢迎关注狗熊会&quot;, 1, 4) ## [1] &quot;欢迎关注&quot; substr(&quot;一懒众衫小&quot;, 3, 5) ## [1] &quot;众衫小&quot; # paste基本玩法 paste(c(&quot;双11&quot;, &quot;是个&quot;, &quot;什么节日&quot;), collapse = &quot;&quot;) ## [1] &quot;双11是个什么节日&quot; paste(&quot;A&quot;, 1:4) ## [1] &quot;A 1&quot; &quot;A 2&quot; &quot;A 3&quot; &quot;A 4&quot; # paste花式玩法 paste(1:4, collapse = &quot;&quot;) ## [1] &quot;1234&quot; paste(1:4, sep=&quot;&quot;) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; paste(&quot;A&quot;, 1:4, sep=&quot;_&quot;) ## [1] &quot;A_1&quot; &quot;A_2&quot; &quot;A_3&quot; &quot;A_4&quot; txt = c(&quot;狗熊会&quot;, &quot;CluBear&quot;, &quot;双11&quot;, &quot;生日&quot;) # 返回含有关键字的字符位置 grep(&quot;Bear&quot;, txt) ## [1] 2 gsub(&quot;生日&quot;, &quot;happy birthday&quot;, txt) ## [1] &quot;狗熊会&quot; &quot;CluBear&quot; ## [3] &quot;双11&quot; &quot;happy birthday&quot; # grep返回movie的director中包含“青春”的行号2，movie[2, ]即提取出movie数据集的第2行 (index &lt;- grep(&quot;陈凯歌&quot;, movie$director)) ## [1] 2 (cmovie &lt;- movie[index, ]) ## rank name showtime duration director type ## 2 2 霸王别姬 1993/1/1 171 陈凯歌 爱情 ## score nation ## 2 9.5 中国 salary = c(&quot;22万&quot;, &quot;30万&quot;, &quot;50万&quot;, &quot;120万&quot;, &quot;11万&quot;) (salary0 = gsub(&quot;万&quot;, &quot;0000&quot;, salary)) ## [1] &quot;220000&quot; &quot;300000&quot; &quot;500000&quot; &quot;1200000&quot; ## [5] &quot;110000&quot; mean(as.numeric(salary0)) ## [1] 466000 median(as.numeric(salary0)) # 结果是科学计数法的形式 ## [1] 3e+05 2.2.2 矩阵 上一部分讲述的向量只能够展示一维数据信息。如果想要展示二维数据信息，则需要用到矩阵的组织形式。矩阵(matrix)是一个二维数组，矩阵每一个元素的数据类型相同。 首先，这里给出矩阵的创建与引用示例： ##(1)创建## # 生成全部是0的矩阵 (zero = matrix(1:9, nrow = 3, ncol = 3)) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 # 生成一个对角全是1的矩阵,直接在diag中输入对角线向量即可 (dig14 = diag(rep(1, 4))) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 ##(2)创建## # 从已有数据转化成矩阵 (M = matrix(1:12, nrow = 3, ncol = 4)) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 # 生成指定对角元素的对角矩阵 (N = diag(1:4)) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 2 0 0 ## [3,] 0 0 3 0 ## [4,] 0 0 0 4 下面介绍基本的矩阵操作方法： ## 矩阵概览 ## # 查看矩阵的维度 dim(M) ## [1] 3 4 # 提取矩阵的行数 nrow(M) ## [1] 3 # 提取矩阵的列数 ncol(M) ## [1] 4 ## [1] 4 # 引用元素 M[1, 2] ## [1] 4 M[1:2, 2:3] ## [,1] [,2] ## [1,] 4 7 ## [2,] 5 8 # 给行列命名 colnames(M) = paste(&quot;x_&quot;, 1:4) rownames(M) = 1:3; M ## x_ 1 x_ 2 x_ 3 x_ 4 ## 1 1 4 7 10 ## 2 2 5 8 11 ## 3 3 6 9 12 # 同样的命令可调用行列名 colnames(M) ## [1] &quot;x_ 1&quot; &quot;x_ 2&quot; &quot;x_ 3&quot; &quot;x_ 4&quot; rownames(M) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; # 将多个矩阵合并 (A = matrix(1:9, nrow = 3, ncol = 3, byrow = T)) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 (B = diag(11:13)) ## [,1] [,2] [,3] ## [1,] 11 0 0 ## [2,] 0 12 0 ## [3,] 0 0 13 rbind(A, B) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 11 0 0 ## [5,] 0 12 0 ## [6,] 0 0 13 cbind(A, B) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 3 11 0 0 ## [2,] 4 5 6 0 12 0 ## [3,] 7 8 9 0 0 13 除了上述的基本操作，这里还介绍常用的矩阵数学计算操作： A + B #矩阵的加法 ## [,1] [,2] [,3] ## [1,] 12 2 3 ## [2,] 4 17 6 ## [3,] 7 8 22 A - B #矩阵的减法 ## [,1] [,2] [,3] ## [1,] -10 2 3 ## [2,] 4 -7 6 ## [3,] 7 8 -4 A * B #矩阵各元素对应相乘 ## [,1] [,2] [,3] ## [1,] 11 0 0 ## [2,] 0 60 0 ## [3,] 0 0 117 A %*% B #矩阵的乘法 ## [,1] [,2] [,3] ## [1,] 11 24 39 ## [2,] 44 60 78 ## [3,] 77 96 117 solve(B) #矩阵B的逆 ## [,1] [,2] [,3] ## [1,] 0.09090909 0.00000000 0.00000000 ## [2,] 0.00000000 0.08333333 0.00000000 ## [3,] 0.00000000 0.00000000 0.07692308 eigen(B) #矩阵B的特征值 ## eigen() decomposition ## $values ## [1] 13 12 11 ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0 0 1 ## [2,] 0 1 0 ## [3,] 1 0 0 2.2.3 数组 数组(array)是向量和矩阵的推广，用于表达三维或者三维以上的数据。 ##1.创建及引用## # 创建数组 (result &lt;- array(1:18,dim=c(3,3,2),dimnames = list(c(&quot;r1&quot;,&quot;r2&quot;,&quot;r3&quot;),c(&quot;c1&quot;,&quot;c2&quot;,&quot;c3&quot;),c(&quot;h1&quot;,&quot;h2&quot;)))) ## , , h1 ## ## c1 c2 c3 ## r1 1 4 7 ## r2 2 5 8 ## r3 3 6 9 ## ## , , h2 ## ## c1 c2 c3 ## r1 10 13 16 ## r2 11 14 17 ## r3 12 15 18 result[1,2,2]#获取单个元素 ## [1] 13 result[1,,] #获取第一维度的数据 ## h1 h2 ## c1 1 10 ## c2 4 13 ## c3 7 16 ##2. 操作数组元素## matrix1&lt;-result[,,1]#获取数组中第1水平的矩阵 matrix2&lt;-result[,,2]#获取数组中第2水平的矩阵 (add&lt;-matrix1+matrix2)#矩阵相加 ## c1 c2 c3 ## r1 11 17 23 ## r2 13 19 25 ## r3 15 21 27 2.2.4 数据框 数据框(dataframe)是实际数据处理中最常用的数据结构形式。数据框的每一行可以存储一条数据记录，每一列可以存储不同类型的变量。 首先介绍如何创建数据框： ### 1.创建数据框 ### # 读入一个txt,csv等格式数据,即自成一个数据框 movie &lt;- read.csv(&quot;./data/top250.csv&quot;, fileEncoding = &quot;gbk&quot;, stringsAsFactors = F) class(movie) ## [1] &quot;data.frame&quot; # 自己创建 director &lt;- c(&quot;陈凯歌&quot;, &quot;宫崎骏&quot;, &quot;李廷香&quot;,&quot;詹姆斯·卡梅隆&quot;, &quot;刘镇伟&quot;, &quot;周星驰&quot;, &quot;李安&quot;, &quot;姜文&quot;, &quot;张艺谋&quot;, &quot;吴宇森&quot;,&quot;岩井俊二&quot;, &quot;王家卫&quot;, &quot;陈可辛&quot; ) birthyear &lt;- c(1952,1941,1964,1954,1952,1962,1954,1963,1950,1946,1963,1958,1962) gender &lt;- c(&quot;男&quot;, &quot;男&quot;, &quot;女&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;) directors &lt;- data.frame(director, birthyear, gender); head(directors) ## director birthyear gender ## 1 陈凯歌 1952 男 ## 2 宫崎骏 1941 男 ## 3 李廷香 1964 女 ## 4 詹姆斯·卡梅隆 1954 男 ## 5 刘镇伟 1952 男 ## 6 周星驰 1962 男 下面介绍数据框的变形——长宽表转换的操作方法： library(reshape2) ## (1) 宽表变长表 ## mWide = data.frame(Name = c(&quot;A&quot;, &quot;B&quot;), Type = c(&quot;喜剧&quot;, &quot;动作&quot;), GF2018 = c(6.5, 8.0), GF2019 = c(7.0, 7.5), GF2020 = c(8.1, 7.3)) # 由于构造数据框时列名不可以为纯数字，在数字前添加GF # 将列名中的GF去掉 colnames(mWide)[3:5] = gsub(&quot;GF&quot;, &quot;&quot;, colnames(mWide)[3:5]) mWide #查看原表 ## Name Type 2018 2019 2020 ## 1 A 喜剧 6.5 7.0 8.1 ## 2 B 动作 8.0 7.5 7.3 (mLong = reshape2::melt(mWide, id.vars = c(&quot;Name&quot;, &quot;Type&quot;), variable.name = &quot;Year&quot;)) ## Name Type Year value ## 1 A 喜剧 2018 6.5 ## 2 B 动作 2018 8.0 ## 3 A 喜剧 2019 7.0 ## 4 B 动作 2019 7.5 ## 5 A 喜剧 2020 8.1 ## 6 B 动作 2020 7.3 ## (2) 长表变宽表 ## # 长表变宽表 dcast(mLong, Name + Type ~ Year) ## Name Type 2018 2019 2020 ## 1 A 喜剧 6.5 7.0 8.1 ## 2 B 动作 8.0 7.5 7.3 使用**ply族函数可以在R中对数据框进行向量化操作，从而实现数据透视表的功能，下面对此进行介绍。 library(dplyr) # 根据电影类型进行分组，查看不同类型电影评分的平均水平 popular_type_grouped=group_by(movie,type)#根据电影类型进行分组 popular_type1=summarise(popular_type_grouped, mean_score=mean(score),#计算不同类型的平均评分 max_score=max(score))#计算不同类型的最高评分 head(popular_type1) ## # A tibble: 6 x 3 ## type mean_score max_score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 爱情 8.22 9.5 ## 2 传记 7.97 9.1 ## 3 动画 7.31 9.2 ## 4 动作 8.01 9.4 ## 5 犯罪 7.43 9.6 ## 6 歌舞 8.95 9 #利用管道函数%&gt;%省去中间变量命名 popular_type2 = movie%&gt;%group_by(type)%&gt;% summarise(mean_score=mean(score),#计算不同类型的平均评分 max_score=max(score))#计算不同类型的最高评分 head(popular_type2) ## # A tibble: 6 x 3 ## type mean_score max_score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 爱情 8.22 9.5 ## 2 传记 7.97 9.1 ## 3 动画 7.31 9.2 ## 4 动作 8.01 9.4 ## 5 犯罪 7.43 9.6 ## 6 歌舞 8.95 9 2.2.5 列表 列表（list）是R语言中可以容纳各种类型的数据对象，如向量、矩阵、数据框，甚至一个列表也可以成为另一个列表的元素。 首先介绍列表的创建方法与基本操作： ##1.创建## (example = list(&quot;abc&quot;, 3:5, matrix(1, nrow = 3, ncol = 4), data.frame(x = 1:4, y = paste0(&quot;boy_&quot;, 1:4)))) ## [[1]] ## [1] &quot;abc&quot; ## ## [[2]] ## [1] 3 4 5 ## ## [[3]] ## [,1] [,2] [,3] [,4] ## [1,] 1 1 1 1 ## [2,] 1 1 1 1 ## [3,] 1 1 1 1 ## ## [[4]] ## x y ## 1 1 boy_1 ## 2 2 boy_2 ## 3 3 boy_3 ## 4 4 boy_4 ##2.基本操作## # 查看 (complex = list(first = list(1:2), second = list(letters, list(matrix(1:4, nrow = 2, ncol = 2))))) ## $first ## $first[[1]] ## [1] 1 2 ## ## ## $second ## $second[[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; ## ## $second[[2]] ## $second[[2]][[1]] ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 str(complex) ## List of 2 ## $ first :List of 1 ## ..$ : int [1:2] 1 2 ## $ second:List of 2 ## ..$ : chr [1:26] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ... ## ..$ :List of 1 ## .. ..$ : int [1:2, 1:2] 1 2 3 4 # 利用名字引用元素 complex$first ## [[1]] ## [1] 1 2 # 利用序号引用元素 complex[[1]] ## [[1]] ## [1] 1 2 # 利用名字添加元素 complex$new = 1:5; complex ## $first ## $first[[1]] ## [1] 1 2 ## ## ## $second ## $second[[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; ## ## $second[[2]] ## $second[[2]][[1]] ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## ## ## $new ## [1] 1 2 3 4 5 # 利用序号添加元素 complex[[3]] = matrix(1, 2, 3); complex ## $first ## $first[[1]] ## [1] 1 2 ## ## ## $second ## $second[[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; ## ## $second[[2]] ## $second[[2]][[1]] ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## ## ## $new ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 1 1 与数据框相似，对列表也可以使用**ply族函数进行操作： ##3. 列表中的**ply函数## # 老王耗子药的单价,单位(元/袋) (price = list(year2014 = 36:33, year2015 = 32:35, year2016 = 30:27)) ## $year2014 ## [1] 36 35 34 33 ## ## $year2015 ## [1] 32 33 34 35 ## ## $year2016 ## [1] 30 29 28 27 # lapply返回列表 lapply(price, mean) ## $year2014 ## [1] 34.5 ## ## $year2015 ## [1] 33.5 ## ## $year2016 ## [1] 28.5 # 求方差 lapply(price, sd) ## $year2014 ## [1] 1.290994 ## ## $year2015 ## [1] 1.290994 ## ## $year2016 ## [1] 1.290994 # 求分位数 lapply(price, quantile) ## $year2014 ## 0% 25% 50% 75% 100% ## 33.00 33.75 34.50 35.25 36.00 ## ## $year2015 ## 0% 25% 50% 75% 100% ## 32.00 32.75 33.50 34.25 35.00 ## ## $year2016 ## 0% 25% 50% 75% 100% ## 27.00 27.75 28.50 29.25 30.00 # sapply默认返回向量或矩阵 sapply(price, mean) ## year2014 year2015 year2016 ## 34.5 33.5 28.5 sapply(price, sd) ## year2014 year2015 year2016 ## 1.290994 1.290994 1.290994 sapply(price, quantile) ## year2014 year2015 year2016 ## 0% 33.00 32.00 27.00 ## 25% 33.75 32.75 27.75 ## 50% 34.50 33.50 28.50 ## 75% 35.25 34.25 29.25 ## 100% 36.00 35.00 30.00 # mapply实现了将price与amount对应元素相乘的效果 (amount = list(year2014 = rep(200, 4), year2015 = rep(100, 4), year2016 = rep(300, 4))) ## $year2014 ## [1] 200 200 200 200 ## ## $year2015 ## [1] 100 100 100 100 ## ## $year2016 ## [1] 300 300 300 300 (income_quarter = mapply(&quot;*&quot;, price, amount)) ## year2014 year2015 year2016 ## [1,] 7200 3200 9000 ## [2,] 7000 3300 8700 ## [3,] 6800 3400 8400 ## [4,] 6600 3500 8100 2.3 数据的读入与写出 rm(list = ls())#清理工作空间 movie = read.csv(&quot;./data/top250.csv&quot;,fileEncoding = &quot;gbk&quot;,stringsAsFactors = FALSE)#导入数据 2.3.1 使用键盘输入数据 当读入数据较少时，可通过键盘输入数据。主要方式分为：直接输入以及利用R内置表格编辑器输入。 scores &lt;- c(61,66,84,80,100) #scores &lt;- data.frame() #建立一个空数据框 #scores &lt;-edit(scores) #触发R内置编辑器 # 输入数据集 mydatatxt&lt;-&quot;name gender age 张三 M 20 李四 F 23 &quot; (mydata&lt;-read.table(header = TRUE,text = mydatatxt)) # 读取数据集 ## name gender age ## 1 张三 M 20 ## 2 李四 F 23 2.3.2 从带分隔符的文本文件导入数据 文本文件是一种常用的数据文件格式，函数read.table()可以从带分隔符的文本文件中导入数据，并生成一个数据框。常见的文本文件的数据格式为csv，读入csv文件可使用专有函数read.csv()。 ### 1. read.rable() ### #从txt中读入,分隔符为&quot;\\t&quot; tes = read.table(&quot;./data/top250.txt&quot;, header = TRUE, sep = &quot;\\t&quot;,fileEncoding = &quot;GBK&quot;); head(tes) ## rank name showtime duration director type score nation ## 1 1 肖申克的救赎 1994/9/10 142 弗兰克·德拉邦特 剧情 9.6 美国 ## 2 2 霸王别姬 1993/1/1 171 陈凯歌 爱情 9.5 中国 ## 3 3 这个杀手不太冷 1994/9/14 110 吕克·贝松 动作 9.4 法国 ## 4 4 阿甘正传 1994/6/23 142 Robert 爱情 9.4 美国 ## 5 5 美丽人生 1997/12/20 116 罗伯托·贝尼尼 喜剧 9.5 意大利 ## 6 6 千与千寻 2001/7/20 125 宫崎骏 动画 9.2 日本 ### 2. read.csv() ### #专用函数read.csv movie_csv = read.csv(&quot;./data/top250.csv&quot;,fileEncoding = &quot;GBK&quot;); head(movie_csv) ## rank name showtime duration director type score nation ## 1 1 肖申克的救赎 1994/9/10 142 弗兰克·德拉邦特 剧情 9.6 美国 ## 2 2 霸王别姬 1993/1/1 171 陈凯歌 爱情 9.5 中国 ## 3 3 这个杀手不太冷 1994/9/14 110 吕克·贝松 动作 9.4 法国 ## 4 4 阿甘正传 1994/6/23 142 Robert 爱情 9.4 美国 ## 5 5 美丽人生 1997/12/20 116 罗伯托·贝尼尼 喜剧 9.5 意大利 ## 6 6 千与千寻 2001/7/20 125 宫崎骏 动画 9.2 日本 2.3.3 导入Excel数据 library(&quot;readxl&quot;) # 加载包 # 其中col_names参数仍然是为了设定是否把第一行当做变量名 movie_excel = data.frame(read_excel(&quot;./data/top250.xlsx&quot;, col_names = TRUE));head(movie_excel) ## rank name showtime duration director type score nation ## 1 1 肖申克的救赎 34587 142 弗兰克·德拉邦特 剧情 9.6 美国 ## 2 2 霸王别姬 33970 171 陈凯歌 爱情 9.5 中国 ## 3 3 这个杀手不太冷 34591 110 吕克·贝松 动作 9.4 法国 ## 4 4 阿甘正传 34508 142 Robert 爱情 9.4 美国 ## 5 5 美丽人生 35784 116 罗伯托·贝尼尼 喜剧 9.5 意大利 ## 6 6 千与千寻 37092 125 宫崎骏 动画 9.2 日本 2.3.4 逐行读入数据 在文件较大或格式较为复杂的情况下，直接将文件读入内存会花费很长时间。因此，可以每次读入一行文件，进行逐行处理。 #建立与文件的连接 con&lt;-file(&quot;./data/top250.csv&quot;, encoding = &quot;GBK&quot;) #逐行读入所有数据 line_all&lt;-readLines(con) #读取前10行数据 line_10&lt;-readLines(con,n=10) line_10 ## [1] &quot;rank,name,showtime,duration,director,type,score,nation&quot; ## [2] &quot;1,肖申克的救赎,1994/9/10,142,弗兰克·德拉邦特,剧情,9.6,美国&quot; ## [3] &quot;2,霸王别姬,1993/1/1,171,陈凯歌,爱情,9.5,中国&quot; ## [4] &quot;3,这个杀手不太冷,1994/9/14,110,吕克·贝松,动作,9.4,法国&quot; ## [5] &quot;4,阿甘正传,1994/6/23,142,Robert,爱情,9.4,美国&quot; ## [6] &quot;5,美丽人生,1997/12/20,116,罗伯托·贝尼尼,喜剧,9.5,意大利&quot; ## [7] &quot;6,千与千寻,2001/7/20,125,宫崎骏,动画,9.2,日本&quot; ## [8] &quot;7,泰坦尼克号,1998/4/3,194,詹姆斯·卡梅隆,爱情,9.2,美国&quot; ## [9] &quot;8,辛德勒的名单,1993/11/30,195,史蒂文·斯皮尔伯格,历史,9.4,美国&quot; ## [10] &quot;9,盗梦空间,2010/9/1,148,克里斯托弗·诺兰,科幻,9.3,美国&quot; close(con)#关闭连接 split_line=strsplit(line_all,&quot;,&quot;)#分隔符为”,” head(split_line,3) ## [[1]] ## [1] &quot;rank&quot; &quot;name&quot; &quot;showtime&quot; &quot;duration&quot; &quot;director&quot; &quot;type&quot; &quot;score&quot; &quot;nation&quot; ## ## [[2]] ## [1] &quot;1&quot; &quot;肖申克的救赎&quot; &quot;1994/9/10&quot; &quot;142&quot; &quot;弗兰克·德拉邦特&quot; ## [6] &quot;剧情&quot; &quot;9.6&quot; &quot;美国&quot; ## ## [[3]] ## [1] &quot;2&quot; &quot;霸王别姬&quot; &quot;1993/1/1&quot; &quot;171&quot; &quot;陈凯歌&quot; &quot;爱情&quot; &quot;9.5&quot; &quot;中国&quot; 2.4 数据集管理及预处理 rm(list = ls())#清理工作空间 movie = read.csv(&quot;./data/top250.csv&quot;,fileEncoding = &quot;gbk&quot;,stringsAsFactors = FALSE)#导入数据 2.4.1 了解数据概况 拿到数据集后需要先查看数据的概况，主要通过R语言中的汇总函数实现。首先使用str()函数查看每列数据的类型，了解取值情况。接下来，可通过summary()函数查看每列数据的汇总统计。 str(movie) ## &#39;data.frame&#39;: 250 obs. of 8 variables: ## $ rank : int 1 2 3 4 5 6 7 8 9 10 ... ## $ name : chr &quot;肖申克的救赎&quot; &quot;霸王别姬&quot; &quot;这个杀手不太冷&quot; &quot;阿甘正传&quot; ... ## $ showtime: chr &quot;1994/9/10&quot; &quot;1993/1/1&quot; &quot;1994/9/14&quot; &quot;1994/6/23&quot; ... ## $ duration: int 142 171 110 142 116 125 194 195 148 98 ... ## $ director: chr &quot;弗兰克·德拉邦特&quot; &quot;陈凯歌&quot; &quot;吕克·贝松&quot; &quot;Robert&quot; ... ## $ type : chr &quot;剧情&quot; &quot;爱情&quot; &quot;动作&quot; &quot;爱情&quot; ... ## $ score : num 9.6 9.5 9.4 9.4 9.5 9.2 9.2 9.4 9.3 9.3 ... ## $ nation : chr &quot;美国&quot; &quot;中国&quot; &quot;法国&quot; &quot;美国&quot; ... summary(movie) ## rank name showtime duration director type ## Min. : 1.0 Length:250 Length:250 Min. : 45.0 Length:250 Length:250 ## 1st Qu.: 60.5 Class :character Class :character 1st Qu.:103.0 Class :character Class :character ## Median :126.5 Mode :character Mode :character Median :118.0 Mode :character Mode :character ## Mean :125.9 Mean :122.2 ## 3rd Qu.:190.0 3rd Qu.:136.0 ## Max. :250.0 Max. :238.0 ## NA&#39;s :12 ## score nation ## Min. :-1.000 Length:250 ## 1st Qu.: 8.500 Class :character ## Median : 8.700 Mode :character ## Mean : 7.853 ## 3rd Qu.: 8.900 ## Max. : 9.600 ## 2.4.2 变量类型转换 1.基本数据类型之间的转换 变量类型转换可以分为两步，首先利用is族函数判断变量类型，再通过as族函数转换变量类型。 a=&quot;1&quot;#将1赋值给a is.numeric(a)#判断a是否是数值型数据 ## [1] FALSE a&lt;-as.numeric(a)#将a转换为数值型数据 is.numeric(a)#再次判断a是否是数值型数据 ## [1] TRUE 2.不同结构化数据类型间的转换 结构化数据类型之间的转换与基本数据类型转换相似，首先使用class()函数判断数据结构，再使用as族函数进行转换。 tbl &lt;- table(movie$nation)#统计不同国家的频次 class(tbl)#查看数据格式 ## [1] &quot;table&quot; tbl &lt;- as.data.frame(tbl)#转化为数据框 class(tbl) ## [1] &quot;data.frame&quot; 3.日期值转换 日期类型的变量需要使用如下方法进行转换。 ###(1)将字符转换成Date日期格式### # 函数head用来查看数据前6个元素，函数class用来查看对象数据类型 head(movie$showtime) ## [1] &quot;1994/9/10&quot; &quot;1993/1/1&quot; &quot;1994/9/14&quot; &quot;1994/6/23&quot; &quot;1997/12/20&quot; &quot;2001/7/20&quot; class(movie$showtime) ## [1] &quot;character&quot; movie$showtime = as.Date(movie$showtime) head(movie$showtime) ## [1] &quot;1994-09-10&quot; &quot;1993-01-01&quot; &quot;1994-09-14&quot; &quot;1994-06-23&quot; &quot;1997-12-20&quot; &quot;2001-07-20&quot; class(movie$showtime) ## [1] &quot;Date&quot; as.Date(&#39;1/15/2020&#39;, format = &#39;%m/%d/%Y&#39;) #对日/月/年类型字符进行日期转换 ## [1] &quot;2020-01-15&quot; ###(2)将字符转换成POSIXct/POSIXlt时间格式### as.POSIXct(1472562988, origin = &quot;1960-01-01&quot;)#日期值转换，以&quot;1960-01-01&quot;为起点 ## [1] &quot;2006-08-30 21:16:28 CST&quot; 2.4.3 时间型数据的操作 针对时间型数据的常用操作为：特征提取、差值运算、排序运算。 1.特征提取 library(lubridate)#加载lubridate包 t = &quot;2020-11-20 01:30:29&quot; year(t)#提取年份 ## [1] 2020 month(t)#提取月份 ## [1] 11 mday(t)#提取日期是一个月中的第几天 ## [1] 20 wday(t)#提取日期是一周中的第几天 ## [1] 6 hour(t)#取出日期中的小时数 ## [1] 1 minute(t)#取出日期中的分钟数 ## [1] 30 second(t)#取出日期中的秒 ## [1] 29 2.插值运算 # 求任意两个日期距离的天数 begin = as.Date(&quot;2016-03-04&quot;) end = as.Date(&quot;2016-05-08&quot;) (during = end - begin) ## Time difference of 65 days # 求任意两个日期距离的周数和小时数 difftime(end, begin, units = &quot;weeks&quot;) ## Time difference of 9.285714 weeks difftime(end, begin, units = &quot;hours&quot;) ## Time difference of 1560 hours 3.排序 # 单独对时间进行排序 head(sort(movie$showtime)) ## [1] &quot;1931-01-30&quot; &quot;1936-02-25&quot; &quot;1939-12-15&quot; &quot;1940-05-17&quot; &quot;1950-08-26&quot; &quot;1952-04-11&quot; # 对数据表格中的数据按照时间顺序排列,这里只选取前6行，对电影名称、上映日期做展示 head(movie[order(movie$showtime), c(&quot;name&quot;, &quot;showtime&quot;)]) ## name showtime ## 214 城市之光 1931-01-30 ## 103 摩登时代 1936-02-25 ## 20 乱世佳人 1939-12-15 ## 147 魂断蓝桥 1940-05-17 ## 171 罗生门 1950-08-26 ## 157 雨中曲 1952-04-11 2.4.4 数据集合并 实践中，常常需要将不同表的信息进行合并。在R语言中可以使用函数merge()来实现数据框的合并。 director &lt;- c(&quot;陈凯歌&quot;, &quot;宫崎骏&quot;, &quot;李廷香&quot;,&quot;詹姆斯·卡梅隆&quot;, &quot;刘镇伟&quot;, &quot;周星驰&quot;, &quot;李安&quot;, &quot;姜文&quot;, &quot;张艺谋&quot;, &quot;吴宇森&quot;,&quot;岩井俊二&quot;, &quot;王家卫&quot;, &quot;陈可辛&quot; ) birthyear &lt;- c(1952,1941,1964,1954,1952,1962,1954,1963,1950,1946,1963,1958,1962) gender &lt;- c(&quot;男&quot;, &quot;男&quot;, &quot;女&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;, &quot;男&quot;) directors &lt;- data.frame(director, birthyear, gender); # merge实现的效果是：将movie和directors按照列director匹配并合并起来 (movie.star = merge(movie[1:10, ], directors,by = &quot;director&quot;)) ## director rank name showtime duration type score nation birthyear gender ## 1 陈凯歌 2 霸王别姬 1993-01-01 171 爱情 9.5 中国 1952 男 ## 2 宫崎骏 6 千与千寻 2001-07-20 125 动画 9.2 日本 1941 男 ## 3 詹姆斯·卡梅隆 7 泰坦尼克号 1998-04-03 194 爱情 9.2 美国 1954 男 2.4.5 数据缺失、异常 在R语言中检测缺失值可使用is.na()函数。该函数将会返回逻辑值TRUE或FALSE，TRUE代表缺失，FALSE代表未缺失。数据中的异常值为不符合常理或与总体取值高度不一致的数据。通常对数据是否缺失、异常的判断是在数据汇总之后进行的。 ###(1)删除法### movie_new=na.omit(movie)#保留完整观测的行 ###(2)插补法### #将均值替换电影时长缺失值 movie[is.na(movie$duration), ]$duration&lt;-mean(movie$duration, na.rm = T) movie[which(movie$score&lt;0), ]$score&lt;-NA#将异常值赋值为NA movie[is.na(movie$score), ]$score&lt;-mean(movie$score, na.rm = T)#赋值均值 习题答案 题目 2.1 如何理解R语言中的“向量化”操作？请举一个例子说明。 对于一个向量\\(x\\)，“向量化”操作指命令可以直接对向量的每个元素进行操作，不需要使用循环实现计算。比如命令语句\\(x\\)+2或者\\(x^3\\)，向量\\(x\\)中的每个元素都加2或每个元素都变成三次幂。 题目 2.2 请描述R语言中矩阵与数据框之间的两个不同点。 a.数据框实际上是由多个长度相同的向量组成，而矩阵实际上是一个二维向量。 b.数据框所包含的向量可以是不同数据类型的，而矩阵仅能包含一种数据类型。 题目 2.3 对矩阵的操作： 1.在R中生成生成下面的矩阵A。 \\[A=\\left(\\begin{array}{lll} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 2 &amp; 1 \\\\ 2 &amp; 3 &amp; 0 \\end{array}\\right)\\] #构造矩阵 (A=matrix(c(1,4,2,2,2,3,3,1,0),nrow=3)) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 2 1 ## [3,] 2 3 0 2.计算矩阵A的转置矩阵B和逆矩阵C。 (B=t(A))#求转置矩阵 ## [,1] [,2] [,3] ## [1,] 1 4 2 ## [2,] 2 2 3 ## [3,] 3 1 0 (C=solve(A))#求逆矩阵 ## [,1] [,2] [,3] ## [1,] -0.12 0.36 -0.16 ## [2,] 0.08 -0.24 0.44 ## [3,] 0.32 0.04 -0.24 3.求矩阵A和矩阵C的乘积。 A%*%C#矩阵乘积 ## [,1] [,2] [,3] ## [1,] 1.000000e+00 0 0 ## [2,] 5.551115e-17 1 0 ## [3,] 0.000000e+00 0 1 题目 2.4（实训题目） 实训题目：使用电视剧网播量数据集，该数据集收集了4266条电视剧的信息。请完成以下任务。 获取数据集，查看数据概况。 tv&lt;-read.csv(&quot;./data/电视剧播量.csv&quot;,fileEncoding=&quot;gbk&quot;,stringsAsFactors = F)#读取数据 head(tv)#查看数据前几行 ## 剧名 类型 播放量 点赞 差评 得分 采集日期 ## 1 花千骨2015 言情剧\\n/\\n穿越剧\\n/\\n网络剧 3.07亿 992,342 357,808 7.3 2015-9-23 23:48:48 ## 2 还珠格格2015 古装剧\\n\\n/\\n喜剧\\n\\n/\\n网络剧 73.3万 2,352 7,240 2.5 2015-9-23 23:48:48 ## 3 天局 武侠剧\\n/\\n古装剧\\n/\\n悬疑剧\\n/\\n网络剧 3454万 38,746 3,593 9.2 2015-9-23 23:48:52 ## 4 明若晓溪 青春剧\\n/\\n言情剧\\n/\\n偶像剧 1.57亿 518,660 72,508 8.8 2015-9-23 23:48:51 ## 5 多情江山 言情剧\\n/\\n古装剧\\n/\\n宫廷剧 1126万 22,553 6,955 7.6 2015-9-23 23:48:52 ## 6 我是机器人 偶像剧\\n\\n/\\n喜剧 1660万 37,905 3,605 9.1 2015-9-23 23:48:51 删除数据集中剧名缺失的值。 tv&lt;-tv[-which(tv$剧名 == &quot;null&quot;),]#删除缺失数据 不考虑缺失数据影响，计算电视剧的平均得分。 #将缺失数据处理成NA tv[which(tv$得分==&quot;null&quot;),]$得分=NA tv[which(tv$得分==&quot;.&quot;),]$得分=NA #转换数据格式 tv$得分&lt;-as.numeric(tv$得分) #计算平均得分 mean(tv$得分,na.rm = T) ## [1] 7.592556 题目 2.5 （实训题目） 实训题目：使用手机游戏数据集，该数据集收集了1141条手机游戏信息及评分。请完成以下任务。 获取数据集，查看数据概况。 game&lt;-read.csv(&quot;./data/安卓手机游戏.csv&quot;,fileEncoding = &quot;gbk&quot;,stringsAsFactors = F)#读取数据 summary(game)#查看数据概况 ## 游戏名称 评分 类别 语言 热度 最后更新时间 ## Length:1141 Min. :1.000 Length:1141 Length:1141 Length:1141 Length:1141 ## Class :character 1st Qu.:5.200 Class :character Class :character Class :character Class :character ## Mode :character Median :6.600 Mode :character Mode :character Mode :character Mode :character ## Mean :6.611 ## 3rd Qu.:7.900 ## Max. :9.600 ## NA&#39;s :203 ## 游戏版本 资费 开发商 支持系统 评论数 喜欢数 ## Length:1141 Length:1141 Length:1141 Length:1141 Min. : 0 Min. : 0.0 ## Class :character Class :character Class :character Class :character 1st Qu.: 446 1st Qu.: 67.0 ## Mode :character Mode :character Mode :character Mode :character Median : 859 Median : 233.0 ## Mean : 2322 Mean : 786.3 ## 3rd Qu.: 2510 3rd Qu.: 854.8 ## Max. :96354 Max. :13323.0 ## NA&#39;s :1 NA&#39;s :1 提取热度中的数值部分，计算各游戏类型的热度均值，找出平均热度最高的游戏类型。 #加载包 library(stringr) library(dplyr) #转换数据格式 game$hot&lt;-as.numeric(str_extract(game$热度,&quot;\\\\d+&quot;)) #计算热度均值 hot_aver&lt;-game%&gt;% group_by(类别)%&gt;% summarise(meanhot=mean(hot,na.rm = T))#平均热度 #找出平均热度最高的游戏类型 (hot_aver=hot_aver[order(hot_aver$meanhot, decreasing = T), ]) ## # A tibble: 16 x 2 ## 类别 meanhot ## &lt;chr&gt; &lt;dbl&gt; ## 1 &quot;体育运动&quot; 46.0 ## 2 &quot;竞速游戏&quot; 45.6 ## 3 &quot;游戏工具&quot; 43.6 ## 4 &quot;动作游戏&quot; 42.6 ## 5 &quot;养成游戏&quot; 42.3 ## 6 &quot;角色扮演&quot; 42.2 ## 7 &quot;益智休闲&quot; 41.8 ## 8 &quot;飞行游戏&quot; 41.6 ## 9 &quot;策略塔防&quot; 40.9 ## 10 &quot;音乐游戏&quot; 40.6 ## 11 &quot;格斗游戏&quot; 40.6 ## 12 &quot;射击游戏&quot; 40.0 ## 13 &quot;冒险解谜&quot; 39.5 ## 14 &quot;模拟经营&quot; 38.3 ## 15 &quot;棋牌游戏&quot; 37.1 ## 16 &quot;&quot; NaN 平均热度最高的游戏类别为体育运动。 计算各游戏类型的平均评分，最高评分，最低评分，评分标准差，并作简要分析。 grade_aver&lt;-game%&gt;% group_by(类别)%&gt;% summarise(meangrade=mean(评分,na.rm = T),#平均评分 maxgrade=max(评分,na.rm = T),#最高评分 mingrade=min(评分,na.rm = T),#最低评分 sdgrade=sd(评分,na.rm = T))#评分标准差 (grade_aver=grade_aver[order(grade_aver$meangrade, decreasing = T), ])#按照平均得分排序 ## # A tibble: 16 x 5 ## 类别 meangrade maxgrade mingrade sdgrade ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;角色扮演&quot; 7.00 9.1 4.6 1.33 ## 2 &quot;策略塔防&quot; 6.88 9.3 4.8 1.39 ## 3 &quot;冒险解谜&quot; 6.86 9 4.8 1.23 ## 4 &quot;体育运动&quot; 6.84 8.8 4.9 1.46 ## 5 &quot;格斗游戏&quot; 6.78 8.8 5 1.18 ## 6 &quot;模拟经营&quot; 6.67 9.3 4.7 1.28 ## 7 &quot;动作游戏&quot; 6.65 9 1 1.44 ## 8 &quot;射击游戏&quot; 6.61 8.6 4.7 1.30 ## 9 &quot;养成游戏&quot; 6.55 8.3 5 1.23 ## 10 &quot;益智休闲&quot; 6.46 9.6 4.6 1.31 ## 11 &quot;音乐游戏&quot; 6.4 9 5 1.34 ## 12 &quot;竞速游戏&quot; 6.37 8.8 4.7 1.35 ## 13 &quot;棋牌游戏&quot; 5.84 8.1 4.5 1.22 ## 14 &quot;飞行游戏&quot; 5.70 8.2 4.7 1.09 ## 15 &quot;游戏工具&quot; 5.03 6.4 1 1.63 ## 16 &quot;&quot; NaN -Inf Inf NA 分析：虽然平均热度最高的游戏类别为体育运动，但平均评分最高的游戏类别为角色扮演，同时角色扮演的评分波动较大。 "],["ch3.html", "第3章：基本统计分析 案例引入 3.1 基本描述统计量 3.2 交叉分组下的频数分析 习题答案", " 第3章：基本统计分析 案例引入 根据相关的行业报告，2018年中国在线短租用户规模达到1.47亿人，2020年有望突破3亿人，其中有大量租房需求的高校毕业生人数正在逐年上升。目前租赁房源出现向一线城市集中的趋势，随着90后“只租不买”的观念逐渐流行，以及租房政策的进一步完善，未来北上深等一线城市租房人群比例或将超过40%。在如此大规模的租房市场需求的驱动下，在线租房平台更是如雨后春笋般蓬勃发展。更多的年轻人不再通过中介租房，转而在线上平台选择房源。不仅如此，由于经济条件限制，租房的模式也发生了变化，由传统的整租转变为性价比更高的合租。 面对海量的房源，众多的合租房用户怎么才能找到物美价廉的合租房呢？这需要对租房市场深入调研。比如，需要了解：目前在租的房源价格一般水平是多少？不同城区之间差异如何？如果想租一间北京海淀区邻近地铁线的20平米左右的卧室，那么月租大概多少？这些问题都可以通过基本描述分析作答。 本案例的数据来源于某租房平台的租房数据集，共采集了北京市某年某月5149条合租房源的信息。具体变量说明表如下表所示： 变量类型 变量名 详细说明 取值范围 月租金 rent 月租金 [1150,6460] 租房信息 内部结构 area 租赁房间面积 [5,30] room 租赁房间类型 主卧、次卧 bedroom 卧室数 [2,5] livingroom 厅数 [1,2] bathroom 卫生间数 [1,2] heating 供暖方式 集中供暖、自采暖 外部条件 floor_grp 所在楼层分组 高楼层、中楼层、低楼层 subway 邻近地铁 是、否 region 城区 朝阳、海淀、东城、西城、昌平、大兴、通州、石景山、丰台、顺义、房山 rm(list = ls())#清理工作空间 tenement = read.csv(&quot;./data/rent.csv&quot;,fileEncoding = &quot;gbk&quot;,stringsAsFactors = FALSE)#导入数据 3.1 基本描述统计量 以下从频数统计开始，一一介绍均值、分位数、方差、标准差、最大最小值、峰度、偏度等基本统计量。 3.1.1 频数统计 频数（freguency）统计是对定性变量的各水平计数进行统计的描述统计量。 (type&lt;- table(tenement$region))#频数统计 ## ## 昌平 朝阳 大兴 东城 房山 丰台 海淀 石景山 顺义 通州 西城 ## 702 1317 361 94 183 581 424 258 314 819 96 tenement$regiontype=tenement$region#设置新变量 tenement[!is.element(tenement$region,c(&quot;朝阳&quot;,&quot;通州&quot;,&quot;昌平&quot;,&quot;丰台&quot;)),]$regiontype=&quot;其他&quot;#将非朝阳、通州、昌平和丰台的租赁房间合并为为其他 sort(table(tenement$regiontype)) ## ## 丰台 昌平 通州 朝阳 其他 ## 581 702 819 1317 1730 3.1.2 均值 均值（mean），即一组数值型数据的平均数，是最常用的统计量之一。 mean(tenement$rent)#求房租均值 ## [1] 2917.9 tenement_num=tenement[c(&quot;rent&quot;,&quot;area&quot;)]#选取数值型子集 colMeans(tenement_num)#同时计算列和 ## rent area ## 2917.89989 12.85143 3.1.3 分位数 在数据分布偏度较大的情况下，均值并不能很好地反映数据的一般水平。此时，可使用中位数（median）刻画数据的一般情形。中位数是分位数（quantile）的一个特殊情况。 median(tenement$rent)#求租金的中位数 ## [1] 2700 quantile(tenement$rent, probs = 0.5)#利用quantile()函数求中位数 ## 50% ## 2700 quantile(tenement$rent, probs = seq(0, 1, 0.25))#求向量最小值、最大值、四分位数 ## 0% 25% 50% 75% 100% ## 1343.001 2448.000 2700.000 3327.316 6547.042 3.1.4 方差和标准差 方差（variance）和标准差（standard deviation）是描述一组数值型数据的分散程度的描述统计量。协方差（covariance）和相关系数（correlation coefficient）是描述变量相关程度的描述统计量。 mean(tenement$rent[tenement$regiontype == &quot;昌平&quot;])#昌平区租金均值 ## [1] 2801.377 var(tenement$rent[tenement$regiontype == &quot;昌平&quot;])#昌平区租金方差 ## [1] 292458.5 mean(tenement$rent[tenement$regiontype == &quot;其他&quot;])#其他区租金均值 ## [1] 2835.275 var(tenement$rent[tenement$regiontype == &quot;其他&quot;])#其他区租金方差 ## [1] 636694.4 sd(tenement$rent[tenement$regiontype == &quot;昌平&quot;])#昌平区租金标准差 ## [1] 540.7943 sd(tenement$rent[tenement$regiontype == &quot;其他&quot;])#其他区租金标准差 ## [1] 797.9314 cov(tenement$rent, tenement$area)#协方差 ## [1] 1366.097 cor(tenement$rent, tenement$area)#相关系数 ## [1] 0.4474291 cov(tenement_num)#协方差矩阵 ## rent area ## rent 536375.336 1366.09651 ## area 1366.097 17.37983 cor(tenement_num)#相关系数矩阵 ## rent area ## rent 1.0000000 0.4474291 ## area 0.4474291 1.0000000 3.1.5 最大值、最小值 最大值（maximum value）、最小值（minimum value）描述一组数值型变量由高到低排序后最大和最小的元素。 max(tenement$rent)#租金最高值 ## [1] 6547.042 min(tenement$rent)#租金最低值 ## [1] 1343.001 a=which.max(tenement$rent)#找出租金最高的房间对应数据所在行 tenement[a,]#输出这一行的内容 ## X rent bedroom livingroom bathroom area room floor_grp subway region heating regiontype ## 4821 4821 6547.042 4 1 1 27 主卧 中楼层 是 朝阳 集中供暖 朝阳 3.1.6 峰度和偏度 偏度（skewness）和峰度（kurtosis）是一组用于描述数据分布形态的统计量。 library(e1071)#加载包 skewness(tenement$rent)#用e1071包求租金的偏度 ## [1] 0.9925526 ## attr(,&quot;method&quot;) ## [1] &quot;moment&quot; kurtosis(tenement$rent) #用e1071包求租金的峰度 ## [1] 1.08197 ## attr(,&quot;method&quot;) ## [1] &quot;excess&quot; 3.2 交叉分组下的频数分析 rm(list = ls())#清理工作空间 tenement = read.csv(&quot;./data/rent.csv&quot;,fileEncoding = &quot;gbk&quot;,stringsAsFactors = FALSE)#导入数据 3.2.1 交叉列联表 交叉列联表(contingency table)是对两个类别变量每个交叉水平下的频数统计。交叉列联表可以展示两个类别变量之间的相关关系。 tenement$regiontype=tenement$region#设置新变量 tenement[!is.element(tenement$region,c(&quot;朝阳&quot;,&quot;通州&quot;,&quot;昌平&quot;,&quot;丰台&quot;)),]$regiontype=&quot;其他&quot;#将非朝阳、通州、昌平和丰台的租赁房间合并为为其他 (tab &lt;- table(tenement$regiontype,tenement$floor_grp))#建立城区和楼层的交叉列联表 ## ## 低楼层 高楼层 中楼层 ## 昌平 208 215 279 ## 朝阳 477 398 442 ## 丰台 184 185 212 ## 其他 549 555 626 ## 通州 261 239 319 prop.table(tab)#频率形式交叉列联表 ## ## 低楼层 高楼层 中楼层 ## 昌平 0.04039619 0.04175568 0.05418528 ## 朝阳 0.09263935 0.07729656 0.08584191 ## 丰台 0.03573509 0.03592931 0.04117304 ## 其他 0.10662265 0.10778792 0.12157701 ## 通州 0.05068945 0.04641678 0.06195378 margin.table(tab,1)#求行变量边际函数 ## ## 昌平 朝阳 丰台 其他 通州 ## 702 1317 581 1730 819 margin.table(tab,2)#求列变量边际函数 ## ## 低楼层 高楼层 中楼层 ## 1679 1592 1878 addmargins(tab)#添加边际和 ## ## 低楼层 高楼层 中楼层 Sum ## 昌平 208 215 279 702 ## 朝阳 477 398 442 1317 ## 丰台 184 185 212 581 ## 其他 549 555 626 1730 ## 通州 261 239 319 819 ## Sum 1679 1592 1878 5149 3.2.2 描述统计量的分组统计 描述统计量的分组统计是指对定量变量按照类别变量各取值水平分组后，进行描述统计的操作。 library(dplyr) tenement_group&lt;-group_by(tenement,regiontype,floor_grp) summarise(tenement_group, meanrent=mean(rent),#分组计算平均平均租金 maxrent=max(rent),#分组计算最高租金 minrent=min(rent))#分组计算最低租金 ## # A tibble: 15 x 5 ## # Groups: regiontype [5] ## regiontype floor_grp meanrent maxrent minrent ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 昌平 低楼层 2922. 4659 1575. ## 2 昌平 高楼层 2800. 4644 1814. ## 3 昌平 中楼层 2712. 4613. 1703. ## 4 朝阳 低楼层 3370. 5681. 2023. ## 5 朝阳 高楼层 3443. 5939. 2219. ## 6 朝阳 中楼层 3379. 6547. 2119. ## 7 丰台 低楼层 2834. 4488 1849 ## 8 丰台 高楼层 2857. 4688. 1778. ## 9 丰台 中楼层 2827. 4524 1712. ## 10 其他 低楼层 2838. 5828. 1364. ## 11 其他 高楼层 2799. 5388 1369. ## 12 其他 中楼层 2865. 5595. 1343. ## 13 通州 低楼层 2508. 4144 1730. ## 14 通州 高楼层 2501. 4729. 1669 ## 15 通州 中楼层 2444. 4125 1800. 习题答案 tenement = read.csv(&quot;./data/rent.csv&quot;,fileEncoding = &quot;gbk&quot;,stringsAsFactors = FALSE)#导入数据 题目 3.1 请求出租房数据集中租赁房间面积（area）的偏度和峰值，并作出简要分析。 library(fBasics) skewness(tenement$area) #用fBasics包求评分的偏度 ## [1] 1.025208 ## attr(,&quot;method&quot;) ## [1] &quot;moment&quot; kurtosis(tenement$area) #用fBasics包求评分的峰度 ## [1] 1.066606 ## attr(,&quot;method&quot;) ## [1] &quot;excess&quot; 分析：面积的偏度为1.03，说明面积是一个右偏分布；面积的峰度为1.07，说明标准化后的面积分布比正态分布更尖锐，尾部更粗。 题目 3.2 请求出向量城区(region)和供暖方式(heating)的交叉列联表的频率形式,并添加边际和，给出简要解读。 tab &lt;- table(tenement$region,tenement$heating)#建立城区和供暖方式的交叉列联表 p_tab &lt;- prop.table(tab)#频率形式交叉列联表 addmargins(p_tab)#添加边际和 ## ## 集中供暖 自采暖 Sum ## 昌平 0.1062342202 0.0301029326 0.1363371528 ## 朝阳 0.1899397941 0.0658380268 0.2557778209 ## 大兴 0.0642843271 0.0058263741 0.0701107011 ## 东城 0.0157312099 0.0025247621 0.0182559720 ## 房山 0.0178675471 0.0176733346 0.0355408817 ## 丰台 0.0912798602 0.0215575840 0.1128374442 ## 海淀 0.0563216158 0.0260244708 0.0823460866 ## 石景山 0.0489415420 0.0011652748 0.0501068169 ## 顺义 0.0576811031 0.0033016120 0.0609827151 ## 通州 0.1491551758 0.0099048359 0.1590600117 ## 西城 0.0176733346 0.0009710623 0.0186443970 ## Sum 0.8151097300 0.1848902700 1.0000000000 分析：租赁房间的供暖方式以集中供暖为主，自采暖为辅。不同城区中，朝阳区房源数量最多。朝阳区的集中供暖房源占比最大。 题目 3.3 利用租房数据集，通过分组统计求出每个城区的平均租金并简要解读。 library(dplyr) tenement%&gt;%group_by(region)%&gt;% summarise(meanrent=mean(rent))#平均租金 ## # A tibble: 11 x 2 ## region meanrent ## &lt;chr&gt; &lt;dbl&gt; ## 1 昌平 2801. ## 2 朝阳 3395. ## 3 大兴 2411. ## 4 东城 3348. ## 5 房山 1957. ## 6 丰台 2839. ## 7 海淀 3582. ## 8 石景山 2917. ## 9 顺义 2288. ## 10 通州 2481. ## 11 西城 3878. 分析：西城区的房源平均租金最高，为3785元；房山区的房源平均租金最低，为1752元。 题目 3.4 利用租房数据集，通过分组统计计算出不同城区房源的平均租赁面积、最大租赁面积和最小租赁面积并简要解读。 library(dplyr) tenement%&gt;%group_by(region)%&gt;% summarise(meanarea=mean(area), maxarea=max(area), minarea=min(area)) ## # A tibble: 11 x 4 ## region meanarea maxarea minarea ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 昌平 13.2 30 6 ## 2 朝阳 13.0 29 5 ## 3 大兴 12.5 29 6 ## 4 东城 12.8 27 6 ## 5 房山 11.9 25 6 ## 6 丰台 12.3 29 6 ## 7 海淀 12.5 29 6 ## 8 石景山 13.7 30 6 ## 9 顺义 12.1 28 6 ## 10 通州 13.1 30 5 ## 11 西城 14.2 28 7 分析：石景山区房源的平均租赁面积最大，为13.7平米，房山区房源的平均租赁面积最小，为11.9平米；最大租赁面积为30平米，分别在昌平区和通州区；最小租赁面积为5平米，分别在朝阳区和通州区。 "],["ch4.html", "第4章：描述分析 案例引入 4.1 统计表格 4.2 数据可视化基础 4.3 数据可视化进阶 习题答案", " 第4章：描述分析 案例引入 近年来，网络小说市场正在逐渐膨胀。据相关数据表明，到2020年，我国网络文学市场规模达到120亿元，网络文学用户规模达到3.33亿，移动阅读用户规模达到3.04亿。网络小说市场迎来了前所未有的发展机遇。 以网络小说作者天蚕土豆的作品《斗破苍穹》以及忘语的《凡人修仙传》等为首的一系列网络小说，近些年来尤为火爆，它们在某平台上的点击量直破1亿大关。究竟有哪些因素使得这些小说如此之受欢迎？本章使用一个网络小说数据集作为案例进行简单的描述分析，来探究这一问题。 本章使用一个网络小说数据集作为案例数据集。该数据集收集了1549部排名较高的网络小说信息，其中包含《斗破苍穹》、《凡人修仙传》等多部热门小说的类型介绍、点击数、评分等信息。数据变量说明表如下所示： 变量类型 变量名 详细说明 取值范围 因变量 总点击数 定量，单位：次 12920-149800000 自变量 小说信息 小说名称 定性 作者 定性 小说类型 定性，13个水平 总字数 定量，单位：个 小说性质 定性，2个水平 写作进程 定性，11个水平 授权状态 定性，8个水平 更新时间 单位：年／月／日／时／分 内容简介 定性，1549个水平 会员评价 评分 定量，单位：分 会员周点击数 定量，单位：次 评论数 定量，单位：个 这里我们关心的核心指标是小说点击数，它对于小说而言至关重要，代表了一部小说的热度。在所收集的小说数据集中，最大值《斗破苍穹》的点击数接近1.5亿，而最小值《鸣的大冒险》仅有不到1.3万，可谓差距悬殊。那么，如何分析哪些因素与小说点击数息息相关？想要探索这个问题的答案，需要对数据进行描述分析。 ### 数据准备 ### # 清空工作空间 rm(list = ls()) # 载入相关包及设定路径 library(plyr) library(dplyr) library(stringr) library(tidyr) library(corrplot) library(ggplot2) library(zoo) library(reshape2) library(plotly) Sys.setlocale(&quot;LC_ALL&quot;, &quot;zh_cn.utf-8&quot;) ## [1] &quot;zh_cn.utf-8/zh_cn.utf-8/zh_cn.utf-8/C/zh_cn.utf-8/en_US.UTF-8&quot; # 读入数据 novel = read.csv(&#39;./data/novel.csv&#39;, fileEncoding = &quot;UTF-8&quot;) # 数据查看与异常处理 head(novel) ## 人气排序 小说名称 作者 小说类型 总点击数 会员周点击数 总字数 评论数 评分 小说性质 写作进程 授权状态 ## 1 1 一念永恒 耳根 仙侠小说 4383898 10691 1155534 435429 9.8 公众作品 连载中 Ａ级签约 ## 2 2 斗战狂潮 骷髅精灵 仙侠小说 1678379 36587 422116 23159 10.0 公众作品 连载中 Ａ级签约 ## 3 3 天影 萧鼎 仙侠小说 1248708 32019 373763 25253 9.8 公众作品 连载中 Ａ级签约 ## 4 4 不朽凡人 鹅是老五 仙侠小说 2457382 9610 995669 146715 9.9 公众作品 连载中 Ａ级签约 ## 5 5 玄界之门 忘语 仙侠小说 3736897 6709 1784999 238113 9.8 公众作品 新书上传 Ａ级签约 ## 6 6 龙王传说 唐家三少 玄幻小说 2968846 3080 1552654 293934 9.8 公众作品 新书上传 Ａ级签约 ## 更新时间 ## 1 2016/10/23 11:50 ## 2 2016/10/22 17:05 ## 3 2016/10/23 10:40 ## 4 2016/10/22 20:50 ## 5 2016/10/23 10:15 ## 6 2016/10/23 7:00 ## 内容简介 ## 1 一念成沧海，一念化桑田。一念斩千魔，一念诛万仙。??? 唯我念……永恒??? 这是耳根继《仙逆》《求魔》《我欲封天》后，创作的第四部长篇小说《一念永恒》 ## 2 双月当空，无限可能的英魂世界 \\n??? 孤寂黑暗，神秘古怪的嬉命小丑 \\n??? 百城联邦，三大帝国，异族横行，魂兽霸幽 \\n??? 这是一个英雄辈出的年代，人类卧薪尝胆重掌地球主权，孕育着进军高纬度的野望！ \\n??? 重点是……二年级的废柴学长王同学，如何使用嬉命轮盘，撬动整个世界，学妹们，请注意，学长来了！！！ \\n??? 斗战一群：21222419（两千人战力群）\\n??? 斗战二群：12962047 \\n??? 骷髅的微信公共号：kuloujingling00 \\n??? 新浪微博：骷髅精灵 ## 3 阴阳分天地，五行定乾坤。\\n??? 天穹之下岁月沧桑的中土神州，正是仙道昌盛的时代，亿万生灵欣欣向荣。\\n??? 纵横千万里间，总有人间一幕幕悲欢离合，在恢弘长生的仙道中上演着。\\n??? 有光便有暗，天穹之下光辉之中，仍有沉默的影子悄然前行着……\\n??? 新书上线！精彩万分！请各位书友多多投票支持！另外，大家可以添加微信公众号zhuxianxiaoding（诛仙萧鼎），QQ官方群 176378308 进行交流。 ## 4 在这里，拥有灵根才能修仙，所有凡根注定只是凡人。\\n??? 莫无忌，只有凡根，一介凡人！\\n??? 是就此老去，还是不甘？??? ## 5 天降神物！异血附体！??? 群仙惊惧！万魔退避！??? 一名从东洲大陆走出的少年。??? 一具生死相依的红粉骷髅。??? 一个立志成为至强者的故事。??? 一段叱咤星河，大闹三界的传说。??? 忘语新书，已完本《凡人修仙传》》《魔天记》。 ## 6 伴随着魂导科技的进步，斗罗大陆上的人类征服了海洋，又发现了两片大陆。魂兽也随着人类魂师的猎杀无度走向灭亡，沉睡无数年的魂兽之王在星斗大森林最后的净土苏醒，它要带领仅存的族人，向人类复仇！\\n??? 唐舞麟立志要成为一名强大的魂师，可当武魂觉醒时，苏醒的，却是……\\n??? 旷世之才，龙王之争，我们的龙王传说，将由此开始。\\n??? 4.1 统计表格 4.1.1 变量说明表 对于定性变量，列示其主要水平及对应频数；对于定量变量，关注其取值范围、最大值、最小值、中位数等描述性统计量。这些变量描述指标在R语言中可以通过summary()函数直接获取。 # 对小说数据的若干列使用summary()函数 summary(novel[, 2:4]) ## 小说名称 作者 小说类型 ## Length:1549 Length:1549 Length:1549 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character 4.1.2 分组汇总统计表格 分组统计表格按照各个类别进行汇总，描述类别内部各个变量的特征。通过这种方式，可以比较各个类别之间的差异，输出所关心的汇总报表。 # 利用group_by()函数对小说按照小说类型分类 novel.小说类型 = novel %&gt;% group_by(小说类型) # 制作分组统计表 novel.小说类型 %&gt;% summarise(平均评分 = mean(评分), 最大评论数 = max(评论数)) ## # A tibble: 13 x 3 ## 小说类型 平均评分 最大评论数 ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 都市小说 9.13 183799 ## 2 二次元小说 8.22 23428 ## 3 军事小说 9.38 92845 ## 4 科幻小说 8.93 369417 ## 5 历史小说 9.01 222505 ## 6 灵异小说 9.1 17602 ## 7 奇幻小说 9.02 231883 ## 8 体育小说 9.33 27316 ## 9 武侠小说 8.13 196111 ## 10 仙侠小说 8.95 604560 ## 11 玄幻小说 9.12 569756 ## 12 游戏小说 9.22 237676 ## 13 职场小说 8.73 398307 4.2 数据可视化基础 统计表格是使用表格的形式展示汇总的统计指标，但从直观性而言，使用统计图形则更为合适。基本统计图形包括柱状图、饼图、箱线图、散点图、折线图和直方图。 4.2.2 柱状图 柱状图（barplot）是用柱子高低来表示取值大小的一种统计图形。常见的柱状图有频数柱状图及均值柱状图。频数柱状图展示的是定性变量各水平的频数统计，柱子高度代表每个水平频数大小。 # 统计小说类型频数 a=table(novel$小说类型) # 对频数排序 a=a[order(a,decreasing = T)] # 绘制柱状图 par(family = &quot;STHeiti&quot;,las = 2) barplot(a[1:5],names.arg = names(a)[1:5],family=&quot;STHeiti&quot;,ylab = &quot;频数&quot;,xlab=&quot;&quot;,col = c(&quot;gold&quot;,&quot;grey&quot;,&quot;grey&quot;,&quot;grey&quot;,&quot;grey&quot;)) title(&quot;各类型小说频数分布柱状图&quot;,family=&quot;STHeiti&quot;) 柱状图并不仅用于展现频数，也可以展示按照分组划分后各类别的均值等指标。这种柱状图称为均值柱状图。 # 求得各类型小说平均点击数的均值 means&lt;-novel%&gt;% group_by(小说类型)%&gt;% summarise(mean = mean(总点击数/10000))%&gt;% arrange(desc(mean)) par(family = &quot;STHeiti&quot;,las = 2) # 绘制均值柱状图 barplot(means$mean[1:5], names.arg=means$小说类型[1:5],family=&quot;STHeiti&quot;,ylab = &quot;总点击数(万次)&quot;,xlab=&quot;&quot;,col = c(&quot;gold&quot;,&quot;grey&quot;,&quot;grey&quot;,&quot;grey&quot;,&quot;grey&quot;)) title(&quot;各类型小说平均总点击数&quot;,family=&quot;STHeiti&quot;) options(scipen = 200) 对两个或多个定性变量绘制柱状图可使用分组柱状图或堆积柱状图来展现。 # 构建绘图所需数据矩阵d d = novel%&gt;%group_by(小说类型, 小说性质)%&gt;% summarise(count=n())%&gt;% spread(小说性质,count)%&gt;% select(-V1)%&gt;% arrange(desc(公众作品)) topics = d$小说类型[1:5] artwork_type = colnames(d)[2:3] # 选取前五类小说 d = matrix(as.numeric(t(d[1:5,])[-1,]),nrow=2) # 对列命名 colnames(d) = topics # 对行命名 rownames(d) = artwork_type # 绘制分组柱状图(beside = T) par(mfrow=c(1,2), family = &quot;STHeiti&quot;,las=2) barplot(d, beside = T, col = c(&quot;gold&quot;,&quot;grey&quot;), ylab = &quot;频数&quot;, ylim = c(0,500)) # 添加图例 legend(&quot;topright&quot;, legend = rownames(d), fill = c(&quot;gold&quot;,&quot;grey&quot;), cex = 0.8) # 绘制堆积柱状图(beside = F) barplot(d, beside = F, col = c(&quot;gold&quot;,&quot;grey&quot;), ylab = &quot;频数&quot;, ylim = c(0,700)) # 添加图例 legend(&quot;topright&quot;, legend = rownames(d), fill = c(&quot;gold&quot;, &quot;grey&quot;), cex = 0.8) 4.2.3 饼图 饼图（pie），是用圆形及圆内扇形的度数来表示数值大小的图形，主要用于表示总体中各组成部分所占的比例。 # 将小说类型进行简要合并 novel$&#39;小说类别&#39; = &quot;其他&quot; novel$&#39;小说类别&#39;[novel$小说类型 == &quot;都市小说&quot; | novel$小说类型 == &quot;职场小说&quot;] = &quot;都市类小说&quot; novel$&#39;小说类别&#39;[novel$小说类型 == &quot;科幻小说&quot; | novel$小说类型 == &quot;玄幻小说&quot; | novel$小说类型 == &quot;奇幻小说&quot;] = &quot;幻想类小说&quot; novel$&#39;小说类别&#39;[novel$小说类型 == &quot;武侠小说&quot; | novel$小说类型 == &quot;仙侠小说&quot;] = &quot;武侠类小说&quot; # 求出每一类所占百分比 ratio = table(novel$&#39;小说类别&#39;) / sum(table(novel$&#39;小说类别&#39;)) * 100 # 定义标签 label1 = names(ratio) label2 = paste0(round(ratio, 2), &quot;%&quot;) # 绘制饼图 par(family=&quot;STHeiti&quot;) pie(ratio, col = heat.colors(5, alpha = 0.4), labels = paste(label1, label2, sep = &quot;\\n&quot;), font = 1) 4.2.4 直方图 直方图（histogram）是用于展示定量变量分布形态的一种统计图形。 # 随机生成1000个正态分布数 r=rnorm(1000) # 绘制直方图 hist(r) # 去掉异常值 chara = sort(novel$总字数/10000)[1:1500] # 绘制直方图 par(mfrow=c(1,2),family = &quot;STHeiti&quot;,las=2) hist(chara, breaks = 10, xlab = &quot;总字数(万字)&quot;, ylab = &quot;频数&quot;, main = &quot;&quot;, col = &quot;gold&quot;,border = NA) # 调整直方图组距 hist(chara, breaks = 100, xlab = &quot;总字数(万字)&quot;, ylab = &quot;频数&quot;, main = &quot;&quot;, col = &quot;gold&quot;,border = NA) 4.2.5 折线图 折线图（line chart），是以折线的上升或下降来表示统计数量的增减变化的统计图，一般用于反映变量随时间变化的特征。它不仅可以表示数量的多少，而且可以反映数据的变化趋势。 # 获取数据 data(AirPassengers) # 画时间序列图 plot(AirPassengers) 4.2.6 箱线图 箱线图（boxplot）主要用于描述定量变量在定性变量各个水平上的分布差异。 ## 定性与定量变量--分组箱线图 ## # 不同性质的小说总点击数和评论数有差别吗 novel_ = novel %&gt;% dplyr::filter((小说性质 == &#39;公众作品&#39;)|(小说性质 == &#39;VIP作品&#39;)) %&gt;% mutate(小说性质=factor(小说性质)) # 将画板分成1行2列 par(mfrow=c(1,2),family = &quot;STHeiti&quot;) boxplot(log(总点击数) ~ 小说性质, data = novel_, col = c(&#39;gold&#39;,&#39;grey&#39;), ylab = &quot;总点击数对数&quot;) boxplot(log(总点击数) ~ 小说性质, data = novel_, col = c(&#39;gold&#39;,&#39;grey&#39;), ylab = &quot;总评论数对数&quot;) 4.2.7 散点图 散点图（scatter diagram）是用二维坐标展示两个连续变量之间关系的一种图形。散点图用坐标横轴代表变量x，纵轴代表变量y，每组数据在坐标系中用一个点表示，n个样本点就形成了n个散点。 ## 两个定量变量--散点图 ## # 去除较大的异常值后画图 test = novel[novel$评论数 &lt; 8000 &amp; novel$总点击数 &lt; 200000, ] x = test$总点击数 y = test$评论数 par(family = &quot;STHeiti&quot;) plot(x, y, pch = 1, cex = 0.6, xlab = &quot;总点击数&quot;, ylab = &quot;评论数&quot;) 4.2.8 相关系数图 相关系数图（corrplot）是对多变量相关关系矩阵的可视化展示。两变量间的相关系数的绝对值越大，说明两组变量的相关性越强。 # 选取数据 y = novel[,5:8] # 计算相关系数矩阵 r = cor(y,use=&quot;na.or.complete&quot;) # 绘制相关系数图 par(family = &quot;STHeiti&quot;) corrplot(r, tl.col = &quot;black&quot;) 4.3 数据可视化进阶 4.3.1 ggplot2 包 柱状图 首先通过频数柱状图查看数据中小说类型的分布。 # 计算频数 novel_count = novel%&gt;%group_by(小说类型)%&gt;%summarise(count = n())%&gt;%arrange(desc(count)) # 构建坐标轴图层p,将变量按照频数排序并取前十 p = ggplot(novel_count[1:10,], mapping = aes(x = reorder(小说类型,desc(count)), y = count)) # 绘制柱状图 p + geom_bar(stat = &#39;identity&#39;)+ theme_gray(base_family = &#39;STHeiti&#39;)+ labs(x = &quot;小说类型&quot;, y = &quot;频数&quot;) 饼图 如前所述，饼图可以查看定性变量分布比例。 ## 饼图 ## ## step 1: 统计频数（此处也可使用table()），即统计出来每一类别的频数。 df1 = ddply(novel, .(小说类别), nrow) df1 = df1[order(df1$V1, decreasing = T), ] ## step 2: 画出堆积柱状图。 # 计算各个标签的位置 pos = (cumsum(df1$V1) - df1$V1/2) # 构建坐标轴图层p p = ggplot(df1, aes(x = &quot;&quot;, y = V1, fill = factor(小说类别,levels = c(&#39;武侠类小说&#39;,&#39;都市类小说&#39;,&#39;幻想类小说&#39;,&#39;其他&#39;)))) # 绘制柱状图 p = p + geom_bar(width = 1,stat = &quot;identity&quot;)+theme_gray(base_family = &#39;STHeiti&#39;) + scale_fill_brewer(&#39;&#39;,palette = &quot;Blues&quot;) # step 3 : 变成极坐标，并加比例标签。 # 换到极坐标 p = p+ coord_polar(theta = &quot;y&quot;) # 添加文本信息 p + geom_text(aes(y = pos, label = paste(round(V1 / sum(V1) * 100, 2), &quot;%&quot;, &quot;&quot;))) + theme(axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), text = element_text(family=&#39;STHeiti&#39;)) 直方图 频数柱状图和饼图往往用于展示定性变量的分布频率，对于连续变量（如：小说总点击数）分布则需用直方图展示。 # 构建坐标轴图层p p = ggplot(novel, mapping = aes(x = log(总点击数))) # 绘制直方图 p + geom_histogram(bins = 30) + theme_gray(base_family = &#39;STHeiti&#39;) + xlab(&#39;对数总点击数&#39;) + ylab(&#39;频数&#39;) 折线图 对于连续变量的时间序列数据，可以通过折线图形式展示。 ## 折线图 ## # 构建坐标轴图层p p = ggplot(economics, aes(x = date, y = unemploy)) # 绘制折线图 p + geom_line() 箱线图 分组箱线图可用于比较组别之间变量分布差异。 # 构建坐标轴图层p p = ggplot(novel_, mapping = aes(x = 小说性质, y = log(总点击数))) # 绘制分组箱线图 p+ geom_boxplot(fill = c(&#39;gold&#39;,&#39;grey&#39;)) + theme_gray(base_family = &#39;STHeiti&#39;) + xlab(&#39;小说性质&#39;) + ylab(&#39;对数总点击数&#39;) 散点图 对于两个定量变量，可以用散点图展示其关系。 # 构建坐标轴图层p data = novel[novel$评论数 &lt; 8000 &amp; novel$总点击数 &lt; 200000, ] p = ggplot(data , mapping = aes(x = 总点击数, y = 评论数)) # 绘制散点图 p + geom_point() + theme_gray(base_family = &#39;STHeiti&#39;) + xlab(&#39;总点击数&#39;) + ylab(&#39;评论数&#39;) 利用ggplot2包，也可以将其它变量映射到散点图当中，例如将不同性质的小说用不同的颜色来表示。 # 构建坐标轴图层p p = ggplot(na.omit(data) , mapping = aes(x = 总点击数, y = 评论数 , col = 小说性质)) # 绘制散点图 p + geom_point() + theme_gray(base_family = &#39;STHeiti&#39;) + xlab(&#39;总点击数&#39;) + ylab(&#39;评论数&#39;) 4.3.2 交互可视化 柱状图 此处将type设置为bar就可以绘出柱状图，当把鼠标移动到任何一根柱子上面，就会实时显示出它的数值和对应的小说类型。 ### 均值柱状图 ### plot_ly(novel, y= ~ 总点击数, x = ~ 小说类型, type = &quot;bar&quot;) 饼图 交互式饼图可以展现每个组成部分更多的数据细节。 ## 动态饼图 ## # 读取数据 piedata = data.frame(value = c(29.05, 24.08, 10.85, 36.23), group = c(&quot;幻想类小说&quot;, &quot;都市类小说&quot;, &quot;武侠小说&quot;, &quot;其他&quot;)) # 绘制图形 plot_ly(piedata, values = ~value, labels = ~group, type = &quot;pie&quot;) 直方图 在表现定量变量的分布时，直方图是必不可少的工具。 ## 直方图 ## p = plot_ly(novel, x = ~ log10(总点击数), type = &quot;histogram&quot;) # 增加坐标轴及标题信息 layout(p, title = &quot;对数点击数分布直方图&quot;, xaxis = list( title = &quot;对数点击数&quot;, showgrid = F), yaxis = list( title = &quot;频数&quot;), margin = list(l = 50, r = 50, b = 50, t = 50, pad = 4)) 习题答案 library(readxl) # 读取.xlsx数据 library(dplyr) # 用导管运算 library(ggplot2) # 画ggplot图 library(stringr) # 处理字符串 library(purrr) # 使用map函数 library(data.table) # 读入较大的csv文件 library(magrittr) # 管道操作符 Sys.setlocale(&quot;LC_ALL&quot;, &quot;zh_cn.utf-8&quot;) ## [1] &quot;zh_cn.utf-8/zh_cn.utf-8/zh_cn.utf-8/C/zh_cn.utf-8/en_US.UTF-8&quot; 题目 4.3（实训题目） 实训题目：使用“北美旅游产品数据集”，该案例数据提供了2926条北美旅游数据观测。其中包括产品名称、旅游方式、供应商、等级、景点个数、交通情况、用餐情况、是否有自由活动、客户评分、出游人数、评价人数、报价信息、旅游线路等。尝试完成以下分析。 a.读入数据，筛选出产品名称、旅游方式、供应商、等级、景点个数、交通、用餐、自由活动、总评价、出游人数、出发地、每日报价、旅游线路变量并重命名为英文变量名。 dat = read_excel(&quot;./data/data_4_3.xlsx&quot;) # 原始数据 travel_dat = dat[,c(1:8,12:13,15,17:23,25)] # 筛选出需要用到的变量 colnames(travel_dat) = c(&quot;Product&quot;, &quot;TravelMethod&quot;, &quot;Agency&quot;, &quot;Star&quot;, &quot;Place&quot;, &quot;Traffic&quot;, &quot;Meal&quot;,&quot;FreeActivitie&quot;, &quot;Evaluate&quot;,&quot;Sale&quot;,&quot;Depart&quot;,&quot;SunPrice&quot;,&quot;MonPrice&quot;, &quot;Tuesprice&quot;,&quot;WedPrice&quot;,&quot;ThusPrice&quot;, &quot;Friprice&quot;,&quot;Satprice&quot;,&quot;Routine&quot;) # 将变量名改为英文 b.提取周一到周日报价中的数值部分，计算一周报价的均值（若一周7天均无报价则缺失），并以新变量“Price”存入数据集travel_dat中，剔除平均价格缺失的样本。绘制价格的对数分布直方图并进行简要解读。 allPrice = travel_dat[,12:18] # 取出周一到周日的数据 allPrice[allPrice==&quot;&quot;] = NA # 处理缺失值 PriceToNum = function(price) # 自定义函数，提取价格的数值部分 { price = gsub(&quot;\\\\D&quot;, &quot;&quot;, as.character(price)) %&gt;% as.numeric() # 去字符串中的价格部分 return(price) } price = allPrice %&gt;% apply(2, PriceToNum) # 得到价格的数值部分 avgprice = apply(price, 1, function(x) mean(x, na.rm = T)) # 平均价格 travel_dat$Price = avgprice # 建立新变量价格 travel_dat = travel_dat[!is.na(travel_dat$Price),] # 去除缺失价格的观测 ggplot(travel_dat)+ geom_histogram(aes(x = Price),fill = &quot;gold&quot;,bins = 30) + xlab(&quot;产品价格（对数变换）&quot;) + ylab(&quot;频数&quot;) + theme_bw(base_family = &quot;STHeiti&quot;)+ #设置背景颜色和字体 scale_x_log10(breaks = c(1000, 10000), labels = c(&quot;1千&quot;,&quot;1万&quot;))+#对数变换 theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;black&quot;)) #设置网格，取消边框 c.提取“Star”变量中的“*钻”字符来表示产品等级，当一个产品包含多个钻级时取最大钻级，并将产品钻级以新变量“Star2”存入数据集travel_dat中，变量类型为因子型。 绘制价格的对数对产品等级的分组箱线图，并按每一等级的平均价格由低到高进行排列，对箱线图结果作出简要解读。 star = strsplit(as.character(travel_dat$Star), &quot;\\\\,&quot;) # 按“,”分开等级 StarNum = function(x) # 自定义函数，提取产品等级 { if(&quot;暂无酒店信息&quot; %in% x){ return(&quot;无&quot;) }else{ x = x %&gt;% strsplit(&quot;晚|钻&quot;) # 得到几晚几钻的信息 x = x %&gt;% map(2) %&gt;% unlist() %&gt;% as.numeric() %&gt;% ceiling() %&gt;% max(na.rm=T) # 用最大的钻级表示该产品的等级 return(x) } } star1= star %&gt;% lapply(StarNum) %&gt;% unlist() # 得到产品等级 star1[star1==-Inf] = &quot;无&quot; # 将-Inf表示为无产品等级信息 travel_dat$Star2 = as.factor(star1) # 变成因子变量并保存至travel_dat中 travel_dat$Star2 = factor(travel_dat$Star2, # 按照均价重新排序水平 levels = levels(travel_dat$Star2)[c(1,5,2,3,4)], labels = c(&quot;2钻&quot;,&quot;无信息&quot;,&quot;3钻&quot;,&quot;4钻&quot;,&quot;5钻&quot;)) ggplot(travel_dat, aes_string(x=&quot;Star2&quot;, y=&quot;Price&quot;)) + # 箱线图 theme_bw(base_family = &quot;STHeiti&quot;)+ geom_boxplot(varwidth=T, color = adjustcolor(&quot;#8CE52E&quot;), fill = adjustcolor(&quot;#8CE52E&quot;, alpha.f = 0.4)) + scale_y_log10(breaks=c(2e3,1e4,8e4), # 对价格取对数 labels=c(&quot;2千&quot;, &quot;1万&quot;, &quot;8万&quot;)) + ylab(&quot;产品价格（对数变换）&quot;) + xlab(&quot;&quot;) + theme(panel.background = element_rect(fill = &quot;transparent&quot;), # 背景透明 panel.grid.major = element_blank(), # 去掉背景网格 panel.grid.minor = element_blank(), axis.ticks = element_blank()) # 去掉坐标轴 题目 4.4（实训题目） 实训题目：RTB（Real Time Bidding，实时竞拍）是目前一种重要的广告投放方式。当前的各种APP都有许多广告位，等待广告主的投放。对于投放广告的广告主，他们通过竞拍获得广告位，自然希望自己的广告能有更高的点击量。使用RTB数据集，本数据包括了来自某广告外包承包商（DPS）的4695条观测值，因变量为是否点击（1-点击/0-未点击），正样本大约占总样本的20%，请对以下自变量进行分析。 rtb_data = read.csv(&quot;./data/data_4_4.csv&quot;) #将定性变量转为因子性变量，并设置标签 rtb_data$dc = factor(rtb_data$dc,levels = c(1,0)) rtb_data$atype = factor(rtb_data$atype,levels = c(3,7,8,13),labels = c(&quot;Inmobi&quot;,&quot;Zplay&quot;,&quot;Baidu&quot;,&quot;Iflytek&quot;)) rtb_data$instl = factor(rtb_data$instl,levels = c(1,0),labels = c(&quot;是&quot;,&quot;否&quot;)) rtb_data$isp = factor(rtb_data$isp,levels = c(0,1,2,3),labels = c(&quot;未知&quot;,&quot;中国移动&quot;,&quot;中国联通&quot;,&quot;中国电信&quot;)) rtb_data$nt = factor(rtb_data$nt,levels = c(0,1,2,3,4,5),labels = c(&quot;未知&quot;,&quot;WIFI&quot;,&quot;2G&quot;,&quot;3G&quot;,&quot;4G&quot;,&quot;5G&quot;)) rtb_data$mfr = as.factor(rtb_data$mfr) rtb_data$period = as.factor(rtb_data$period) #计算不同ADX平台的点击率 ratio_atype = rtb_data %&gt;% group_by(atype) %&gt;% summarise(ratio=sum(dc==1)/n()) list = ratio_atype$atype[order(ratio_atype$ratio,decreasing = T)] #将ADX平台类型按照点击率降序排序 rtb_data$atype = factor(rtb_data$atype,levels = list) par(las = 2,mai = c(1,1,0.5,1),family = &quot;STHeiti&quot;) spineplot(dc~atype,rtb_data,col = c(&#39;gold&#39;,&#39;grey&#39;),xlab = &#39;&#39;,yaxlabels = c(&#39;点击&#39;,&#39;不点击&#39;),ylab = &#39;&#39;) bidf_plot = as.data.frame(subset(rtb_data,select = c(&#39;dc&#39;,&#39;bidf&#39;))) bidf_plot$dc = factor(bidf_plot$dc,levels = c(1,0),labels = c(&#39;点击&#39;,&#39;不点击&#39;)) ggplot(bidf_plot)+ geom_boxplot(aes(x = dc,y = bidf,fill = dc),varwidth = T)+ #将箱线图按每一等级的平均价格由低到高进行排列，并调节图像的颜色 ylab(&quot;竞标底价（对数变换）&quot;)+ xlab(&quot;&quot;)+#添加坐标轴信息 labs(fill = &quot;是否点击广告&quot;)+ scale_y_log10()+#对纵坐标轴做对数变化 scale_fill_manual(values = c(&quot;gold&quot;, &quot;grey&quot;))+ theme_bw(base_family = &quot;STHeiti&quot;)+ theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;black&quot;))#调整主题样式 题目 4.5（实训题目） 实训题目：使用“数据分析招聘数据集”，该数据集包括某网站数据分析岗位2018-2019年招聘情况。该数据共7493条招聘信息，覆盖北京、上海、深圳、山西、陕西以及河北六个地区。 fin_data = data.table::fread(&quot;./data/data_4_5.csv&quot;, header = T) colnames(fin_data) &lt;- paste0(&quot;X&quot;, 0:8) ###提取工资的函数 fun_wage = function(x){ if(x == &quot;面议&quot;){#面议，工资12000 wage = 12000 }else if (!is.na(str_extract(x,&quot;-&quot;))){#提取中间点 wage1 = as.numeric(str_split(x, &quot;-&quot;)[[1]][1]) wage2 = as.numeric(str_split(x, &quot;-&quot;)[[1]][2]) wage=0.5*(wage1+wage2) }else{#提取xxx以上，以下 wage=gsub(&quot;\\\\D&quot;, &quot;&quot;, as.character(x)) %&gt;% as.numeric() } return(wage) } fin_data$wage = fin_data$X6%&gt;% lapply(fun_wage) %&gt;% unlist() ###提取工资 fin_data=fin_data[which(fin_data$wage &lt;= 40000 &amp; fin_data$wage &gt;= 3000),]#筛选工资在3000-40000的 ggplot(fin_data)+ geom_histogram(aes(x = wage),fill = &quot;gold&quot;,bins = 25) + xlab(&quot;薪资水平（元／月）&quot;) + ylab(&quot;频数&quot;) + theme_bw(base_family = &quot;STHeiti&quot;)+ #设置背景颜色和字体 theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;black&quot;)) #设置网格，取消边框 summary(fin_data$wage) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3000 6500 9000 9563 12500 40000 ###对岗位进行分类 fin_data$is_tech = 0 fin_data[str_detect(fin_data$X3,&quot;数据|IT|量化|工程师&quot;),]$is_tech = 1 ###计算各个城市按照岗位类别分类的平均工资和岗位个数 fin_data_city=fin_data%&gt;% group_by(X0,is_tech)%&gt;% summarise(meanwage=mean(wage,na.rm = T),#计算平均工资 countwage=length(wage))#计算岗位个数 ###绘制并列柱状图 ggplot(fin_data_city)+ geom_bar(aes(x = reorder(X0,X = -meanwage,FUN = median),#将城市按照工资均值排序 y = meanwage/1000, fill = factor(is_tech, levels = c(0,1), labels = c(&quot;传统金融&quot;,&quot;金融科技&quot;))), #is_tech对应传统金融和金融科技 stat = &quot;identity&quot;,position = &quot;dodge&quot;)+#position = &quot;dodge&quot;设置为并列柱状图 theme_bw(base_family = &quot;STHeiti&quot;)+#背景设置为白色，字体为宋体 labs(fill = &quot;岗位类别&quot;)+#设置图例 scale_fill_manual(values = c(&quot;DimGrey&quot;,&quot;gold&quot;))+#填充颜色 xlab(&quot;城市&quot;)+#设置横轴名称 ylab(&quot;薪资平均水平（千元）&quot;)+#设置纵轴名称 theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;black&quot;))#设置背景和坐标轴 ###提取工作经验要求的函数 fun_work = function(x){ str_list0 = str_split(x,&quot;[|]&quot;)[[1]] work0 = str_list0[str_detect(str_list0,&quot;经验&quot;)] if(length(work0) == 1){#提取有经验要求的 work = substring(work0, 4) }else{#提取无经验要求的 work = &quot;不限&quot; } return(work) } #提取工作经验要求 fin_data$work = fin_data$X7%&gt;% lapply(fun_work) %&gt;% unlist() fin_data[which(fin_data$work == &quot;无经验&quot;),]$work = &quot;不限&quot;#将无经验变为不限 #设置标签wage0，将不限排在最后 fin_data$wage0=fin_data$wage fin_data[which(fin_data$work == &quot;不限&quot;),]$wage0 = 0 #绘制箱线图 ggplot(fin_data)+ geom_boxplot(aes(x = reorder(work,X = -wage0),#按照wage0对work排序，“不限”排在最后 y = wage/10000, fill = factor(is_tech,levels = c(0,1),labels = c(&quot;传统金融&quot;,&quot;金融科技&quot;))),#设置is_tech标签 varwidth = T)+ labs(fill = &quot;岗位类别&quot;)+#设置图例 scale_fill_manual(values = c(&quot;DimGrey&quot;,&quot;gold&quot;))+#填充颜色 xlab(&quot;岗位类别&quot;)+#设置横轴名称 ylab(&quot;岗位工资（万元）&quot;)+#设置纵轴名称 theme_bw(base_family = &quot;STHeiti&quot;)+#背景设置为白色，字体为宋体 theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = &quot;black&quot;))#设置背景和坐标轴 "],["ch5.html", "第5章：参数估计与假设检验 案例引入 5.2 参数估计 5.3 假设检验 习题答案", " 第5章：参数估计与假设检验 ### 数据准备 ### # 清空工作空间 rm(list = ls()) 案例引入 本章主要包括三个简短的案例，以下对每个案例及数据进行简要介绍。 案例一：手机续航时间研究 手机续航能力是指手机在正常工作时的待机时间。它与手机本身的耗电量和所配电池容量的大小有关。在电池容量相同的情况下，耗电量越小，则续航能力越强；在耗电量相同的情况下，电池容量越大，待机时间越长。近年来，各个品牌厂家都致力于打造续航能力更强的新款手机，从而满足现代人们对手机设备的长时间使用需求，提高自身的市场竞争力。 battery &lt;-c(20.2,19.4,18.6,18.2,19.8,21.4,18.2,19.8,17.4,19.4,21.4,19.0,19.0,17.0,23.0,22.6,21.4,20.2,22.6,20.2,21.4,20.2,18.2,20.2,19.0,19.8,19.8,20.6,20.6,18.6,19.0,21.4,20.6,21.8,19.4,19.8,21.8,19.4,17.4,20.2,19.8,17.8,17.4,19.8,20.2,19.0,20.2,18.2,21.0,19.8) # 显示续航时间样本数据 print(battery) ## [1] 20.2 19.4 18.6 18.2 19.8 21.4 18.2 19.8 17.4 19.4 21.4 19.0 19.0 17.0 23.0 22.6 21.4 20.2 22.6 20.2 21.4 20.2 ## [23] 18.2 20.2 19.0 19.8 19.8 20.6 20.6 18.6 19.0 21.4 20.6 21.8 19.4 19.8 21.8 19.4 17.4 20.2 19.8 17.8 17.4 19.8 ## [45] 20.2 19.0 20.2 18.2 21.0 19.8 本案例研究的问题是判断这一批手机的续航时间是否达到了声称的20小时。 案例二：减肥药疗效研究 随着生活水平的提高，油腻、高热量的食物成为了很多年轻人的便捷之选，肥胖患者日剧增多。然而由此引发的肥胖问题会极大危害到身体健康，仅在2015年，全球因肥胖造成死亡的人数就已超过400万，世界卫生组织已将超重、肥胖定义为一种慢性病。因此，减肥不仅是一个时尚与外貌相关的话题，更关乎身体的健康。为了应对肥胖带来的身体健康问题，制药公司纷纷推出减肥特效药。减肥药药效如何，需要大量试验验证，得出结论。 在减肥的案例中，将会使用以下减肥药药效的数据。一个合规的减肥药要在美国市场上合法上市，必须有美国食品药品管理局（FDA）的批准。为此，FDA对使用该药物的受试者做了一组试验，记录了其在使用减肥药前后的体重（单位：kg）。试验的数据如下所示： 受试者编号 1 2 3 4 5 6 7 8 9 10 服用药物前的体重x 43 55 49 62 59 49 57 54 55 48 服用药物后的体重y 50 59 55 60 58 54 56 53 61 51 差 d = x-y -7 -4 -6 2 1 -5 1 1 -6 - 3 受试者在服用减肥药后是否体重发生了显著下降，这是本章所要探究的问题。 案例三：红楼梦作者争议 《红楼梦》是一部章回体长篇小说，被列为中国古代四大名著之首。《红楼梦》文学价值极高，围绕作品本身的争议也很多。其中一大争议便是作者归属问题。 本案例收集了《红楼梦》中有代表性的若干个文言虚词，并希望探究这些虚词在《红楼梦》前后的使用差异。在本章中，我们选择具有代表性的“之”和“亦”两个虚词的章回词频，通过对比词频在整本书前后出现的差异，可以了解作者用语习惯的改变。具体而言，统计每个虚词在每个章回的总词频，将全书1-40回、41-80回、81-120回切分为三个总体。全书1-40回、41-80回、81-120回中两个字的平均字频如下图所示： 众多大家各执一词，学术界仍无定论。本章从文本分析的角度，分析作者用语习惯的改变，探索《红楼梦》作者归属的问题。 5.2 参数估计 5.2.1 矩估计 电池续航时间 2030年，苹果公司发布新一代手机iPhone SSR。该手机声称续航能力比上一版本（20小时）有较大提升。为调查市面上售卖的苹果手机是否满足这样的续航能力，我们收集并测试了50部苹果手机的续航时间，得到如下数据。假设苹果手机的续航时间服从指数分布，那么使用矩估计推断该苹果手机的平均续航时间是多久？ # 例1 battery &lt;- c(20.2,19.4,18.6,18.2,19.8,21.4,18.2,19.8,17.4,19.4,21.4,19,19,17, 23,22.6,21.4,20.2,22.6,20.2,21.4,20.2,18.2,20.2,19,19.8,19.8,20.6, 20.6,18.6,19,21.4,20.6,21.8,19.4,19.8,21.8,19.4,17.4,20.2,19.8,17.8, 17.4,19.8,20.2,19,20.2,18.2,21,19.8) mean(battery) # 样本均值 ## [1] 19.824 答：可以计算得到以上50部手机的续航时间均值为19.824。 正态总体参数估计 使用模拟的方法，估计来自于正态总体的均值\\(\\mu\\)和方差\\(\\sigma^2\\)。 # 例2 set.seed(123) data1 &lt;- rnorm(n = 1000, mean = 5, sd = 2) # 产生服从正态分布的随机数 mean(data1) # 样本均值 ## [1] 5.032256 var(data1) # 样本方差 ## [1] 3.933836 产生1000个服从\\(N(5,4)\\)的随机数，分别求这个样本的一阶原点矩（均值）和二阶中心矩（方差），作为总体均值和方差的估计。结果显示，对总体均值的估计为5.03，总体方差的估计为3.93。 5.2.2 极大似然估计 苹果手机续航时间 假设苹果公司发布的新一代手机iPhone SSR续航时间是服从正态分布\\(N(\\mu, \\sigma^2)\\)，其中，\\(\\mu\\) 是总体均值，\\(\\sigma^2\\) 是总体方差。请根据案例数据中的50个续航时间样本使用最大似然估计的方法，估计总体的均值和方差。 cat(&#39;苹果手机电池续航数据为: &#39;, battery, &#39;\\n&#39;) ## 苹果手机电池续航数据为: 20.2 19.4 18.6 18.2 19.8 21.4 18.2 19.8 17.4 19.4 21.4 19 19 17 23 22.6 21.4 20.2 22.6 20.2 21.4 20.2 18.2 20.2 19 19.8 19.8 20.6 20.6 18.6 19 21.4 20.6 21.8 19.4 19.8 21.8 19.4 17.4 20.2 19.8 17.8 17.4 19.8 20.2 19 20.2 18.2 21 19.8 # 续航时间均值的极大似然估计是 mu_mle &lt;- mean(battery) # 续航时间方差的极大似然估计是 sigma_mle &lt;- mean((battery - mean(battery)) ** 2) cat(&#39;均值的最大似然估计值是: &#39;, mu_mle, &#39;方差的最大似然估计值是: &#39;, sigma_mle, &#39;\\n&#39;) ## 均值的最大似然估计值是: 19.824 方差的最大似然估计值是: 1.948224 根据极大似然估计的方法，可以得到手机电池续航时间均值的最大似然估计值是19.824 方差的最大似然估计值是1.948224。 均匀分布总体参数估计 设\\(x_1, ..., x_n\\)是从均匀分布总体 \\(U(0,\\theta)\\) 中抽出的一个样本，观测值如下： 6.0627 9.3764 2.6435 3.8009 8.0748 9.7808 9.5793 7.6273 5.0965 0.6448 6.4357 9.1591 0.9523 2.9537 7.6993 2.5589 5.179 6.7785 1.4723 7.0053 试求解\\(\\theta\\)的最大似然估计。 set.seed(6) theta = 10 # 生成的样本是 samples = runif(n=20, 0, theta) samples = round(samples, 4) cat(&#39;样本是: &#39;, samples, &#39;\\n&#39;) ## 样本是: 6.0627 9.3764 2.6435 3.8009 8.0748 9.7808 9.5793 7.6273 5.0965 0.6448 6.4357 9.1591 0.9523 2.9537 7.6993 2.5589 5.179 6.7785 1.4723 7.0053 # theta 的最大似然估计值是 theta_mle = max(samples) cat(&#39;theta的最大似然估计值是: &#39;, theta_mle, &#39;\\n&#39;) ## theta的最大似然估计值是: 9.7808 在此案例数据中，均匀分布总体的参数\\(\\theta\\)极大似然估计值是9.7808。 5.2.3 区间估计 手机续航时间的区间估计 已知上述案例中苹果手机的续航时间服从正态分布，其标准差为2小时，试求该型号手机续航时间的0.95置信区间。可以直接使用z.test()函数，也可以根据公式进行计算。 # 例5：续航时间区间估计 # 方法一：按照公式计算 mu &lt;- mean(battery) sigma &lt;- 2 n &lt;- 50 spread &lt;- qnorm(1-0.05/2, 0, 1) * sigma / sqrt(n) cat(sprintf(&#39;续航时间的置信区间是: [%s, %s]&#39;, round(mu - spread, 4), round(mu + spread, 4), &#39;\\n&#39;)) ## 续航时间的置信区间是: [19.2696, 20.3784] # 方法二：使用z.test()计算 library(BSDA) interval_1 &lt;- z.test(battery, sigma.x = sigma, conf.level = 0.95)$conf.int cat(sprintf(&#39;续航时间的置信区间是: [%s, %s]&#39;, round(interval_1[1], 4), round(interval_1[2], 4), &#39;\\n&#39;)) ## 续航时间的置信区间是: [19.2696, 20.3784] 无论按照公式计算，还是直接使用R中自带的函数，续航时间的置信区间估计都是 [19.2696, 20.3784]。 红楼梦词频区间估计 为了了解红楼梦在前、中、后三个部分对虚词使用的习惯，拟对不同章节的“之”字词频进行分析。假设小说对文言虚词的使用频率服从正态分布，试求第41-80回和第81-120回虚词“之”词频差的0.95置信区间。（注意：请区分两总体方差是否相等两种情况讨论） # 例6 wenyan &lt;- read.csv(&quot;./data/红楼梦虚词词频统计.csv&quot;, stringsAsFactors = F, fileEncoding = &quot;utf-8&quot;) freqx &lt;- wenyan$`之`[41:80] freqy &lt;- wenyan$`之`[81:120] m = length(freqx); n = length(freqy) u_x = mean(freqx); u_y = mean(freqy) s_x = sd(freqx); s_y = sd(freqy) 如果两部分数据的方差相等: ## 方法一：使用公式计算 s_w = sqrt(((m-1)*s_x^2 + (n-1)*s_y^2) / (m+n-2)) spread = qt(1 - 0.05/2, m+n-2) * s_w * sqrt(1/m + 1/n) cat(sprintf(&#39;如果两部分数据方差相等，第41-80回和第81-120回“之”词频差的置信区间是: [%s, %s]&#39;, round(u_x - u_y - spread, 4), round(u_x - u_y + spread, 4), &#39;\\n&#39;)) ## 如果两部分数据方差相等，第41-80回和第81-120回“之”词频差的置信区间是: [8.9824, 20.3176] ## 方法二：使用t.test()计算 interval_3 &lt;- t.test(freqx, freqy, var.equal = T, conf.level = 0.95)$conf.int cat(sprintf(&#39;如果两部分数据方差相等，第41-80回和第81-120回“之”词频差的置信区间是: [%s, %s]&#39;, round(interval_3[1], 4), round(interval_3[2], 4), &#39;\\n&#39;)) ## 如果两部分数据方差相等，第41-80回和第81-120回“之”词频差的置信区间是: [8.9824, 20.3176] 如果两部分数据方差不等： ## 方法一：使用公式计算 s_o = sqrt(s_x^2 / m + s_y^2 / n) freedom = s_o^4 / (s_x^4/(m^2 * (m-1)) + s_y^4/(n^2 * (n-1))) freedom = round(freedom, 0) spread = qt(1 - 0.05/2, freedom) * s_o cat(sprintf(&#39;如果两部分数据方差不等，第41-80回和第81-120回“之”词频差的置信区间是: [%s, %s]&#39;, round(u_x - u_y - spread, 4), round(u_x - u_y + spread, 4), &#39;\\n&#39;)) ## 如果两部分数据方差不等，第41-80回和第81-120回“之”词频差的置信区间是: [8.9291, 20.3709] ## 方法二：使用t.test()计算 interval_4 &lt;- t.test(freqx, freqy, var.equal = F, conf.level = 0.95)$conf.int cat(sprintf(&#39;如果两部分数据方差不等，第41-80回和第81-120回“之”词频差的置信区间是: [%s, %s]&#39;, round(interval_4[1], 4), round(interval_4[2], 4), &#39;\\n&#39;)) ## 如果两部分数据方差不等，第41-80回和第81-120回“之”词频差的置信区间是: [8.928, 20.372] 如果两部分数据方差相等，第41-80回和第81-120回“之”词频差的置信区间是: [8.9824, 20.3176]；如果两部分数据方差不等，第41-80回和第81-120回“之”词频差的置信区间是: [8.9291, 20.3709]。在两总体方差不等的情况中，对t统计量的自由度做了近似处理，因此两种方法计算得到的结果稍有不同。 5.3 假设检验 苹果手机续航时间 假设苹果手机SSR的续航时间服从正态分布\\(N(\\theta, 0.8^2)\\)，其中\\(\\theta\\)的设计值不低于20小时。根据案例引入中给出的50次抽样检测数据，判断该苹果手机的续航时间是否满足出厂设计的要求？除了使用公式计算p值的方法。对于这个问题，你还可以尝试使用BSDA包中的z.test()和t.test()函数（假设总体方差未知的情况）进行检验。 battery &lt;- c(20.2,19.4,18.6,18.2,19.8,21.4,18.2,19.8,17.4,19.4,21.4,19,19,17, 23,22.6,21.4,20.2,22.6,20.2,21.4,20.2,18.2,20.2,19,19.8,19.8,20.6, 20.6,18.6,19,21.4,20.6,21.8,19.4,19.8,21.8,19.4,17.4,20.2,19.8,17.8, 17.4,19.8,20.2,19,20.2,18.2,21,19.8) a &lt;- 20 s &lt;- 0.8 n &lt;- 50 xbar &lt;- mean(battery) p_value &lt;- pnorm(xbar, mean = a, sd = s/sqrt(n)) print(paste0(&quot;检验的p值为&quot;, round(p_value, 4))) ## [1] &quot;检验的p值为0.0599&quot; 检验的p值为0.0599。 当\\(\\alpha&lt;0.0599\\)时，\\(z_\\alpha&lt;-1.56\\)，由于拒绝域为\\(W=\\{z≤z_\\alpha\\}\\)，于是观测值\\(z=-1.56\\)不在拒绝域里，应接受原假设； 当\\(\\alpha≥0.0599\\)时，\\(z_\\alpha≥-1.56\\)，由于拒绝域为\\(W=\\{z≤z_\\alpha\\}\\)，于是观测值\\(z=-1.56\\)落在拒绝域里，应拒绝原假设； 由此可以看出，0.0599是能用观测值\\(z=-1.56\\)做出“拒绝\\(H_0\\)”的最小的显著性水平。 # 使用检验函数`z.test()`和`t.test()` library(BSDA) # 单样本均值的单边检验 z.test(battery, mu = 20, sigma.x = 0.8, alternative = &quot;less&quot;, conf.level = 0.95) ## ## One-sample z-Test ## ## data: battery ## z = -1.5556, p-value = 0.0599 ## alternative hypothesis: true mean is less than 20 ## 95 percent confidence interval: ## NA 20.01009 ## sample estimates: ## mean of x ## 19.824 # 单样本均值的单边检验（方差未知） t.test(battery, mu = 20, alternative = &quot;less&quot;, conf.level = 0.95) ## ## One Sample t-test ## ## data: battery ## t = -0.88266, df = 49, p-value = 0.1909 ## alternative hypothesis: true mean is less than 20 ## 95 percent confidence interval: ## -Inf 20.1583 ## sample estimates: ## mean of x ## 19.824 使用R中自带的检验函数，在总体方差已知的情况下，检验p值为0.0599；而在总体方差未知的情况下，检验的p值为0.1909。 红楼梦虚词频率 假设小说对文言虚词的使用频率服从正态分布，且词频方差在不同阶段保持不变，在显著性水平\\(\\alpha=0.05\\)下，尝试判断41-80回中“亦”使用的频数均值是否比81-120回中高？ # 红楼梦 wenyan &lt;- read.csv(&quot;./data/红楼梦虚词词频统计.csv&quot;, stringsAsFactors = F, fileEncoding = &quot;UTF-8&quot;) freqx &lt;- wenyan$`亦`[41:80] freqy &lt;- wenyan$`亦`[81:120] t.test(freqx, freqy, alternative = &quot;greater&quot;, var.equal = T, conf.level = 0.95) ## ## Two Sample t-test ## ## data: freqx and freqy ## t = 4.4123, df = 78, p-value = 1.621e-05 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 1.961594 Inf ## sample estimates: ## mean of x mean of y ## 3.90 0.75 由于检验的P值小于显著性水平0.05，因此拒绝原假设，认为41-80回“亦”使用的平均数量显著高于81-120回，后期语言向白话文靠拢。 狗熊制药成对样本检验 假设狗熊会开了一个狗熊制药公司，专业研究减肥药。如果这个药品要在美国市场上合法上市，必须有美国食品药品管理局（FDA）的批准。为此，FDA对使用该药物的受试者做了一组试验，记录了其在使用减肥药前后的体重（单位：kg）。试验的数据分别为 受试者编号 1 2 3 4 5 6 7 8 9 10 服用药物前的体重x 43 55 49 62 59 49 57 54 55 48 服用药物后的体重y 50 59 55 60 58 54 56 53 61 51 差 d = x-y -7 -4 -6 2 1 -5 1 1 -6 - 3 的狗熊制药案例数据中，假定受试者的体重数据服从正态分布，试问：试验前后的受试者体重在显著性水平\\(\\alpha=0.05\\)上有无差异？ # 成对数据检验 x &lt;- c(43,55,49,62,59,49,57,54,55,48) y &lt;- c(50,59,55,60,58,54,56,53,61,51) # 成对数据t检验 t.test(x, y, alternative = &quot;two.sided&quot;, paired = T, var.equal = F, conf.level = 0.95) ## ## Paired t-test ## ## data: x and y ## t = -2.3475, df = 9, p-value = 0.04348 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -5.10545182 -0.09454818 ## sample estimates: ## mean of the differences ## -2.6 检验的p值为0.0435，故应拒绝原假设\\(H_0:\\mu=0\\)，即可认为试验前后的受试者体重有显著差异。进一步，平均含量差值的估计量为\\(\\hat \\mu = \\bar x - \\bar y = -2.6\\)，可见服用药物后的体重要高于服用药物前的体重。 习题答案 题目 5.2 假设某产品在制造时分为合格品与不合格品两类，我们用一个随机变量X来表示某个产品经检查后是否合格，X=0表示合格品，X=1表示不合格品，则X服从两点分布\\(b(1,p)\\)，其中p是未知的不合格品率。现抽取20个产品，看其是否合格，得到样本如下所示： 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 求p的最大似然估计，并在R中进行实现。 set.seed(4) p = 0.5 # 生成的样本是 samples = rbinom(n=20, size=1, prob = p) print(samples) ## [1] 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 # p 的最大似然估计值是 p_mle = mean(samples) cat(&#39;均值的最大似然估计值是: &#39;, p_mle, &#39;\\n&#39;) ## 均值的最大似然估计值是: 0.55 均值的最大似然估计值是:0.55。 题目 5.3 假设灯泡的寿命服从正态分布。为了估计某种灯泡的平均寿命，现随机地抽取15只灯泡测试，得到它们的寿命（单位：小时）如下： 998 1021 988 986 1018 997 996 973 925 985 981 1007 1011 968 1002 试求灯泡平均寿命的0.95置信区间，并在R中进行实现。 set.seed(8) n = 15 u = 1000 samples = rnorm(n, u, 25) samples = round(samples, 0) cat(&#39;寿命样本: &#39;, samples, &#39;\\n&#39;) ## 寿命样本: 998 1021 988 986 1018 997 996 973 925 985 981 1007 1011 968 1002 # 区间估计 # 方法一：按照公式计算 weight_u = mean(samples) weight_var = sum((samples - weight_u) ^ 2) / (n-1) spread = qt(1-0.05/2, n-1) * sqrt(weight_var / n) cat(sprintf(&#39;灯泡寿命的置信区间是: [%s, %s]&#39;, round(weight_u - spread, 4), round(weight_u + spread, 4), &#39;\\n&#39;)) ## 灯泡寿命的置信区间是: [977.2537, 1003.5463] # 方法二：使用t.test()计算 interval_2 &lt;- t.test(samples, conf.level = 0.95)$conf.int cat(sprintf(&#39;灯泡寿命的置信区间是: [%s, %s]&#39;, round(interval_2[1], 4), round(interval_2[2], 4), &#39;\\n&#39;)) ## 灯泡寿命的置信区间是: [977.2537, 1003.5463] 无论使用公式还是使用R中的自带函数计算，灯泡寿命的置信区间都是: [977.2537, 1003.5463]。 "],["ch6.html", "第6章：线性回归 案例引入 6.9 模型实现 习题答案", " 第6章：线性回归 ### 数据准备 ### # 清空工作空间 rm(list = ls()) 案例引入 随着“互联网+”和大数据时代的到来，越来越多的数据科学公司如雨后春笋般涌现。传统行业也面临着“互联网+”时代下的创新转型，对于数据分析及相关领域有大量人才需求，各行各业与数据分析相关的招聘岗位越来越多。 在数据分析相关岗位的招聘中，合理定位岗位薪资，找出与岗位薪资挂钩的特殊技能尤其关键。在市场层面上，可以了解数据分析人才市场现状，合理化市场资源配置；在公司层面上，可以为公司招聘提供借鉴，为数据分析人才定制薪资提供参考；对于应聘者而言，能够更科学地进行职业测评，实现准确地自我定位；对于高校来说，能够明确学生的培养方向，优化应用统计及数据分析方向人才培养方案。 本案例使用数据分析岗位招聘薪酬数据集。该数据收集自各大招聘网站发布的数据分析岗位招聘的相关信息，共包含了6682条岗位招聘数据。数据集的每一列分别对应：岗位薪资、是否要求掌握R、SPSS、Excel、Python、MATLAB、Java、SQL、SAS、Stata、EViews、Spark、Hadoop、公司类别、公司规模、学历要求、工作经验、公司地点。数据变量说明表如下所示： 变量类型 变量名称 详细说明 取值范围 因变量 薪资 单位：元／月 1500-5000元 自变量 软件要求 R 共2个水平 SPSS 共2个水平 Excel 共2个水平 Python 共2个水平 MATLAB 共2个水平 Java 共2个水平 SQL 共2个水平 SAS 共2个水平 Stata 共2个水平 EViews 共2个水平 Spark 共2个水平 Hadoop 共2个水平 公司类别 共6个水平 合资、外资、民营公司等 公司规模 共6个水平 少于50人、50-500人等 学历要求 共7个水平 无、中专、高中、大专等 工作经验 单位：年 0-10年 地区 共2个水平 北上深、非北上深 读入数据，将数据命名为jobinfo，并绘制岗位薪资的频数直方图，解读图中的结果。 library(ggplot2) jobinfo &lt;- read.csv(&quot;./data/jobinfo_ch7.csv&quot;, fileEncoding = &quot;utf-8&quot;) ggplot(data = jobinfo,aes(x=aveSalary)) + geom_histogram(binwidth = 2000,fill=&quot;gold&quot;) + labs(y=&quot;频数&quot;,x = &quot;岗位薪资&quot;) + theme_bw() + theme(panel.border=element_blank(), text = element_text(family = &quot;STHeiti&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 11)) 该数据中的因变量薪资呈右偏分布，如图所示。最高的月薪为19999.5元/月，对应的岗位是一个规模为1000-5000人的国企，这个岗位要求申请人有2年的工作经验。从整体情况来看，有75%的岗位月薪低于10000元。 岗位学历要求对薪资水平是否有影响 通过箱线图，探究岗位学历要求对薪资水平是否有影响。（提示：为了使得箱线图更加美观，可以对因变量取对数） # 利用箱线图画出，学历vs对数平均薪资的分布，箱体的宽度越宽表示样本量越多 # 将学历转化为因子型变量，便于画图 jobinfo$学历要求 = factor(jobinfo$学历要求,levels=c(&quot;中专&quot;,&quot;高中&quot;,&quot;大专&quot;,&quot;无&quot;,&quot;本科&quot;,&quot;研究生&quot;)) jobinfo$对数薪资 &lt;- log(jobinfo$aveSalary) # 绘制箱线图 ggplot(jobinfo,aes(学历要求,对数薪资)) + geom_boxplot(varwidth = TRUE, fill = c(rep(&quot;grey&quot;,4),rep(&quot;gold&quot;,2))) + labs(x=&quot;学历要求&quot;, y = &quot;对数薪资&quot;)+ theme_bw() + theme(panel.border=element_blank(), text = element_text(family = &quot;STHeiti&quot;), axis.title = element_text(size = 13), axis.text = element_text(size = 12)) 从图中可以看到，在五种学历要求中，研究生学历的薪资中位数最高，达到了8999.75元，对应的对数薪资为9.10；其次为本科学历的岗位，达到了8999.50元；薪资水平最低的为中专学历，月薪仅有5249.50元。水平最高的学历岗位比最低的月薪高出了3750.25元，但是仅根据描述分析，我们仍然不能说明学历对薪资有统计意义上的显著影响。 Python和SPSS要求与否对薪资水平是否有影响 通过箱线图，分别探究岗位对于Python和SPSS要求与否对薪资水平是否有影响。 ggplot(jobinfo,aes(as.factor(Python),对数薪资)) + geom_boxplot(fill = c(&quot;grey&quot;,&quot;gold&quot;)) + labs(x=&quot;是否要求会使用Python&quot;, y = &quot;对数薪资&quot;) + theme_bw() + theme(panel.border=element_blank(), text = element_text(family = &quot;STHeiti&quot;), axis.title = element_text(size = 15), axis.text = element_text(size = 14)) ggplot(jobinfo,aes(as.factor(SPSS),对数薪资)) + geom_boxplot(fill = c(&quot;grey&quot;,&quot;gold&quot;)) + labs(x=&quot;是否要求会使用SPSS&quot;, y = &quot;对数薪资&quot;) + theme_bw() + theme(panel.border=element_blank(), text = element_text(family = &quot;STHeiti&quot;), axis.title = element_text(size = 15), axis.text = element_text(size = 14)) 从数目上看，要求掌握Python的岗位占比为4.4%，要求掌握SPSS的岗位占比约为8.0%。从箱线图中可以看出，要求掌握这两种软件的薪资中位数均高于不要求掌握该软件的岗位。 6.9 模型实现 6.9.2 实例分析 建立线性模型 首先以岗位薪资为因变量，以地区、公司类别、公司规模、学历、经验要求以及12种软件需求为自变量，探求这些比变量对于薪资的影响，建立回归模型，并利用summary函数查看回归结果。 需要注意，在在建立回归模型之前，我们需要利用公司所在地、公司类别、公司规模、学历要求创建虚拟变量。回归时，公司类别以国企为基准，公司规模以少于50人为基准，学历以无为基准。 # 数据预处理 ## 转换为factor型变量，地区以河北为基准，公司类别以国企为基准，公司规模以少于50人为基准，学历以无为基准 jobinfo$公司类别 &lt;- factor(jobinfo$公司类别, levels = c(&quot;国企&quot;,&quot;合资&quot;,&quot;外资&quot;,&quot;上市公司&quot;,&quot;民营公司&quot;,&quot;创业公司&quot;)) jobinfo$公司规模 &lt;- factor(jobinfo$公司规模, levels = c(&quot;少于50人&quot;,&quot;50-500人&quot;,&quot;500-1000人&quot;,&quot;1000-5000人&quot;,&quot;5000-10000人&quot;,&quot;10000人以上&quot;)) jobinfo$学历要求 &lt;- factor(jobinfo$学历要求, levels = c(&quot;无&quot;,&quot;中专&quot;,&quot;高中&quot;,&quot;大专&quot;,&quot;本科&quot;,&quot;研究生&quot;)) ## 软件要求 for (i in c(2:13)){ jobinfo[,i] &lt;- as.factor(jobinfo[,i]) } ## 建立线性模型 lm.fit1 = lm(aveSalary ~ . - 对数薪资, data = jobinfo) ## 查看回归结果 summary(lm.fit1) ## ## Call: ## lm(formula = aveSalary ~ . - 对数薪资, data = jobinfo) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10071.2 -2153.6 -676.5 1654.8 13515.5 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6768.910 254.284 26.619 &lt; 2e-16 *** ## R1 466.596 218.212 2.138 0.03253 * ## SPSS1 423.658 207.874 2.038 0.04158 * ## Excel1 -970.032 97.652 -9.934 &lt; 2e-16 *** ## Python1 718.776 234.923 3.060 0.00222 ** ## MATLAB1 -75.241 302.845 -0.248 0.80380 ## Java1 568.885 258.771 2.198 0.02795 * ## SQL1 1275.237 148.395 8.594 &lt; 2e-16 *** ## SAS1 332.544 227.232 1.463 0.14339 ## Stata1 -1062.972 814.138 -1.306 0.19172 ## EViews1 419.079 844.015 0.497 0.61954 ## Spark1 -3.709 436.855 -0.008 0.99323 ## Hadoop1 1736.165 330.751 5.249 1.58e-07 *** ## 公司类别合资 723.833 236.596 3.059 0.00223 ** ## 公司类别外资 293.873 234.330 1.254 0.20985 ## 公司类别上市公司 597.552 264.734 2.257 0.02403 * ## 公司类别民营公司 378.531 211.465 1.790 0.07349 . ## 公司类别创业公司 893.262 402.988 2.217 0.02668 * ## 公司规模50-500人 282.346 111.355 2.536 0.01125 * ## 公司规模500-1000人 77.016 149.989 0.513 0.60763 ## 公司规模1000-5000人 427.060 156.551 2.728 0.00639 ** ## 公司规模5000-10000人 339.565 278.032 1.221 0.22201 ## 公司规模10000人以上 329.871 229.577 1.437 0.15080 ## 学历要求中专 -1432.439 272.913 -5.249 1.58e-07 *** ## 学历要求高中 -1626.832 318.650 -5.105 3.39e-07 *** ## 学历要求大专 -930.555 121.848 -7.637 2.53e-14 *** ## 学历要求本科 776.026 126.739 6.123 9.71e-10 *** ## 学历要求研究生 1725.095 286.849 6.014 1.91e-09 *** ## 工作经验 682.515 23.073 29.580 &lt; 2e-16 *** ## 地区非北上深 -2515.269 102.062 -24.645 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3105 on 6652 degrees of freedom ## Multiple R-squared: 0.3199, Adjusted R-squared: 0.3169 ## F-statistic: 107.9 on 29 and 6652 DF, p-value: &lt; 2.2e-16 模型诊断 对任务四中的模型进行诊断，找出可能存在的问题。 ## 对线性模型进行回归诊断 # 将画布分为2*2的4块 par(mfrow=c(2,2)) plot(lm.fit1, which = c(1:4)) 左上图为残差与拟合值的散点图，若因变量和自变量呈现线性相关的关系，则残差值与拟合值没有任何的系统关联。图中所示残差并不随着拟合值的变化呈现规律性变化，因此基本满足线性假设；右上图为正态Q-Q图，当因变量服从正态分布时，图中的散点应该落在呈45度倾斜的直线上。根据图示，模型中的残差项在较大值的部分偏离直线，因此较大程度上偏离了正态分布；左下图为位置尺度图，若满足同方差假定，则水平线周围的点应呈现无规律随机分布，根据图中所示，随着拟合值增大，标准化残差呈现升高的规律，表明存在一定的异方差问题；右下图为样本点的库克距离，其最大值未超过0.05，因此认为样本中不存在异常点。 模型修正与模型选择 可将因变量进行对数变换，重新拟合回归模型，使用BIC准则对变量进行选择，并对变量选择后的模型结果进行解读。 ## 计算对数因变量 jobinfo$对数薪资 &lt;- log(jobinfo$aveSalary) # 建立对数线性模型，剔除平均薪资变量 lm.fit2 = lm(对数薪资 ~ .-aveSalary, data = jobinfo) ## 使用BIC准则选择模型 n &lt;- nrow(jobinfo) lm.bic &lt;- step(lm.fit2, direction = &quot;both&quot;, k = log(n), trace = F) summary(lm.bic) ## ## Call: ## lm(formula = 对数薪资 ~ SPSS + Excel + Python + SQL + Hadoop + ## 学历要求 + 工作经验 + 地区, data = jobinfo) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.54248 -0.25992 -0.02571 0.25825 1.43138 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.827384 0.013680 645.276 &lt; 2e-16 *** ## SPSS1 0.092435 0.018706 4.941 7.94e-07 *** ## Excel1 -0.117721 0.011646 -10.108 &lt; 2e-16 *** ## Python1 0.110596 0.024510 4.512 6.53e-06 *** ## SQL1 0.160374 0.017389 9.223 &lt; 2e-16 *** ## Hadoop1 0.191131 0.030947 6.176 6.96e-10 *** ## 学历要求中专 -0.202515 0.032791 -6.176 6.97e-10 *** ## 学历要求高中 -0.223880 0.038146 -5.869 4.60e-09 *** ## 学历要求大专 -0.116834 0.014630 -7.986 1.63e-15 *** ## 学历要求本科 0.094769 0.015130 6.263 4.00e-10 *** ## 学历要求研究生 0.201863 0.034147 5.912 3.56e-09 *** ## 工作经验 0.084348 0.002766 30.496 &lt; 2e-16 *** ## 地区非北上深 -0.371999 0.011976 -31.061 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3732 on 6669 degrees of freedom ## Multiple R-squared: 0.3436, Adjusted R-squared: 0.3424 ## F-statistic: 290.9 on 12 and 6669 DF, p-value: &lt; 2.2e-16 在控制其他因素不变的情况下，对数据分析人员的工作经验年限要求每多一年，相应岗位的薪资就平均高出8.4%。 说明要求掌握Python的岗位，薪资平均比不要求掌握Python的岗位高出11.1%，这和任务三中的描述分析趋势是吻合的。 自变量“学历要求”的基准水平为“无”，那么“研究生”对应的系数0.202可解读为：要求研究生学历的岗位薪资平均比不要求任何学历的岗位高20.2%。 模型预测 使用任务六得到的模型，对新样本的薪资水平进行预测。假设有一份北京市上市公司数据分析岗位的工作，该公司为1500人的中小型公司，这个工作要求申请人掌握R、Python、SQL和Hadoop的技能，并且有至少3年的工作经验，最低学历为硕士。希望通过模型预测该岗位的薪资。 ## 新样本 testdata &lt;- data.frame(R = 1, SPSS = 0, Excel = 0, Python = 1, MATLAB = 0, Java = 0, SQL = 1, SAS = 0, Stata = 0, EViews = 0, Spark = 0, Hadoop = 1, 公司类别 = &quot;上市公司&quot;, 公司规模 = &quot;1000-5000人&quot;, 学历要求 = &quot;研究生&quot;, 工作经验 = 3, 地区 = &quot;北上深&quot;) ## 将软件技能转换为factor类型 for (i in c(1:12)) { testdata[,i] &lt;- as.factor(testdata[,i]) } logsalary_hat &lt;- predict(lm.bic, newdata = testdata) # 预测值 sigma_hat2 &lt;- sum(lm.bic$residuals^2)/lm.bic$df.residual # sigma^2估计值 y_hat &lt;- exp(logsalary_hat + sigma_hat2/2) # cat(&quot;平均薪资水平约为&quot;, round(y_hat, 2), &quot;元/月&quot;) ## 平均薪资水平约为 18288.72 元/月 根据模型的预测结果，该工作岗位薪资约为18288.72元/月。 习题答案 案例背景 截至2016年5月25日的北京住宅年内交易数据显示，北京市已经全面进入二手房时代。二手房定价是二手房交易过程中重要的环节之一。若能根据住房的特征，更准确地估计价格，住房业主将会获得更准确的市场定位。 数据集house.csv为来自某二手房中介网站的北京在售二手房2016年5月的相关数据，共包括单位面积房价（price）、城区（CATE）、卧室数（bedrooms）、厅数（halls）、房屋面积（AREA）、楼层（floor）、是否临近地铁（subway）、是否是学区房（school）这几个变量。以房价为因变量，在R中建立普通线性回归模型，并对模型结果进行诊断。 题目 6.2 数据读入与预处理 读入数据，命名为dat0，并将“厅数”变量处理为“有厅”、“无厅”和“其他”三个等级。 # 清除工作环境 cat(&quot;\\014&quot;) rm(list=ls()) dat0 &lt;- read.csv(&quot;./data/house.csv&quot;,header=T,fileEncoding = &quot;utf-8&quot;) #读入数据 # 处理厅数 n=dim(dat0)[1] style=rep(&quot;其他&quot;,n) style[which(dat0$halls==0)]=&quot;无厅&quot; style[which(dat0$halls&gt;0)]=&quot;有厅&quot; style=factor(style,levels=c(&quot;无厅&quot;,&quot;有厅&quot;)) dat0 &lt;- cbind(dat0,style) 模型的建立与诊断 以房价为因变量，建立普通线性回归模型，并对模型结果进行诊断。 lm1 &lt;- lm(price~CATE+school+subway+style+floor+bedrooms+AREA,data=dat0) summary(lm1) #回归结果展示 ## ## Call: ## lm(formula = price ~ CATE + school + subway + style + floor + ## bedrooms + AREA, data = dat0) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.6695 -0.8872 -0.1489 0.7983 8.0933 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.190659 0.070500 59.442 &lt; 2e-16 *** ## CATE东城 1.567919 0.038929 40.276 &lt; 2e-16 *** ## CATE丰台 -0.743800 0.038178 -19.482 &lt; 2e-16 *** ## CATE海淀 1.316134 0.038621 34.079 &lt; 2e-16 *** ## CATE石景山 -0.875123 0.043665 -20.042 &lt; 2e-16 *** ## CATE西城 2.830479 0.039980 70.797 &lt; 2e-16 *** ## school 1.183122 0.027768 42.607 &lt; 2e-16 *** ## subway 0.672102 0.030914 21.741 &lt; 2e-16 *** ## style有厅 0.171935 0.052748 3.260 0.00112 ** ## floorlow 0.198526 0.027815 7.137 9.92e-13 *** ## floormiddle 0.152117 0.027142 5.605 2.12e-08 *** ## bedrooms 0.111101 0.020200 5.500 3.86e-08 *** ## AREA -0.002780 0.000376 -7.393 1.51e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.427 on 16197 degrees of freedom ## Multiple R-squared: 0.5904, Adjusted R-squared: 0.5901 ## F-statistic: 1945 on 12 and 16197 DF, p-value: &lt; 2.2e-16 par(mfrow=c(2,2)) #画2*2的图 plot(lm1,which=c(1:4)) #模型诊断图，存在异方差现象，对因变量取对数 根据模型诊断结果可以看出，残差近似满足零均值假定，图中所示残差并不随着拟合值的变化呈现规律性变化，因此基本满足线性假设；右上图为正态Q-Q图，当因变量服从正态分布时，图中的散点应该落在呈45度倾斜的直线上。根据图示，模型中的残差项在较大值的部分偏离直线，因此一定程度上偏离了正态分布；左下图为位置尺度图，若满足同方差假定，则水平线周围的点应呈现无规律随机分布，根据图中所示，随着拟合值增大，标准化残差呈现升高的规律，表明存在一定的异方差问题；右下图为样本点的库克距离，其最大值未超过0.05，因此认为样本中不存在异常点。 题目 6.3 对于任务二中的数据，建立对数线性模型，并加入城区与学区的交互项，对系数进行解读。 lm2 &lt;- lm(log(price) ~ CATE * school + subway + style + floor + bedrooms + AREA, data = dat0) summary(lm2) #回归结果展示 ## ## Call: ## lm(formula = log(price) ~ CATE * school + subway + style + floor + ## bedrooms + AREA, data = dat0) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.22784 -0.14546 -0.00519 0.14964 1.00392 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.459e+00 1.142e-02 127.742 &lt; 2e-16 *** ## CATE东城 2.283e-01 7.580e-03 30.114 &lt; 2e-16 *** ## CATE丰台 -1.754e-01 6.458e-03 -27.162 &lt; 2e-16 *** ## CATE海淀 1.937e-01 7.585e-03 25.543 &lt; 2e-16 *** ## CATE石景山 -2.184e-01 7.248e-03 -30.132 &lt; 2e-16 *** ## CATE西城 3.993e-01 8.168e-03 48.882 &lt; 2e-16 *** ## school 9.796e-02 1.045e-02 9.372 &lt; 2e-16 *** ## subway 1.257e-01 4.934e-03 25.475 &lt; 2e-16 *** ## style有厅 2.709e-02 8.401e-03 3.225 0.00126 ** ## floorlow 3.433e-02 4.426e-03 7.757 9.18e-15 *** ## floormiddle 2.638e-02 4.318e-03 6.110 1.02e-09 *** ## bedrooms 1.427e-02 3.216e-03 4.437 9.18e-06 *** ## AREA -3.525e-04 5.987e-05 -5.888 3.99e-09 *** ## CATE东城:school 9.261e-02 1.357e-02 6.826 9.07e-12 *** ## CATE丰台:school 1.684e-02 2.603e-02 0.647 0.51770 ## CATE海淀:school 1.096e-01 1.344e-02 8.160 3.61e-16 *** ## CATE石景山:school -2.780e-01 5.480e-02 -5.073 3.95e-07 *** ## CATE西城:school 8.577e-02 1.362e-02 6.296 3.13e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2271 on 16192 degrees of freedom ## Multiple R-squared: 0.6112, Adjusted R-squared: 0.6108 ## F-statistic: 1497 on 17 and 16192 DF, p-value: &lt; 2.2e-16 经过对数处理，异方差问题得到了极大改善；Cook距离表现正常，表明没有异常点。另外，模型仍然是显著的（F检验通过了），模型的拟合程度有略微上升（调整的\\(R^2=0.6108\\)）。 当考虑了“城区×学区”的交互效应后，一个明显的变化是：学区变量系数估计变成负数。由于“城区”、“城区×学区”两个因素的基准组均为石景山区，因此对应的结论是：在石景山区，学区房比非学区房的单位面积房价低。可以从石景山区的学区房比例和学区资源等角度解释原因。 其他变量的解读示例：在保持其他变量不变的情况下，临近地铁的房价比不临近地铁的房价更贵；与高楼层的房源相比，中低楼层的二手房房价更高。 题目 6.4 使用BIC准则对任务三的模型进行选择，使用最终的模型进行新样本单位面积房价的预测。其中，预测样本为一间海淀区的两室一厅学区房，其在楼中的低楼层，并且临近地铁，房屋面积为70平方米。 lm3.bic &lt;- step(lm2, k = log(nrow(dat0)), trace = F) newdata &lt;- data.frame(CATE = &quot;海淀&quot;, bedrooms = 2, halls = 1, AREA = 70, floor = &quot;low&quot;, subway = 1, school = 1, style = &quot;有厅&quot;) logprice_hat &lt;- predict(lm3.bic, newdata = newdata) # 预测值 sig_hat2 &lt;- sum(lm3.bic$residuals^2)/lm3.bic$df.residual # sigma^2估计值 y_hat &lt;- exp(logprice_hat + sig_hat2/2) # cat(&quot;单位面积房价约为&quot;, round(y_hat, 2), &quot;万元/平方米&quot;) ## 单位面积房价约为 7.98 万元/平方米 使用任务三中的模型可以得到，新样本的单位面积房价预测值约为7.98万元/平方米。 "],["ch7.html", "第7章：逻辑回归 案例引入 7.5 实例分析 习题答案", " 第7章：逻辑回归 ### 数据准备 ### # 清空工作空间 rm(list = ls()) 案例引入 近十年来，出国留学已经列入越来越多的同学的“未来规划清单”，根据教育部数据，2018年在我国出国留学人数66.21万人，同比增长8.83%。其中，自费留学人数占比最多，达59.63万人，占总留学人数的90%，国家公派3.02万人，单位公派3.56万人。从留学生选择留学国家/地区来看，留学目的地选择多样化，美国依旧是留学人群最喜欢的留学国家，英国增幅明显。数据显示，2019年留学生选择美国的人数占比下滑，为43%。而选择英国的群体占比在2019年大幅上升，占比41%，同美国差距正在缩小。但是，留学申请本身不是一件容易的事——不仅需要准备文书、英文考试，还要从上百个学校列表中选出合适自己的高校。若能最有效地提升背景，就更有可能获得“dream school”录取。 本章使用一个留学申请数据集，该数据来自某留学申请论坛的录取汇报结果，包含了15908条申请者的申请学校及录取与否信息。变量数据说明表如下所示： 变量类型 变量名 详细说明 取值范围 因变量 申请结果 定性变量 3个水平：拒绝(rej)、录取(offer/ad)、候补(waiting list) 自变量 基础信息 编号 连续变量 [14, 38183] 作者 文本数据 例：leucocyte, yycenty 时间 连续变量 [2010, 2019] 申请信息 申请学校所属国家和地区 定性变量 8个水平：美国、澳洲、欧洲等 申请季节 定性变量 2个水平：fall（秋季）、spring（春季） 申请类型 定性变量 3个水平：PhD（博士）、MS（硕士）、混合 申请院校 定性变量 2431个水平：Yale University、Duke University等 申请专业 定性变量 34个水平：Accounting、Bio、CS等 学校-专业录取率 连续变量 [0, 1] 申请学校是否为top 50 定性变量 2个水平：0为否，1为是 原学校信息 申请前是否研究生毕业 定性变量 2个水平：0为否，1为是 原院校 定性变量 1359个水平：青岛大学、北京大学等 原专业 定性变量 1915个水平：自动化、信息工程等 是否转专业 定性变量 2个水平：0为否，1为是 其它背景 文本数据 例：4篇sci论文 成绩信息 托福成绩 连续变量 [70, 119] GRE总成绩 连续变量 新：[208, 340]，老：[512, 760] GRE Verbal成绩 连续变量 新：[135, 170]，老：[370, 760] GRE Quantitative成绩 连续变量 新：[147, 180]，老：[730, 800] GRE 写作成绩 连续变量 新：[1.5, 8]，老：[170, 1190] GRE sub分数 文本数据 例：Math 880 (94%) GPA 连续变量 [1, 95] GPA算法 定性变量 3个水平：100、4.3、4 拓展信息 是否有牛推 定性变量 2个水平：0为否，1为是 是否有实习 是否有科研 是否有发表论文 是否以第一作者发表论文 是否有SCI论文 是否有交流经历 ## 设置绘图主题 library(ggplot2) plot_theme_pie &lt;- theme(panel.background = element_rect(fill = rgb(255, 255, 255, maxColorValue = 255)), plot.background = element_rect(rgb(255, 255, 255, maxColorValue = 255)), axis.text = element_text(color = rgb(236, 241, 249, maxColorValue = 255)), panel.grid.major = element_line(color = rgb(236, 241, 249, maxColorValue = 255)), panel.grid.minor = element_line(color = rgb(236, 241, 249, maxColorValue = 255)), plot.title = element_text(family = &quot;STHeiti&quot;, face = &quot;bold&quot;, size = 14), legend.title = element_text(family = &quot;STHeiti&quot;, face = &quot;bold&quot;,size = 12), legend.text = element_text(family = &quot;STHeiti&quot;,size = 11)) # 饼图绘制主题 ## 设置绘图主题 plot_theme &lt;- theme(panel.background = element_rect(fill = rgb(255, 255, 255, maxColorValue = 255)), plot.background = element_rect(rgb(255, 255, 255, maxColorValue = 255)), axis.text = element_text(size = 12,family = &quot;STHeiti&quot;), axis.text.x = element_text(size = 12, family = &quot;STHeiti&quot;, face = &quot;bold&quot;) , axis.text.y = element_text(size = 12, family = &quot;STHeiti&quot;, face = &quot;bold&quot;) , axis.ticks = element_line(color = rgb(236, 241, 249, maxColorValue = 255)), axis.title = element_text(size = 13, family = &quot;STHeiti&quot;), panel.grid.major = element_line(size = 1), panel.grid.minor = element_line(color = rgb(236, 241, 249, maxColorValue = 255)), plot.title = element_text(family = &quot;STHeiti&quot;, face = &quot;bold&quot;, size = 14), legend.title = element_text(family = &quot;STHeiti&quot;, face = &quot;bold&quot;,size = 12), legend.text = element_text(family = &quot;STHeiti&quot;,size = 11)) # 其他图形绘制主题 申请结果分布 读入数据Data_Cleaning.csv，命名为descriptive，将数据按照变量 index_origin（原始编号）由小到大排序。查看变量 offertype（录取结果） 的类型。为简化研究，将录取类型”AD小奖”、“Offer”、“AD无奖”统一为 “Admitted”，将录取类型”Rej”修改为”Rejected”，并删去缺失录取结果的样本。根据录取结果绘制饼状图，描述你观察到的结果。 ## 读入数据 descriptive &lt;- read.csv(&quot;./data/Data_Cleaning.csv&quot;, header = T, stringsAsFactors = F) # 读取原始数据 descriptive &lt;- descriptive[order(descriptive$index_origin),] # 将数据按照变量index_origin（原始编号）排序 ## 调整变量类型 descriptive$offertype[descriptive$offertype %in% c(&quot;AD小奖&quot;, &quot;Offer&quot;, &quot;AD无奖&quot;)] &lt;- &quot;Admitted&quot; # 不考虑奖学金，均归入“Admitted“（录取） descriptive$offertype[descriptive$offertype == &quot;Rej&quot;] &lt;- &quot;Rejected&quot; descriptive &lt;- descriptive[ - which(descriptive$offertype == &quot;&quot;),] # 删去缺失录取结果的样本 ## 绘制饼状图 (piechart1 &lt;- ggplot(descriptive, aes(x = factor(1), fill = factor(descriptive$offertype))) + geom_bar(position = &quot;fill&quot;, width = 1) + scale_fill_manual(&quot;申请结果&quot;, values = c(&quot;grey&quot;,&quot;gold&quot;,&quot;skyblue&quot;)) + coord_polar(theta = &quot;y&quot;) + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;\\n录取类型&quot;) + plot_theme_pie) 数据中，录取的案例占一半以上，其中包括无奖录取、小奖录取和全奖录取，拒绝的申请占比为27.6%，还有少部分申请结果为Waiting List。在后续的建模中，将把Waiting List划分为拒绝类别。 最热门的top10申请学校 我们先来看热门的申请学校（college_apply）。由于数据存在错误，我们需要对申请学校这一变量进行简单修正：将学校名”Texas A”、“M University”替换”Texas A&amp;M University”，将”Washington University in St”、” Louis”替换为”Washington University in St. Louis”。随后，为了统一申请学校名称，我们要将学校缩写替换为全称，得到的新变量College_apply_new 并入原数据（注意：不考虑缩写字母的大小写差异，需要借助文件美国大学缩写汇总.txt）。根据统一后的学校名称，找出申请人数最多的10所热门学校。 ## 修正数据 descriptive$college_apply[descriptive$college_apply %in% c(&quot;Texas A&quot;, &quot;M University&quot;)] &lt;- &quot;Texas A&amp;M University&quot; descriptive$college_apply[descriptive$college_apply %in% c(&quot;Washington University in St&quot;, &quot; Louis&quot;)] &lt;- &quot;Washington University in St. Louis&quot; ## 统一学校名称 SuoXie &lt;- read.table(&quot;./data/美国大学缩写汇总.txt&quot;, header = T) # 读入常见的美国大学缩写汇总 college_apply_new &lt;- NULL # 设置初始值 college_low &lt;- tolower(descriptive$college_apply) # 不考虑大小写差异（下同） suoxie_low &lt;- tolower(SuoXie$ysuoxie) for(i in 1:dim(descriptive)[1]){ # 统一全称和缩写 if (college_low[i] %in% suoxie_low) { college_apply_new[i] &lt;- as.character(SuoXie$yquancheng[suoxie_low %in% college_low[i]]) } else college_apply_new[i] &lt;- descriptive$college_apply[i] } descriptive$College_apply_new &lt;- college_apply_new # 统一学校名称后的新变量 ## 找出10大热门学校 (top10_college_apply &lt;- names(sort(table(descriptive$College_apply_new), decreasing = T)[c(1:10)])) ## [1] &quot;University of California&quot; &quot;Carnegie Mellon University&quot; ## [3] &quot;University of Southern California&quot; &quot;Columbia University&quot; ## [5] &quot;University of Pennsylvania&quot; &quot;Northeastern University&quot; ## [7] &quot;University of Illinois at Urbana-Champaign&quot; &quot;University of Michigan&quot; ## [9] &quot;Cornell University&quot; &quot;Texas A&amp;M University&quot; 申请GPA对于结果的影响 找到10所热门学校后，下一步探究这些学校的录取情况。注意：为简化后续分析，删掉所有录取结果为 WaitingList 的样本，只保留15908条数据。 GPA（标准化考试成绩）是申请的关键之一。用变量gpa除以分制变量gpa_measure，再乘以4.0，可以将GPA标准化为四分制，得到新变量Standardgap并入原数据。考虑到不同排名的学校对学生成绩的要求差异，对申请学校进行排名划分。读入“QS大学排名前百（美国）.txt”，数据集前19所学校是世界前五十名的美国名校，据此将所有申请学校划分为”Top50”和”Others”两类，得到因子型的新变量CollegeRankTop50并入原数据。 接着，考查学业成绩Standardgap和学校排名CollegeRankTop50对录取结果的影响。绘制成绩与申请结果箱线图，并谈谈你的发现。注意，从现实出发，只画出Standardgap&gt;2.0的样本。 最后，将成绩划分为”&lt;=3.4”, “3.4-3.55”, “3.55-3.7”, “&gt;3.7”四类，得到新变量gpa_dis并入原数据。 ## 为简化后续分析，删掉录取结果为 WaitingList 的样本 descriptive &lt;- descriptive[-which(descriptive$offertype == &quot;WaitingList&quot;),] ## 按学校名称匹配大学排名 universities &lt;- read.table(&quot;./data/QS大学排名前百（美国）.txt&quot;,header = F, sep=&quot;\\n&quot;)$V1 # 读入QS世界大学排名 top50university &lt;- NULL # 变量初始化 for(i in 1:dim(descriptive)[1]){ top50university[i] &lt;- descriptive$College_apply_new[i] %in% universities[1:19] # 共19所美国名校进入世界前五十名 } ## 整理变量 collegerank &lt;- rep(&quot;Others&quot;,dim(descriptive)[1]) collegerank[top50university] &lt;- &quot;Top50&quot; # 大学排名前50 descriptive$CollegeRankTop50 &lt;- collegerank # 并入原数据 ## gpa标准化 descriptive$Standardgap &lt;- (descriptive$gpa/descriptive$gpa_measure)*4 # 将gpa统一整理为4分制 gpa_offertype &lt;- descriptive[, c(&quot;Standardgap&quot;, &quot;offertype&quot;, &quot;CollegeRankTop50&quot;)] gpa_offertype$offertype &lt;- factor(gpa_offertype$offertype, levels = c(&quot;Admitted&quot;, &quot;Rejected&quot;), labels = c(&quot;录取&quot;,&quot;被拒&quot;)) # 调整因子水平 gpa_offertype$CollegeRankTop50 &lt;- factor(gpa_offertype$CollegeRankTop50, levels = c(&quot;Top50&quot;,&quot;Others&quot;)) ## 画图 (boxplot1 &lt;- ggplot(gpa_offertype, aes(x = factor(CollegeRankTop50), y = Standardgap, fill = factor(offertype))) + geom_boxplot(show.legend = T, varwidth = T) + scale_fill_manual(&quot;申请结果&quot;, values = c(&quot;grey&quot;, &quot;gold&quot;)) + # 按照申请结果填色 scale_y_continuous(limits = c(2, 4),breaks = seq(2, 4, by = 0.2))+ # 不考虑gpa&lt;2.0的样本 labs(x = &quot;申请学校的世界排名&quot;, y = &quot;GPA&quot;, title = &quot;成绩与申请结果&quot;) + plot_theme) ## 成绩分段 descriptive$gpa_dis &lt;- cut(descriptive$Standardgap, breaks = c(0, 3.4, 3.55, 3.7, Inf), labels = c(&quot;&lt;=3.4&quot;, &quot;3.4~3.55&quot;, &quot;3.55~3.7&quot;, &quot;&gt;3.7&quot;)) 结论：申请 Top50 学校时，录取的学生平均GPA（中位数）较高。申请其他学校时，GPA差异不明显。而从波动程度来看，无论是申请Top50还是其他学校，录取同学的GPA的差异都比未录取的GPA差异大，说明高GPA不一定是录取的必要因素。 托福成绩对于申请结果的影响 托福成绩也是申请时至关重要的一环。将变量toefl转化为数值变量，随后划分为”&lt;=98”, “98-102”, “102-106”, “&gt;106”四个分数段，得到因子型的新变量toefl_dis并入数据。计算不同排名的学校在各分数段的录取率，绘制复式条形图。注意标注出录取率百分数，并添加一条平均录取率作为参考线。从图中你能得到什么结论？ ## 整理托福成绩 descriptive$toefl &lt;- as.numeric(descriptive$toefl) # 将托福成绩信息变为数值型 descriptive$toefl_dis &lt;- cut(descriptive$toefl, breaks = c(0, 98, 102, 106, Inf), labels = c(&quot;&lt;=98&quot;, &quot;98~102&quot;, &quot;102~106&quot;, &quot;&gt;106&quot;)) ## 计算录取率 ifadmitted &lt;- ifelse(descriptive$offertype == &quot;Admitted&quot;,1,0) admittedPct &lt;- aggregate(ifadmitted, list(descriptive$toefl_dis, descriptive$CollegeRankTop50), mean) colnames(admittedPct) &lt;- c(&quot;TOEFL&quot;,&quot;学校排名&quot;,&quot;admittedpct&quot;) admittedPct$学校排名 &lt;- factor(admittedPct$学校排名,levels = c(&quot;Top50&quot;,&quot;Others&quot;)) ### 画图 (barplot2 &lt;- ggplot(admittedPct, aes(TOEFL, admittedpct, fill = 学校排名)) + geom_bar(stat=&#39;identity&#39;,position=&#39;dodge&#39;) + scale_fill_manual(&quot;学校排名&quot;, values = c(&quot;grey&quot;, &quot;gold&quot;)) + # 按照申请结果填色 labs(x=&quot;&quot;, y=&quot;&quot;, title=&quot;\\n不同托福成绩的平均录取率&quot;) + geom_text(label = paste(round(admittedPct[order(admittedPct$TOEFL), 3], 2)*100, &quot;%&quot;, sep=&#39;&#39;), colour = &quot;black&quot;, position = position_dodge(1), size = 3, vjust = - 0.8) + geom_hline(aes(yintercept = mean(ifadmitted)), col = &quot;orange&quot;, lwd = 1)+ geom_text(label = paste(round(mean(ifadmitted), 2)*100, &quot;%&quot;, sep=&#39;&#39;), colour = &quot;orange&quot;,x = 5.4, y = 0.7, size = 5.7, vjust = - 0.5) + plot_theme) 结论：对于排名靠前的学校来说，托福成绩越高，平均录取率越高。 硬件条件对于不同学位申请的影响 申请博士与硕士需要做哪些准备呢？我们来探究硬件条件对于不同学位（type）申请的影响。硬件条件共包括6个变量：“research”、“paper”、“first”、“sci”、“rl”、“intern”和”exchange”，依次代表科研、论文、一作、 SCI论文、牛推、实习和交换。对这些变量取绝对值后，得到6个0-1变量，0代表不具备某种硬件条件，反之为1。 下面，绘制出硬件条件与申请学位的矩阵图。矩阵的每个元素代表对应学位的所有申请者中，提到自己具备对应硬件条件的人数比例。以（1, 1）元素为例，表示硕士申请者中，提到自己有过交换出国经历的比例为2.8%。注意：利用R包data.table和reshape能够简化矩阵计算；横纵坐标各水平的先后顺序与下图保持一致。从图中你能得到什么结论？ library(reshape) ## 预处理 descriptive$first &lt;- abs(descriptive$first) descriptive$sci &lt;- abs(descriptive$sci) ## 录取情况与硬件条件 extra_offertype &lt;- descriptive[, c(&quot;rl&quot;, &quot;intern&quot;, &quot;research&quot;, &quot;paper&quot;, &quot;first&quot;, &quot;sci&quot;, &quot;exchange&quot;, &quot;type&quot;)] tab1 &lt;- table(extra_offertype$type) extra_offertype &lt;- melt(extra_offertype, id = &quot;type&quot;) # 短表变长表 count &lt;- subset(extra_offertype, extra_offertype$value == 1) tab2 &lt;- table(count$type, count$variable) # 申请硕博拥有某硬件条件的情况 count_plot &lt;- melt(rbind(tab2[1, ]/tab1[1], tab2[2, ]/tab1[2], tab2[3, ]/tab1[3])) # 学位×硬件条件 colnames(count_plot)[1:2] &lt;- c(&quot;X1&quot;, &quot;X2&quot;) count_plot$X1 &lt;- factor(count_plot$X1, levels = c(1, 2, 3), # 调整变量因子水平 labels = c(&quot;混合&quot;, &quot;MS&quot;, &quot;PhD&quot;)) count_plot$X2 &lt;- factor(count_plot$X2, levels = c(&quot;research&quot;,&quot;paper&quot;,&quot;first&quot;, &quot;sci&quot;,&quot;rl&quot;,&quot;intern&quot;,&quot;exchange&quot;), labels = c(&quot;科研&quot;, &quot;论文&quot;, &quot;一作&quot;, &quot;SCI论文&quot;, &quot;牛推&quot;, &quot;实习&quot;, &quot;交换&quot;)) ## 绘制硬件条件与申请学位的矩阵图 (matrix1 &lt;- ggplot(count_plot, aes(x = X1, y = X2, label = value, fill = value)) + # 画图 geom_tile(show.legend = F) + geom_text(label = paste(round(count_plot$value, 3)*100, &quot;%&quot;, sep = &#39;&#39;), color = &quot;black&quot;, family = &quot;STHeiti&quot;, size = 4.5) + scale_fill_gradient(&quot;count&quot;, low = &quot;white&quot;, high = &quot;lightCoral&quot;) + labs(x = &quot;申请学位&quot;, y = &quot;硬件条件&quot;, title = &quot;&quot;) + plot_theme) 结论：申请硕士实习更加重要，申请博士论文更加重要，而混合申请介于两者之间。 7.5 实例分析 模型建立与选择 最后，我们将申请结果offertype作为因变量（录取=1，被拒=0），在训练集上建立逻辑回归模型，选取的变量如下： ## [1] &quot;offertype ~ season + type + cross + rl + intern + research + paper + first + sci + exchange + CollegeRankTop50 + gpa_dis + toefl_dis&quot; 对数据进行训练集与测试集的划分（0.8: 0.2）。以上述方程在训练集数据上建立逻辑回归模型，对模型进行变量选择（使用AIC准则），并对模型结果给出合理的解读。 ## 抽取训练集 set.seed(1234) # 随机数种子 nsample &lt;- sample(x = dim(descriptive)[1], size = dim(descriptive)[1]/5, replace = F) ## 重新划分训练集和测试集 descriptive_train &lt;- descriptive[-nsample, ] descriptive_test &lt;- descriptive[nsample, ] ## 建立逻辑回归模型 myglm0 &lt;- glm(formula, family = binomial(), data = descriptive_train) # 逻辑回归 myglm &lt;- step(myglm0, trace = F) # AIC准则逐步回归 summary(myglm) # 查看回归结果 ## ## Call: ## glm(formula = offertype ~ season + type + cross + intern + research + ## paper + sci + exchange + CollegeRankTop50 + gpa_dis + toefl_dis, ## family = binomial(), data = descriptive_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1342 -1.3457 0.7548 0.8532 1.2424 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.13736 0.20887 5.445 5.17e-08 *** ## seasonSpring 0.60585 0.17403 3.481 0.000499 *** ## typeMS -0.20011 0.20368 -0.982 0.325855 ## typePhD -0.32947 0.21078 -1.563 0.118021 ## cross -0.16464 0.05809 -2.834 0.004594 ** ## intern -0.09656 0.06055 -1.595 0.110782 ## research -0.20227 0.06707 -3.016 0.002564 ** ## paper 0.19493 0.08362 2.331 0.019748 * ## sci 0.19950 0.09938 2.008 0.044692 * ## exchange 0.73152 0.14850 4.926 8.39e-07 *** ## CollegeRankTop50Top50 -0.56726 0.04850 -11.695 &lt; 2e-16 *** ## gpa_dis3.4~3.55 0.01246 0.05856 0.213 0.831454 ## gpa_dis3.55~3.7 0.23086 0.06219 3.712 0.000205 *** ## gpa_dis&gt;3.7 0.40755 0.06966 5.850 4.90e-09 *** ## toefl_dis98~102 -0.05808 0.06240 -0.931 0.351984 ## toefl_dis102~106 0.01725 0.06343 0.272 0.785626 ## toefl_dis&gt;106 0.18947 0.07032 2.695 0.007048 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 12122 on 9976 degrees of freedom ## Residual deviance: 11871 on 9960 degrees of freedom ## (2750 observations deleted due to missingness) ## AIC: 11905 ## ## Number of Fisher Scoring iterations: 4 经过AIC准则选择后，模型留下了对申请结果影响显著的变量。根据模型形式的理解，逻辑回归模型的系数体现了因变量分别取1和0的可能性大小。例如，对于申请类型而言，其基准组为只申请硕士（MS），在控制其他变量不变的情况下，申请PhD的同学被录取的可能性比只申请硕士的同学录取可能性大；同样地，控制其他变量不变的情况下，有一作论文发表、海外交流经历的同学比没有相关经验的同学更易获得录取；从GPA和托福成绩这两个硬性指标来看，随着成绩区间档位的上升，获得录取的可能性增大，因此提高提高英语考试成绩是申请季稳中求胜的“法宝”。 模型预测 用逐步回归后的模型对测试集的申请结果进行预测，并利用R包pROC绘制出ROC曲线图，根据曲线对模型进行评价。 library(pROC) # 进行预测 pred &lt;- predict(myglm, descriptive_test, type=&quot;response&quot;) par(family=&#39;STHeiti&#39;) # 绘制ROC曲线 plot.roc(descriptive_test$offertype, pred, col = &quot;dodgerblue&quot;, print.auc=TRUE, auc.polygon=TRUE, auc.polygon.col=&quot;#f6f6f6&quot;, xlab = &quot;FPR&quot;,ylab = &quot;TPR&quot;, main = &quot;预测ROC曲线&quot;) 根据曲线的结果，AUC值（即ROC曲线下的面积）为0.585，说明该模型对数据的预测效果适中。 习题答案 案例背景 社交电商在运营中有一个非常重要的工作是留存客户，通过社交电商平台获取用户和积累了大量的老客户以后，可以专门针对会员建立会员群，以提升这部分群体的粘性和复购率，帮助社交电商平台提升销量。为了探究影响客户流失的因素，以客户是否流失为因变量，建立一套系统的客户流失预警模型，数据来自国内某社交电商平台。建模时使用sampledata.csv，预测时使用preddata.csv，所有的自变量来自当月，因变量（是否流失）来自下一个月，具体的变量介绍如下表所示。 题目 7.2 数据读入与概览 读入数据sampledata.csv和preddata.csv，分别命名为trainset和testset，并用summary函数展示训练集数据的情况。 trainset &lt;- read.csv(&quot;./data/sampledata.csv&quot;, fileEncoding = &quot;utf-8&quot;, header = T) testset &lt;- read.csv(&quot;./data/preddata.csv&quot;, fileEncoding = &quot;utf-8&quot;, header = T) summary(trainset) ## 数据概览 ## ID tenure expense degree tightness entropy chgexpense ## Min. : 1 Min. : 163.0 Min. : -3.378 Min. : -0.4319 Min. :-2.583 Min. :0.4846 Min. :-0.79261 ## 1st Qu.:12088 1st Qu.: 535.4 1st Qu.: 97.076 1st Qu.: 31.2269 1st Qu.: 5.397 1st Qu.:2.4177 1st Qu.:-0.07968 ## Median :24281 Median : 990.1 Median :134.250 Median : 54.6392 Median : 7.964 Median :3.0005 Median : 0.00000 ## Mean :24224 Mean :1256.1 Mean :157.270 Mean : 64.8805 Mean : 9.231 Mean :2.9491 Mean :-0.01584 ## 3rd Qu.:36348 3rd Qu.:1634.4 3rd Qu.:201.157 3rd Qu.: 88.6146 3rd Qu.:11.645 3rd Qu.:3.5461 3rd Qu.: 0.03222 ## Max. :48393 Max. :4475.9 Max. :437.791 Max. :214.9449 Max. :32.255 Max. :5.0324 Max. : 0.79717 ## chgdegree churn ## Min. :-0.992063 Min. :0.00000 ## 1st Qu.:-0.159091 1st Qu.:0.00000 ## Median :-0.009615 Median :0.00000 ## Mean : 0.011054 Mean :0.03147 ## 3rd Qu.: 0.153846 3rd Qu.:0.00000 ## Max. : 1.187500 Max. :1.00000 模型建立与解读 将因变量churn转换为因子型变量，建立逻辑回归模型，输出模型的结果并对结果进行解读。 trainset$churn &lt;- as.factor(trainset$churn) ## 因变量转换为因子型 fit &lt;- glm(churn ~ tenure+expense+degree+tightness+entropy+chgdegree+chgexpense, family = binomial(link = logit), data = trainset) ## 拟合逻辑回归模型 summary(fit) ## 输出模型估计结果 ## ## Call: ## glm(formula = churn ~ tenure + expense + degree + tightness + ## entropy + chgdegree + chgexpense, family = binomial(link = logit), ## data = trainset) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5980 -0.2725 -0.2387 -0.2096 3.0885 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.958e+00 1.604e-01 -12.212 &lt; 2e-16 *** ## tenure -1.263e-04 3.350e-05 -3.770 0.000163 *** ## expense -8.968e-04 3.696e-04 -2.427 0.015235 * ## degree 7.288e-05 1.274e-03 0.057 0.954397 ## tightness -2.460e-02 5.377e-03 -4.575 4.76e-06 *** ## entropy -3.471e-01 6.306e-02 -5.504 3.71e-08 *** ## chgdegree -6.191e-01 9.919e-02 -6.241 4.34e-10 *** ## chgexpense -3.163e-01 1.399e-01 -2.261 0.023768 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 12449 on 44516 degrees of freedom ## Residual deviance: 12207 on 44509 degrees of freedom ## AIC: 12223 ## ## Number of Fisher Scoring iterations: 6 根据建模的结果，可以看出，在保持其他变量不变的情况下，用户的在网时长越长，其流失的可能性就越低；当月的花费越高，流失的可能性越低；用户的联系强度越高，流失的可能性也越低。 模型预测与评估 将任务二中的参数估计结果应用到predata中，给出predata中每个用户预测的流失概率值，绘制预测的ROC曲线，并计算对应的AUC值。 library(pROC) ## 将模型结果应用到新的数据集上 fitted.results &lt;- predict(fit,newdata = testset, type = &#39;response&#39;) ## 绘制ROC曲线 &amp; 计算AUC值 library(ROCR) testset$churn &lt;- as.factor(testset$churn) par(family=&#39;STHeiti&#39;) plot.roc(testset$churn, col = &quot;dodgerblue&quot;, print.auc=TRUE, auc.polygon=TRUE, auc.polygon.col=&quot;#f6f6f6&quot;, xlab = &quot;特异度&quot;,ylab = &quot;敏感度&quot;, fitted.results, main = &quot;预测ROC曲线&quot;) 绘制ROC曲线如上所示，计算得到AUC值为0.617，模型在测试集上的预测表现较好。 "],["ch8.html", "第8章：降维分析 案例引入 8.1 主成分分析 8.2 因子分析 习题答案", " 第8章：降维分析 ### 数据准备 ### # 清空工作空间 rm(list = ls()) 案例引入 NBA，全称为美国职业篮球联赛(National Basketball Association)，作为美国四大职业体育联盟之一，在美国本土，在四大联赛中的排名绝非第一；而就全球而言，尤其是中国，其普及率和收看率，则是远远高于其他三大联盟。 NBA作为商业联盟，不管是联盟还是球队，最关心的问题还是盈利，其球队市值估价也逐年提高。据《福布斯》2019年发布的NBA球队市值，30支NBA球队的估值首次全部达到或超过12亿美元，平均市值为19亿美元，较去年增长13%，是三年前的3倍，其中，纽约尼克斯达到40亿美金，是所有球队中最高的。对于NBA球队来说，其收入包括门票收入、广告收入等，而球队支出中，则有一大部分在于球员薪金。 那么，自由球员在自由市场中的价值，具体表现为其下一份合同的薪金大小，是由哪些因素决定的？根据经验，主要的因素应该包括其在球场上的表现以及其展现的天赋和能力，再或者是对胜利的贡献。对于球队经理来说，需要主观地综合考虑多方面的因素，给出合适的合同。因此对球员薪金影响因素的量化分析，可以更合理地估计出球员在各个方面的水平与价值，对应合理的薪金合同，也有助于球队挑选更具性价比的球员，组建更为合理的阵容。这对球队战绩的提升、球队运营收入的增益，有着重要的意义。 本章案例数据收集自截至2019年NBA球员季后赛总得分和每个球员的比赛详细数据。该数据收集了2448条NBA职业篮球运动员的各项比赛数据，其中包含勒布朗·詹姆斯、迈克尔·乔丹、科比·布莱恩特等多位全能巨星球员的投篮、三分球、罚球、助攻、抢断次数和季后赛总得分等18个变量信息。变量说明表如下所示： 变量类型 变量名 详细说明 取值范围 因变量 生涯总得分 连续变量 0-6911 自变量 个人信息 球员 文本数据 每个球员的官方姓名 出勤统计 出场数 连续变量 1-259 上场总时间 连续变量 0-10059 投篮统计 投篮率 连续变量 0%-100% 命中次数 连续变量 0-2457 出手次数 连续变量 0-5006 三分统计 三分投球率 连续变量 0%-100% 三分命中次数 连续变量 0-410 三分出手次数 连续变量 0-1116 罚球统计 罚球率 连续变量 0%-100% 罚球命中次数 连续变量 0-1627 罚球出手次数 连续变量 0-2317 其他技术统计 篮板数 连续变量 0-4104 助攻次数 连续变量 0-2346 抢断次数 连续变量 0-419 盖帽次数 连续变量 0-568 失误次数 连续变量 0-866 犯规次数 连续变量 0-797 8.1 主成分分析 读入数据并绘制相关系数矩阵 读入数据NBA.xlsx，命名为nba，并将第2-18个变量提取为一个新的数据框，命名为predictor。绘制变量之间的相关系数矩阵图 library(psych) library(readxl) nba &lt;- read_excel(&quot;./data/NBA.xlsx&quot;) predictor &lt;- nba[2:18] ## 变量间相关系数 M &lt;- cor(predictor) par(family=&#39;STHeiti&#39;) corrplot::corrplot(M, tl.srt = 60,tl.col = &quot;black&quot;) 根据变量的相关系数图可以看出，部分变量之间的正相关性较强，例如出场数和上场总时间，命中次数和出手次数等。 选择主成分个数 使用psych包中的scree()函数，绘制崖底碎石土，选择合适的主成分个数。scree()函数中，通过指定参数factors和pc的取值，生成主成分分析或因子分析的崖底碎石图，hline参数取值为绘制水平直线的高度，默认值为1，设置为负数则不绘制水平线。scree()函数会返回根据特征值绘制的崖底碎石图，同时返回特征值向量，因此可以使用该返回值计算累计方差贡献率。 ## 主成分分析选择因子数目 par(family=&#39;STHeiti&#39;) result1 &lt;- scree(predictor, factors = F, pc = T, main = &quot;主成分分析崖底碎石图&quot;, hline = -1) ## 计算累计方差贡献率 cumvar &lt;- round(cumsum(result1$pcv)/sum(result1$pcv),2) cat(&#39;前三个主成分累计方差贡献率为：&#39;, cumvar[1:3]) ## 前三个主成分累计方差贡献率为： 0.63 0.73 0.8 根据崖底碎石图的拐点，结合主成分解释总体方差的比例（约为80%），选择主成分个数为3。 提取主成分与结果解读 根据任务二中确定的主成分个数，提取主成分，并对结果进行解读。 ## 提取主成分 pc &lt;- principal(predictor, nfactors = 3) pc ## Principal Components Analysis ## Call: principal(r = predictor, nfactors = 3) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 RC3 h2 u2 com ## 出场数 0.85 0.30 0.20 0.86 0.144 1.4 ## 上场总时间（min） 0.91 0.33 0.13 0.95 0.051 1.3 ## 投篮率 0.12 -0.07 0.80 0.67 0.334 1.1 ## 命中次数 0.93 0.25 0.08 0.94 0.059 1.2 ## 出手次数 0.92 0.28 0.08 0.93 0.065 1.2 ## 三分投球率 -0.09 0.54 0.51 0.57 0.432 2.0 ## 三分命中次数 0.31 0.90 0.03 0.91 0.087 1.2 ## 三分出手次数 0.33 0.90 0.03 0.93 0.071 1.3 ## 罚球率 0.17 0.11 0.61 0.41 0.591 1.2 ## 罚球命中次数 0.92 0.23 0.05 0.90 0.101 1.1 ## 罚球出手次数 0.94 0.17 0.05 0.92 0.084 1.1 ## 篮板数 0.90 0.01 0.11 0.83 0.174 1.0 ## 助攻次数 0.77 0.41 0.03 0.76 0.238 1.5 ## 抢断次数 0.70 0.56 0.08 0.81 0.191 1.9 ## 盖帽次数 0.71 0.02 0.13 0.53 0.475 1.1 ## 失误次数 0.73 0.52 0.08 0.80 0.195 1.8 ## 犯规次数 0.91 0.18 0.16 0.88 0.120 1.1 ## ## RC1 RC2 RC3 ## SS loadings 9.00 3.16 1.43 ## Proportion Var 0.53 0.19 0.08 ## Cumulative Var 0.53 0.72 0.80 ## Proportion Explained 0.66 0.23 0.10 ## Cumulative Proportion 0.66 0.90 1.00 ## ## Mean item complexity = 1.3 ## Test of the hypothesis that 3 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.05 ## with the empirical chi square 1937.64 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.99 R的输出结果中，前三列为三个成分的载荷，它是指观测变量与主成分的相关系数，这里需要注意，载荷系数的正负本身没有意义 ，但是不同载荷系数之间的正负对比是有意义的；h2栏指主成分对每个原始变量的方差贡献率，即主成分对每个变量的方差解释度，由每个主成分的载荷平方求和得到，例如对于出场数，三个主成分一共解释了86%的方差；u2栏指成分唯一性，由1-h2计算得到，即方差无法被主成分解释的比例；最后一列com为主成分在每个变量上的Hoffman’s复杂度指数，\\(X_i\\)对应的复杂度由\\(\\frac{\\left(\\Sigma_{k} \\rho_{k i}^{2}\\right)^{2}}{\\Sigma_{k}\\left(\\rho_{k i}\\right)^{4}}\\)计算得到，其中\\(\\rho_{k i}\\)为第k个主成分与变量\\(X_i\\)的因子载荷，该指数表示前k个主成分与第i个变量之间的综合相关性大小，值越大表示整体相关性越强。 从代码结果的最后一个输出表可以看出，第一主成分解释了出场数85%的方差，第三主成分解释了投篮率80%的方差，第二主成分则分别解释了三分命中次数和三分出手次数90%的方差。代码结果中的累计贡献率显示，前三个主成分的累计贡献率达到了80%，因此使用这三个主成分可以很好地概括这组数据。 利用主成分分量的值可以对各个主成分进行解释，第一主成分的三分投球率分量为负值，其余都为正值，除了投篮率、罚球率、三分投球率、三分命中次数等表示“命中”占比的指标外，其余分量大多数都大于0.5，因此第一主成分反映球员的比赛活跃与主动程度，可以称为场内活跃因子。第二大主成分在三分球的命中次数和出手次数上分量高达90%，在2010-2018的8个NBA赛季中，球员的三分球得分尝试大幅度增加，因为球队都不约而同地得出了一个结论：根据加入胜率的计算，三分球投篮尝试（特别是从角落投出）在统计数据上让球队赢得比赛的可能性增加20%到35%。因此，我们可以称第二大主成分为胜率加成因子。第三大主成分在投篮（命中）率、罚球（命中）率和三分投球（命中）率分量较大，由于命中率体现了球员的篮球技术水平，第三主成分则可以被称为技术水平因子。 计算主成分得分 通过principal()函数返回的weights对象，可以得到每个变量的主成分得分系数，从而将主成分表示为变量的线性组合形式。在principal()函数的基础上，添加参数score = TRUE，即可以获得所有球员样本在三个主成分上的得分，通过这个得分，可以从三个角度（场内活跃因子、胜率加成因子、技术水平因子）对球员的综合实力进行评价。请计算主成分得分系数，并输出前六个样本的主成分得分，对结果进行适当解读。 round(unclass(pc$weights), 2) # 计算主成分得分系数 ## RC1 RC2 RC3 ## 出场数 0.09 -0.01 0.07 ## 上场总时间（min） 0.10 0.00 0.01 ## 投篮率 -0.02 -0.13 0.64 ## 命中次数 0.12 -0.04 -0.02 ## 出手次数 0.11 -0.02 -0.03 ## 三分投球率 -0.15 0.25 0.37 ## 三分命中次数 -0.10 0.41 -0.08 ## 三分出手次数 -0.10 0.40 -0.08 ## 罚球率 -0.03 -0.02 0.46 ## 罚球命中次数 0.12 -0.05 -0.05 ## 罚球出手次数 0.14 -0.08 -0.04 ## 篮板数 0.15 -0.16 0.02 ## 助攻次数 0.07 0.08 -0.07 ## 抢断次数 0.02 0.16 -0.04 ## 盖帽次数 0.12 -0.12 0.05 ## 失误次数 0.04 0.13 -0.04 ## 犯规次数 0.12 -0.08 0.05 ## 计算主成分得分 pc &lt;- principal(predictor, nfactors = 3, scores = TRUE) head(pc$scores) ## RC1 RC2 RC3 ## [1,] 8.303699 8.511875 -2.74635320 ## [2,] 7.180972 3.157658 -1.17880314 ## [3,] 7.867405 -3.731918 0.03163649 ## [4,] 6.281025 6.742762 -1.80571153 ## [5,] 10.055778 -4.066821 -0.12789204 ## [6,] 10.340703 -3.711050 0.36334196 利用如下公式可得到主成分得分，以第一主成分为例： \\(PC_1\\)=0.09 * 出场数 + 0.1 * 上场总时间-0.02 * 投篮率 + 0.12 * 命中次数 + 0.11 * 出手次数-0.15 * 三分投球率-0.1 * 三分命中次数-0.1 * 三分出手次数-0.03 * 罚球率 + 0.12 * 罚球命中次数 + 0.14 * 罚球出手次数 + 0.15 * 篮板数 + 0.07 * 助攻次数 + 0.02 * 抢断次数 + 0.12 * 盖帽次数 + 0.04 * 失误次数 + 0.12 * 犯规次数 以第一个球员为例，他的场内活跃因子和胜率加成因子得分较高，说明他是一位在比赛场上主动进攻类型的球员，但是他的技术水平因子得分较低，因此还是需要多加练习，提高自己的投篮命中率，从而获得更多分数。 8.2 因子分析 选择合适的公共因子数目 在进行因子分析之前，需要对数据进行标准化，再计算协方差矩阵；或等价地，使用原始数据的相关系数矩阵作为因子分析函数的输入。利用scree()函数绘制因子分析的崖底碎石图，结合累计方差贡献率（由fa()函数输出）等指标，选择合适的公因子数目。 ## 因子分析 cov &lt;- cov(nba[,2:18]) ## 转换为相关系数矩阵（等价于标准化后数据的协方差矩阵） cor &lt;- cov2cor(cov) ## 选择因子数目 par(family=&#39;STHeiti&#39;) result2 &lt;- scree(cor, factors = T, pc = F, main=&quot;因子分析崖底碎石图&quot;, hline = -1) 利用scree()函数绘制因子分析的崖底碎石图，结合累计方差贡献率（由fa()函数输出）等指标，设置公因子数为3。 提取公共因子 使用NBA季后赛球员数据来进行公共因子的提取，公因子数目根据任务五的结果所得。使用最大似然法提取未旋转的因子，对提取公共因子的结果进行解读。 ## 提取公共因子 fa(cor, n.obs = 2448, nfactors = 3, rotate = &quot;none&quot;, fm = &quot;ml&quot;) ## Factor Analysis using method = ml ## Call: fa(r = cor, nfactors = 3, n.obs = 2448, rotate = &quot;none&quot;, fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML2 ML3 h2 u2 com ## 出场数 0.81 0.31 0.38 0.903 0.0970 1.7 ## 上场总时间（min） 0.87 0.35 0.31 0.977 0.0228 1.6 ## 投篮率 0.12 0.10 0.15 0.046 0.9537 2.7 ## 命中次数 0.86 0.47 0.07 0.964 0.0359 1.6 ## 出手次数 0.88 0.44 0.06 0.964 0.0365 1.5 ## 三分投球率 0.27 -0.26 0.08 0.148 0.8521 2.1 ## 三分命中次数 0.87 -0.49 -0.01 0.993 0.0074 1.6 ## 三分出手次数 0.88 -0.47 -0.01 0.995 0.0048 1.5 ## 罚球率 0.25 0.05 0.10 0.074 0.9258 1.4 ## 罚球命中次数 0.85 0.51 -0.12 0.993 0.0066 1.7 ## 罚球出手次数 0.82 0.55 -0.08 0.985 0.0154 1.8 ## 篮板数 0.67 0.53 0.29 0.812 0.1879 2.3 ## 助攻次数 0.80 0.28 0.07 0.729 0.2715 1.3 ## 抢断次数 0.83 0.07 0.19 0.735 0.2650 1.1 ## 盖帽次数 0.51 0.37 0.24 0.454 0.5462 2.3 ## 失误次数 0.84 0.14 0.08 0.726 0.2738 1.1 ## 犯规次数 0.78 0.43 0.34 0.902 0.0980 2.0 ## ## ML1 ML2 ML3 ## SS loadings 9.34 2.44 0.62 ## Proportion Var 0.55 0.14 0.04 ## Cumulative Var 0.55 0.69 0.73 ## Proportion Explained 0.75 0.20 0.05 ## Cumulative Proportion 0.75 0.95 1.00 ## ## Mean item complexity = 1.7 ## Test of the hypothesis that 3 factors are sufficient. ## ## The degrees of freedom for the null model are 136 and the objective function was 32.07 with Chi Square of 78273.06 ## The degrees of freedom for the model are 88 and the objective function was 4.81 ## ## The root mean square of the residuals (RMSR) is 0.04 ## The df corrected root mean square of the residuals is 0.05 ## ## The harmonic number of observations is 2448 with the empirical chi square 1017.8 with prob &lt; 4.3e-158 ## The total number of observations was 2448 with Likelihood Chi Square = 11717.56 with prob &lt; 0 ## ## Tucker Lewis Index of factoring reliability = 0.77 ## RMSEA index = 0.232 and the 90 % confidence intervals are 0.229 0.236 ## BIC = 11030.9 ## Fit based upon off diagonal values = 1 ## Measures of factor score adequacy ## ML1 ML2 ML3 ## Correlation of (regression) scores with factors 1 1.00 0.96 ## Multiple R square of scores with factors 1 0.99 0.91 ## Minimum correlation of possible factor scores 1 0.99 0.83 得到结果后，如何解读公共因子的含义呢？解释公因子\\(F_i\\)时，可以通过对载荷系数的绝对值较大的输入来解释，与主成分分析的R程序结果类似，这里同样需要注意，载荷系数的正负本身没有意义，但是不同载荷系数之间的正负对比是有意义的。以上面的结果为例，我们取公共因子的个数为3，3个公共因子反映的原始变量信息已占总信息的73%。查看输出的因子载荷矩阵A，以ML1对应的列（第一个公共因子的载荷向量）为例，除了投篮率、三分球技术指标、罚球率和盖帽次数外，其余各数值都接近或大于0.8，这表示其余的变量可以来解释公因子\\(F_1\\)，或者说\\(F_1\\)主要反应这些变量的信息。观察第二个公共因子，其载荷没有0.8左右较大的值，虽然也可以根据相对大小按照以上想法解释，但是容易使公共因子的意义含糊不清，因此我们将介绍因子旋转后的因子载荷矩阵，其实际含义将更加明显。 正交旋转 当直接提取出的公共因子的典型代表变量不是非常突出时，容易使公共因子的实际意义含糊不清，不利于对因子进行解释，为此需要对因子载荷矩阵进行旋转变换，使得各因子载荷矩阵的每一列各元素的平方按列向0或1两极转化，达到其结构简化的目的。在NBA季后赛球员得分的案例中，我们使用正交旋转进一步提取因子，对新的结果进行解读。 ## 因子旋转 fa &lt;- fa(cor, n.obs = 2448, nfactors = 3, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) fa ## Factor Analysis using method = ml ## Call: fa(r = cor, nfactors = 3, n.obs = 2448, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML3 ML2 h2 u2 com ## 出场数 0.54 0.70 0.34 0.903 0.0970 2.4 ## 上场总时间（min） 0.64 0.66 0.35 0.977 0.0228 2.5 ## 投篮率 0.07 0.20 0.02 0.046 0.9537 1.2 ## 命中次数 0.81 0.49 0.24 0.964 0.0359 1.9 ## 出手次数 0.81 0.48 0.28 0.964 0.0365 1.9 ## 三分投球率 -0.01 0.07 0.38 0.148 0.8521 1.1 ## 三分命中次数 0.28 0.10 0.95 0.993 0.0074 1.2 ## 三分出手次数 0.31 0.11 0.94 0.995 0.0048 1.2 ## 罚球率 0.14 0.19 0.14 0.074 0.9258 2.8 ## 罚球命中次数 0.92 0.33 0.20 0.993 0.0066 1.4 ## 罚球出手次数 0.91 0.38 0.16 0.985 0.0154 1.4 ## 篮板数 0.62 0.65 0.08 0.812 0.1879 2.0 ## 助攻次数 0.67 0.41 0.34 0.729 0.2715 2.2 ## 抢断次数 0.51 0.45 0.52 0.735 0.2650 3.0 ## 盖帽次数 0.44 0.50 0.09 0.454 0.5462 2.0 ## 失误次数 0.60 0.38 0.47 0.726 0.2738 2.6 ## 犯规次数 0.61 0.69 0.23 0.902 0.0980 2.2 ## ## ML1 ML3 ML2 ## SS loadings 5.89 3.43 3.07 ## Proportion Var 0.35 0.20 0.18 ## Cumulative Var 0.35 0.55 0.73 ## Proportion Explained 0.48 0.28 0.25 ## Cumulative Proportion 0.48 0.75 1.00 ## ## Mean item complexity = 1.9 ## Test of the hypothesis that 3 factors are sufficient. ## ## The degrees of freedom for the null model are 136 and the objective function was 32.07 with Chi Square of 78273.06 ## The degrees of freedom for the model are 88 and the objective function was 4.81 ## ## The root mean square of the residuals (RMSR) is 0.04 ## The df corrected root mean square of the residuals is 0.05 ## ## The harmonic number of observations is 2448 with the empirical chi square 1017.8 with prob &lt; 4.3e-158 ## The total number of observations was 2448 with Likelihood Chi Square = 11717.56 with prob &lt; 0 ## ## Tucker Lewis Index of factoring reliability = 0.77 ## RMSEA index = 0.232 and the 90 % confidence intervals are 0.229 0.236 ## BIC = 11030.9 ## Fit based upon off diagonal values = 1 ## Measures of factor score adequacy ## ML1 ML3 ML2 ## Correlation of (regression) scores with factors 0.99 0.96 1.00 ## Multiple R square of scores with factors 0.98 0.93 1.00 ## Minimum correlation of possible factor scores 0.96 0.86 0.99 因子旋转后，第一个公共因子的因子载荷旋转后弱化了投篮率和三分投球率的意义，使得第一公因子主要解释了命中、出手次数和罚球的命中、出手次数，反映球员的比赛活跃与主动程度，可以称为场内活跃因子；第二个公共因子比旋转前加强了篮板数、盖帽次数、命中次数这几个变量的含义，因此可以更清晰地反映球员的技术水平因子；第三个公因子与旋转前相比，强化了三分命中次数和三分出手次数的解释，主要反映了三分球相关的信息，是球员对于整场球的胜算率加成因素。这和主成分分析得到的结论是类似的，由于因子分析结果进行了因子旋转，第二个和第三个公因子的共性方差相对大小发生了变化，因此公因子的顺序和旋转之前也略有区别。 计算因子得分 对fa()返回的对象使用factor.scores()函数，可以得到因子得分矩阵，通过这个得分，可以从几个公因子的角度对球员的综合实力进行评价。计算NBA数据的巴特莱特因子得分，设置参数method = 'Bartlett'，查看得分矩阵的前六行。此外，在fa()函数返回的结果中，还可以得到得分系数（标准化的回归权重），储存在weights元素中。对于NBA数据集，输出对载荷矩阵进行旋转后得到的因子得分权重，并进行解读。 my_score &lt;- factor.scores(nba[,2:18], fa, method=&quot;Bartlett&quot;) head(my_score$scores) ## ML1 ML3 ML2 ## [1,] 12.579026 -4.495819 8.081668 ## [2,] 11.992141 -3.365741 1.038594 ## [3,] 6.600081 5.852415 -3.107882 ## [4,] 9.424972 -2.345650 6.347182 ## [5,] 9.883615 2.428957 -3.784769 ## [6,] 8.289221 4.695544 -3.199542 以第一个球员的因子得分为例，其在场内活跃度最高，三分球出手胜率加成也较好，但是命中率技术水平欠佳。 round(fa$weights,2) # 因子得分权重 ## ML1 ML3 ML2 ## 出场数 -0.14 0.32 0.01 ## 上场总时间（min） -0.43 1.09 0.02 ## 投篮率 -0.01 0.01 0.00 ## 命中次数 0.00 0.20 -0.02 ## 出手次数 0.01 0.17 -0.02 ## 三分投球率 0.00 0.01 0.00 ## 三分命中次数 -0.05 -0.16 0.46 ## 三分出手次数 -0.01 -0.31 0.66 ## 罚球率 0.00 0.01 0.00 ## 罚球命中次数 1.19 -1.16 -0.24 ## 罚球出手次数 0.41 -0.29 -0.11 ## 篮板数 -0.05 0.13 0.00 ## 助攻次数 0.00 0.02 0.00 ## 抢断次数 -0.02 0.06 0.01 ## 盖帽次数 -0.01 0.04 0.00 ## 失误次数 -0.01 0.02 0.00 ## 犯规次数 -0.11 0.28 0.00 根据因子得分权重的结果，每一个公共因子都可以表示为原变量的线性组合，由此得出因子得分函数。进一步地，把每个样本的观测值逐个代入因子得分函数后，即可得到样本的因子得分值。 因子分析结果可视化（选做） 若有2-3个公因子，还可以将每个样本的因子得分绘制在直角坐标系中，从而更清晰地看出样本的散布情况。这里以NBA数据为例，提取3个公因子，将2448个观测的因子得分散点图绘制在三维画布上如下所示。 ## 3个公因子可视化因子得分 fa1 &lt;- fa(cor, n.obs = 2448, nfactors = 3, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) my_score &lt;- factor.scores(nba[,2:18], fa1, method=&quot;Bartlett&quot;) s &lt;- data.frame(my_score$scores) ## 绘制3D散点图（在命令行执行，用浏览器打开结果） library(plotly) library(magrittr) plot_ly(x=s$ML1, # x axis y=s$ML2, # y axis z=s$ML3, # z axis type = &quot;scatter3d&quot;, size = 0.5) 习题答案 题目 8.1 R中自带的数据列表Harman23.cor包含了305个女孩的8个身体测量指标，列表的cov对象是原始数据及的相关系数矩阵。使用该相关系数矩阵，对8个身体指标进行主成分分析，用较少的变量替换这些原始的身体指标，并对结果进行解读。 library(psych) my_cov &lt;- Harman23.cor$cov ## 主成分分析选择因子数目 par(family=&#39;STHeiti&#39;) hw1 &lt;- scree(my_cov, factors = F, pc = T, main = &quot;主成分分析崖底碎石图&quot;, hline = -1) ## 提取主成分 pc &lt;- principal(my_cov, nfactors = 2) pc ## Principal Components Analysis ## Call: principal(r = my_cov, nfactors = 2) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 h2 u2 com ## height 0.90 0.25 0.88 0.123 1.2 ## arm.span 0.93 0.19 0.90 0.097 1.1 ## forearm 0.92 0.16 0.87 0.128 1.1 ## lower.leg 0.90 0.22 0.86 0.139 1.1 ## weight 0.26 0.88 0.85 0.150 1.2 ## bitro.diameter 0.19 0.84 0.74 0.261 1.1 ## chest.girth 0.11 0.84 0.72 0.283 1.0 ## chest.width 0.26 0.75 0.62 0.375 1.2 ## ## RC1 RC2 ## SS loadings 3.52 2.92 ## Proportion Var 0.44 0.37 ## Cumulative Var 0.44 0.81 ## Proportion Explained 0.55 0.45 ## Cumulative Proportion 0.55 1.00 ## ## Mean item complexity = 1.1 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.05 ## ## Fit based upon off diagonal values = 0.99 题目 8.2 R中自带的数据集USJudgeRatings包含了律师对美国高等法院法官的评分。数据框包含了43个观测、12个变量，变量表如下所示。 请使用主成分分析的方法，用较少的变量总结从INTG到RTEN这11个变量，使得这些主成分可以尽可能保留原始变量的信息，对结果进行解读，并计算43个观测样本的主成分得分。 USpredictor &lt;- USJudgeRatings[, -1] ## 主成分分析选择因子数目 par(family=&#39;STHeiti&#39;) hw2 &lt;- scree(USpredictor, factors = F, pc = T, main = &quot;主成分分析崖底碎石图&quot;, hline = -1) pc2 &lt;- principal(USpredictor, nfactors = 1, scores = T) pc2 ## Principal Components Analysis ## Call: principal(r = USpredictor, nfactors = 1, scores = T) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 h2 u2 com ## INTG 0.92 0.84 0.1565 1 ## DMNR 0.91 0.83 0.1663 1 ## DILG 0.97 0.94 0.0613 1 ## CFMG 0.96 0.93 0.0720 1 ## DECI 0.96 0.92 0.0763 1 ## PREP 0.98 0.97 0.0299 1 ## FAMI 0.98 0.95 0.0469 1 ## ORAL 1.00 0.99 0.0091 1 ## WRIT 0.99 0.98 0.0196 1 ## PHYS 0.89 0.80 0.2013 1 ## RTEN 0.99 0.97 0.0275 1 ## ## PC1 ## SS loadings 10.13 ## Proportion Var 0.92 ## ## Mean item complexity = 1 ## Test of the hypothesis that 1 component is sufficient. ## ## The root mean square of the residuals (RMSR) is 0.04 ## with the empirical chi square 6.21 with prob &lt; 1 ## ## Fit based upon off diagonal values = 1 head(pc2$scores) ## PC1 ## AARONSON,L.H. -0.1857981 ## ALEXANDER,J.M. 0.7469865 ## ARMENTANO,A.J. 0.0704772 ## BERDON,R.I. 1.1358765 ## BRACKEN,J.J. -2.1586211 ## BURNS,E.B. 0.7669406 题目 8.3 R中自带的数据列表ability.cov提供了六个心理学测验的数据，包括112个参与者的观测值和6个变量：非语言的普通智力测验（general）、画图测验（picture）、积木图案测验（blocks）、迷宫测验（maze）、阅读测验（reading）和词汇测验（vocab），该列表的cov对象为变量间的协方差矩阵。使用因子分析的方法，选择合适的公因子数目，将6个变量转化为计较少的一组潜在心理学因素，解释不同公因子的含义。 cov_hw3 &lt;- ability.cov$cov corr &lt;- cov2cor(cov_hw3) ## 选择因子数目 par(family=&#39;STHeiti&#39;) result2 &lt;- scree(corr, factors = T, pc = F, main=&quot;因子分析崖底碎石图&quot;, hline = -1) ## 提取公共因子 fa_hw2 &lt;- fa(corr, n.obs = ability.cov$n.obs, nfactors = 2, rotate = &quot;none&quot;, fm = &quot;ml&quot;) fa_hw2 ## Factor Analysis using method = ml ## Call: fa(r = corr, nfactors = 2, n.obs = ability.cov$n.obs, rotate = &quot;none&quot;, ## fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML2 h2 u2 com ## general 0.65 0.35 0.54 0.455 1.5 ## picture 0.35 0.54 0.41 0.589 1.7 ## blocks 0.47 0.75 0.78 0.218 1.7 ## maze 0.25 0.41 0.23 0.769 1.7 ## reading 0.96 -0.13 0.95 0.052 1.0 ## vocab 0.82 -0.04 0.67 0.334 1.0 ## ## ML1 ML2 ## SS loadings 2.42 1.16 ## Proportion Var 0.40 0.19 ## Cumulative Var 0.40 0.60 ## Proportion Explained 0.68 0.32 ## Cumulative Proportion 0.68 1.00 ## ## Mean item complexity = 1.4 ## Test of the hypothesis that 2 factors are sufficient. ## ## The degrees of freedom for the null model are 15 and the objective function was 2.48 with Chi Square of 268.35 ## The degrees of freedom for the model are 4 and the objective function was 0.06 ## ## The root mean square of the residuals (RMSR) is 0.04 ## The df corrected root mean square of the residuals is 0.07 ## ## The harmonic number of observations is 112 with the empirical chi square 4.23 with prob &lt; 0.38 ## The total number of observations was 112 with Likelihood Chi Square = 6.11 with prob &lt; 0.19 ## ## Tucker Lewis Index of factoring reliability = 0.968 ## RMSEA index = 0.068 and the 90 % confidence intervals are 0 0.171 ## BIC = -12.77 ## Fit based upon off diagonal values = 0.99 ## Measures of factor score adequacy ## ML1 ML2 ## Correlation of (regression) scores with factors 0.98 0.89 ## Multiple R square of scores with factors 0.96 0.80 ## Minimum correlation of possible factor scores 0.91 0.59 ## 因子旋转 farot_hw2 &lt;- fa(corr, n.obs = ability.cov$n.obs, nfactors = 2, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) farot_hw2 ## Factor Analysis using method = ml ## Call: fa(r = corr, nfactors = 2, n.obs = ability.cov$n.obs, rotate = &quot;varimax&quot;, ## fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML2 h2 u2 com ## general 0.50 0.54 0.54 0.455 2.0 ## picture 0.16 0.62 0.41 0.589 1.1 ## blocks 0.21 0.86 0.78 0.218 1.1 ## maze 0.11 0.47 0.23 0.769 1.1 ## reading 0.96 0.18 0.95 0.052 1.1 ## vocab 0.78 0.22 0.67 0.334 1.2 ## ## ML1 ML2 ## SS loadings 1.86 1.72 ## Proportion Var 0.31 0.29 ## Cumulative Var 0.31 0.60 ## Proportion Explained 0.52 0.48 ## Cumulative Proportion 0.52 1.00 ## ## Mean item complexity = 1.3 ## Test of the hypothesis that 2 factors are sufficient. ## ## The degrees of freedom for the null model are 15 and the objective function was 2.48 with Chi Square of 268.35 ## The degrees of freedom for the model are 4 and the objective function was 0.06 ## ## The root mean square of the residuals (RMSR) is 0.04 ## The df corrected root mean square of the residuals is 0.07 ## ## The harmonic number of observations is 112 with the empirical chi square 4.23 with prob &lt; 0.38 ## The total number of observations was 112 with Likelihood Chi Square = 6.11 with prob &lt; 0.19 ## ## Tucker Lewis Index of factoring reliability = 0.968 ## RMSEA index = 0.068 and the 90 % confidence intervals are 0 0.171 ## BIC = -12.77 ## Fit based upon off diagonal values = 0.99 ## Measures of factor score adequacy ## ML1 ML2 ## Correlation of (regression) scores with factors 0.97 0.90 ## Multiple R square of scores with factors 0.94 0.81 ## Minimum correlation of possible factor scores 0.88 0.63 "]]
